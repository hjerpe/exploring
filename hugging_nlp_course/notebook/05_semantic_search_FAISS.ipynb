{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6e3aa13-153a-442d-b9a6-e87544c48f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from utils import check_file_or_folder_existence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b593af",
   "metadata": {},
   "source": [
    "### Semantic search with FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13622e7",
   "metadata": {},
   "source": [
    "In section 5, we created a dataset of GitHub issues and comments from the ü§ó Datasets repository. In this section we‚Äôll use this information to build a search engine that can help us find answers to our most pressing questions about the library!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38aa3a4f-0c4e-4a14-a0d3-b8e5de746cc0",
   "metadata": {},
   "source": [
    "### TOC\n",
    "1. [Using embeddings for semantic search](#using-embeddings-for-semantic-search)\n",
    "2. [Loading and preparing the dataset](#loading-and-preparing-the-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e74715",
   "metadata": {},
   "source": [
    "As we saw in Chapter 1, Transformer-based language models represent each token in a span of text as an embedding vector. It turns out that one can ‚Äúpool‚Äù the individual embeddings to create a vector representation for whole sentences, paragraphs, or (in some cases) documents. These embeddings can then be used to find similar documents in the corpus by computing the dot-product similarity (or some other similarity metric) between each embedding and returning the documents with the greatest overlap.\n",
    "\n",
    "In this section we‚Äôll use embeddings to develop a semantic search engine. These search engines offer several advantages over conventional approaches that are based on matching keywords in a query with the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe81ac37",
   "metadata": {},
   "source": [
    "<img src=\"images/semantic_search/semantic-search.svg\" alt=\"Alternative text\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f89fbb47",
   "metadata": {},
   "source": [
    "#### Loading and preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e67551e",
   "metadata": {},
   "source": [
    "The first thing we need to do is download our dataset of GitHub issues, so let‚Äôs use load_dataset() function as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23c9638c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a3871403254b16b9da109c0df46d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7d1f2878b54b479a71a8df91eb90a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb097dda226640ad9730b6c2fe9b52d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.70M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9bb95297df4bc5aa0a767f10e3cf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073b103a130f4b6cb4e5d2f87812e82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'is_pull_request'],\n",
       "    num_rows: 1574\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset = load_dataset(\"hjerpe/github-kubeflow-issues\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb79ed6c",
   "metadata": {},
   "source": [
    "Here we‚Äôve specified the default train split in load_dataset(), so it returns a Dataset instead of a DatasetDict. The first order of business is to filter out the pull requests, as these tend to be rarely used for answering user queries and will introduce noise in our search engine. As should be familiar by now, we can use the Dataset.filter() function to exclude these rows in our dataset. While we‚Äôre at it, let‚Äôs also filter out rows with no comments, since these provide no answers to user queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5d9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "default_venv",
   "language": "python",
   "name": "default_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
