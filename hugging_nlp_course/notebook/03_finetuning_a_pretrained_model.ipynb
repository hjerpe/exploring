{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e3aa13-153a-442d-b9a6-e87544c48f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-03 17:50:45.292693: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-03 17:50:47.980286: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38aa3a4f-0c4e-4a14-a0d3-b8e5de746cc0",
   "metadata": {},
   "source": [
    "### TOC\n",
    "1. [Processing the data](#Processing-the-data)\n",
    "2. [Fine-tuning a model with the Trainer API](#Fine-tuning-a-model-with-the-Trainer-API)\n",
    "3. [A full training](#A-full-training)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18919cdf-8fa3-4f2e-9bea-8f0805cc8fce",
   "metadata": {},
   "source": [
    "### Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "765ba9ea-0048-4e2a-af62-67aefcec95d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/vscode/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Same as before\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# This is new\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f4992bc-8751-4f53-8638-e2254fec8eb3",
   "metadata": {},
   "source": [
    "Of course, just training the model on two sentences is not going to yield very good results. To get better results, you will need to prepare a bigger dataset.\n",
    "\n",
    "In this section we will use as an example the MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in a paper by William B. Dolan and Chris Brockett. The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). Weâ€™ve selected it for this chapter because itâ€™s a small dataset, so itâ€™s easy to experiment with training on it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10e9f6f5-20b0-4eb9-9ebb-c24c59e384fa",
   "metadata": {},
   "source": [
    "#### Loading a dataset from the Hub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a9bbed8-7ef8-44dd-8253-a5cf087f8d8a",
   "metadata": {},
   "source": [
    "The Hub doesnâ€™t just contain models; it also has multiple datasets in lots of different languages. You can browse the datasets [here](https://huggingface.co/datasets), and we recommend you try to load and process a new dataset once you have gone through this section (see the general documentation here). But for now, letâ€™s focus on the MRPC dataset! This is one of the 10 datasets composing the GLUE benchmark, which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d911619-907d-4535-97d5-f87220838e29",
   "metadata": {},
   "source": [
    "The ðŸ¤— Datasets library provides a very simple command to download and cache a dataset on the Hub. We can download the MRPC dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c8d3620-f1f6-4fc8-b777-f76f3be53f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e625017-d23a-43d7-a181-a1c3f4587e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "pprint(vars(raw_datasets))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed3f1d04-7978-4c5b-b1c1-9309596dcc90",
   "metadata": {},
   "source": [
    "As you can see, we get a DatasetDict object which contains the training set, the validation set, and the test set. Each of those contains several columns (sentence1, sentence2, label, and idx) and a variable number of rows, which are the number of elements in each set (so, there are 3,668 pairs of sentences in the training set, 408 in the validation set, and 1,725 in the test set).\n",
    "\n",
    "This command downloads and caches the dataset, by default in ~/.cache/huggingface/datasets. Recall from Chapter 2 that you can customize your cache folder by setting the HF_HOME environment variable.\n",
    "\n",
    "We can access each pair of sentences in our raw_datasets object by indexing, like with a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "761cc34f-058d-4540-b81a-ddce408fd134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4394722c-d6d9-4ee5-99ae-d8f042f16860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_data': MemoryMappedTable\n",
      "sentence1: string\n",
      "sentence2: string\n",
      "label: int64\n",
      "idx: int32\n",
      "----\n",
      "sentence1: [[\"Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\",\"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\",\"They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .\",\"Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .\",\"The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange .\",...,\"The G + J executive was testifying in Manhattan 's State Supreme Court where O 'Donnell and G + J are suing each other for breach of contract .\",\"By 10 p.m. , Claudette was centered about 320 miles east of Brownsville , with maximum sustained winds of 65 mph , 9 mph shy of hurricane strength .\",\"Jim Furyk celebrated his first Father 's Day as a father by winning his first major golf championship .\",\"\" We must not engage in borough warfare , \" Thompson testified at a budget hearing by the City Council 's Finance Committee .\",\"The Standard & Poor 's 500 Index slipped 3.14 points , or 0.29 percent , to 1,071.99 .\"],[\"He took batting practice on the field for the second time Tuesday since his opening-day injury .\",\"The redesigned Finder also features search , coloured labels for customized organization of documents and projects and dynamic browsing of the network for Mac , Windows and UNIX file servers .\",\"But to see those pages , users would be required by Amazon to register , and Amazon plans to limit the amount of any single book a customer can view .\",\"Also Thursday , the NYSE 's board elected six new directors - three non-industry representatives and three industry representatives .\",\"Between 1993 and 2002 , 142 allegations of sexual assault were reported .\",...,\"The technology-loaded Nasdaq Composite Index .IXIC added 0.35 of a point , or 0.02 percent , to 1,662 .\",\"\" This will be a marathon , not a sprint , \" said Jimmy D. Staton , a senior vice president with the utility .\",\"\" I am listening to the same things I listened to 17 years ago , \" Hollings said .\",\"Heatley was sixth in the league in goals last season with 41 and ninth in points with 89 .\",\"The cables can be engorged with far more power , and it can be turned on and off like a water spigot , making billing more accurate , Taub said .\"],[\"Law enforcement sources who spoke on condition of anonymity confirmed to The Associated Press that Limbaugh was being investigated by the Palm Beach County state attorney 's office .\",\"One of the most well studied facets of temperament is how people respond to novelty .\",\"July 18 : Hurlbert files a single count of felony sexual assault against Bryant .\",\"Mr Koizumi rebuked Mr Konoike , saying his remarks were \" inappropriate \" .\",\"The CBO analysis of the Senate package said an amendment sponsored by Sen. Maria Cantwell , D-Wash . , accounted for $ 40 billion of the total cost .\",...,\"On Hebert 's body were his wallet , driver license and cell phone , its batteries drained , Tracy said .\",\"Drewes and a friend were playing a game of \" ding-dong-ditch \" -- ringing doorbells and running away -- in the Woodbury neighborhood in suburban Boca Raton .\",\"Advertising and circulation revenues from the flagship Martha Stewart Living magazine have declined .\",\"That risk , described as minor , remains \" the predominant concern for the foreseeable future , \" the statement released after the Fed meeting said .\",\"But right now it looks manageable , \" Gehman told reporters .\"],[\"Elsewhere in the diary , Truman showed a more familiar side , colorful and outspoken in his disdain for life in the White House .\",\"Russian Yuri Malenchenko , American Edward Lu and Pedro Duque of Spain landed in their Soyuz TMA-2 capsule at 0240 GMT .\",\"Authorities in Alabama said the passage of time , and the transfer or retirement of some investigators , has not hurt their case .\",\"Fasting glucose was 142 mg / dL on average for those given usual care , compared with 129 mg / dL in the group given specialized care ( P < .01 ) .\",\"The victim , who was also not identified , was taken to Kings County Hospital in critical condition .\",...,\"\" At this point , Mr. Brando announced : ' Somebody ought to put a bullet ' \" through her head , the motion continued .\",\"Martin , 58 , will be freed today after serving two thirds of his five-year sentence for the manslaughter of 16-year-old Fred Barras .\",\"\" We have concluded that the outlook for price stability over the medium term has improved significantly since our last decision to lower interest rates , \" Duisenberg said .\",\"The notification was first reported Friday by MSNBC .\",\"The 30-year bond US30YT = RR rose 22 / 32 for a yield of 4.31 percent , versus 4.35 percent at Wednesday 's close .\"]]\n",
      "sentence2: [[\"Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\",\"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\",\"On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\",\"Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .\",\"PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .\",...,\"He spoke in Manhattan 's State Supreme Court , where O 'Donnell and G + J sued each other for breach of contract .\",\"Early Monday , the center of Claudette was about 300 miles east of Brownsville , with maximum sustained wind blowing at 65 mph , 9 mph shy of hurricane strength .\",\"His first Father 's Day as a dad , his first major as a champion .\",\"\" We must not engage in borough warfare , \" the Comptroller William Thompson told the Council , according to his written testimony .\",\"The Standard & Poor 's retail index < .RLX > was up more than 1.5 percent .\"],[\"Jeter , who dislocated his left shoulder in a collision March 31 , took batting practice on the field for the first time Monday .\",\"It also supports coloured labels to better organise documents , and dynamic browsing of the network for Mac , Windows and Unix file servers .\",\"But to see those pages Amazon would require users to register , and it plans to limit the amount of any single book a browser can view .\",\"Also Thursday , the NYSE 's board elected six new directors to the board and re-elected six others .\",\"From 1993 to 2002 , there were 142 reported sexual assaults at the academy .\",...,\"The Nasdaq Composite Index .IXIC rose 17.48 points , or 1.06 percent , to 1,661.51 , based on the latest available figures .\",\"\" This will be a marathon , not a sprint , \" said Jimmy D. Staton , senior vice president of operations at Dominion Virginia Power .\",\"Im hearing the same things I listened to 17 years ago .\",\"Heatley led Atlanta in scoring last season with a team-record 41 goals and 48 assists .\",\"The cables can carry far more power , and can be turned on and off like water spigots , making billing more accurate , Taub said .\"],[\"Law enforcement officials confirmed that Limbaugh was being investigated by the Palm Beach County , Fla . , state attorney 's office .\",\"One of the defining and well-studied characteristics of temperament is how people react to novelty .\",\"Bryant , 24 , has been charged with felony sexual assault .\",\"Koizumi told reporters at his office that Konoike 's remarks were \" inappropriate . \"\",\"The budget office said one provision of the Senate bill accounted for $ 40 billion of the cost .\",...,\"He was identified by his driver 's license and cell phone , Utah County Sheriff Jim Tracy said .\",\"Drewes and his friend were pulling a mischievous , late-night game of \" ding-dong-ditch \" knocking on doors or ringing doorbells and running in the Woodbury neighborhood in suburban Boca Raton .\",\"Advertising and circulation revenues for Martha Stewart Living magazine have taken a hit .\",\"\" The risk of inflation becoming undesirably low remains the predominant concern for the foreseeable future , \" the policy-setting Federal Open Market Committee said .\",\"Right now it looks to me like it 's going to be manageable , \" he said .\"],[\"History aside , the diary reveals a colorful , witty , introspective and irreverent president outspoken in his disdain for life in the White House .\",\"Russian Yuri Malenchenko , American Edward Lu and Spain 's Pedro Duque came down in their Soyuz TMA-2 capsule at 2 : 40 a.m. GMT .\",\"Meantime , federal authorities in Alabama said the passage of time has not hurt their case against Rudolph .\",\"Fasting glucose , used to measure diabetes risk , was 142 on average for those given usual care compared to 129 in the group given special treatment .\",\"The shooting victim was taken to Kings County Hospital Center , where he later died , the police said .\",...,\"Brando said that \" somebody ought to put a bullet \" through her head , according to the defense .\",\"Martin served two thirds of a five-year sentence for the manslaughter of Barras and for wounding Fearon .\",\"In a statement , the ECB said the outlook for price stability over the medium term had \" improved significantly \" since its last decision to lower interest rates in March .\",\"MSNBC.com first reported the CIA request on Friday .\",\"The 30-year bond US30YT = RR grew 1-3 / 32 for a yield of 4.30 percent , down from 4.35 percent late Wednesday .\"]]\n",
      "label: [[1,0,1,0,1,...,1,1,1,1,0],[1,0,1,0,1,...,1,1,1,0,1],[1,1,0,1,1,...,0,1,1,1,1],[0,1,0,1,0,...,1,0,1,1,0]]\n",
      "idx: [[0,1,2,3,4,...,1114,1116,1117,1118,1119],[1120,1122,1123,1125,1126,...,2213,2214,2215,2216,2217],[2218,2219,2220,2221,2222,...,3326,3327,3328,3329,3330],[3331,3332,3333,3334,3335,...,4071,4072,4073,4074,4075]],\n",
      " '_fingerprint': 'f3c20ef956d8ec2f',\n",
      " '_format_columns': None,\n",
      " '_format_kwargs': {},\n",
      " '_format_type': None,\n",
      " '_indexes': {},\n",
      " '_indices': None,\n",
      " '_info': DatasetInfo(description='GLUE, the General Language Understanding '\n",
      "                                  'Evaluation benchmark\\n'\n",
      "                                  '(https://gluebenchmark.com/) is a '\n",
      "                                  'collection of resources for training,\\n'\n",
      "                                  'evaluating, and analyzing natural language '\n",
      "                                  'understanding systems.\\n'\n",
      "                                  '\\n',\n",
      "                      citation='@inproceedings{dolan2005automatically,\\n'\n",
      "                               '  title={Automatically constructing a corpus '\n",
      "                               'of sentential paraphrases},\\n'\n",
      "                               '  author={Dolan, William B and Brockett, '\n",
      "                               'Chris},\\n'\n",
      "                               '  booktitle={Proceedings of the Third '\n",
      "                               'International Workshop on Paraphrasing '\n",
      "                               '(IWP2005)},\\n'\n",
      "                               '  year={2005}\\n'\n",
      "                               '}\\n'\n",
      "                               '@inproceedings{wang2019glue,\\n'\n",
      "                               '  title={{GLUE}: A Multi-Task Benchmark and '\n",
      "                               'Analysis Platform for Natural Language '\n",
      "                               'Understanding},\\n'\n",
      "                               '  author={Wang, Alex and Singh, Amanpreet and '\n",
      "                               'Michael, Julian and Hill, Felix and Levy, Omer '\n",
      "                               'and Bowman, Samuel R.},\\n'\n",
      "                               '  note={In the Proceedings of ICLR.},\\n'\n",
      "                               '  year={2019}\\n'\n",
      "                               '}\\n',\n",
      "                      homepage='https://www.microsoft.com/en-us/download/details.aspx?id=52398',\n",
      "                      license='',\n",
      "                      features={'idx': Value(dtype='int32', id=None),\n",
      "                                'label': ClassLabel(names=['not_equivalent',\n",
      "                                                           'equivalent'],\n",
      "                                                    id=None),\n",
      "                                'sentence1': Value(dtype='string', id=None),\n",
      "                                'sentence2': Value(dtype='string', id=None)},\n",
      "                      post_processed=None,\n",
      "                      supervised_keys=None,\n",
      "                      task_templates=None,\n",
      "                      builder_name='glue',\n",
      "                      dataset_name='glue',\n",
      "                      config_name='mrpc',\n",
      "                      version=1.0.0,\n",
      "                      splits={'test': SplitInfo(name='test',\n",
      "                                                num_bytes=442410,\n",
      "                                                num_examples=1725,\n",
      "                                                shard_lengths=None,\n",
      "                                                dataset_name='glue'),\n",
      "                              'train': SplitInfo(name='train',\n",
      "                                                 num_bytes=943843,\n",
      "                                                 num_examples=3668,\n",
      "                                                 shard_lengths=None,\n",
      "                                                 dataset_name='glue'),\n",
      "                              'validation': SplitInfo(name='validation',\n",
      "                                                      num_bytes=105879,\n",
      "                                                      num_examples=408,\n",
      "                                                      shard_lengths=None,\n",
      "                                                      dataset_name='glue')},\n",
      "                      download_checksums={'https://dl.fbaipublicfiles.com/glue/data/mrpc_dev_ids.tsv': {'checksum': None,\n",
      "                                                                                                        'num_bytes': 6222},\n",
      "                                          'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt': {'checksum': None,\n",
      "                                                                                                                            'num_bytes': 441275},\n",
      "                                          'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt': {'checksum': None,\n",
      "                                                                                                                             'num_bytes': 1047044}},\n",
      "                      download_size=1494541,\n",
      "                      post_processing_size=None,\n",
      "                      dataset_size=1492132,\n",
      "                      size_in_bytes=2986673),\n",
      " '_output_all_columns': False,\n",
      " '_split': NamedSplit('train')}\n"
     ]
    }
   ],
   "source": [
    "pprint(vars(raw_train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "794a02cd-638f-4613-872f-5ffc21e9a089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "790152ed-8d27-48b8-9375-c53d4d773a0a",
   "metadata": {},
   "source": [
    "Behind the scenes, label is of type ClassLabel, and the mapping of integers to label name is stored in the names folder. 0 corresponds to not_equivalent, and 1 corresponds to equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c2952d2-db16-415d-9e61-ff6cc9d5b739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Gyorgy Heizler , head of the local disaster unit , said the coach was carrying 38 passengers .',\n",
       " 'sentence2': 'The head of the local disaster unit , Gyorgy Heizler , said the coach driver had failed to heed red stop lights .',\n",
       " 'label': 0,\n",
       " 'idx': 15}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset[14]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "960f47e3-d69a-47f6-8bd4-ca498d3ac5d4",
   "metadata": {},
   "source": [
    "#### Preprocessing a dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bc45d13-d4d6-4e78-9a6f-1024d10c9249",
   "metadata": {},
   "source": [
    "To preprocess the dataset, we need to convert the text to numbers the model can make sense of. As you saw in the previous chapter, this is done with a tokenizer. We can feed the tokenizer one sentence or a list of sentences, so we can directly tokenize all the first sentences and all the second sentences of each pair like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f4afcdd-1a0b-4aed-a216-42461babf955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "107dbc3f-fd6d-4321-98ed-c8b11bb068a0",
   "metadata": {},
   "source": [
    "However, we canâ€™t just pass two sequences to the model and get a prediction of whether the two sentences are paraphrases or not. We need to handle the two sequences as a pair, and apply the appropriate preprocessing. Fortunately, the tokenizer can also take a pair of sequences and prepare it the way our BERT model expects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b19aef7-541d-4ce9-93d9-eb0a7e323e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85dff034-1c4e-432e-b76d-8aa4edba4a0c",
   "metadata": {},
   "source": [
    "We discussed the input_ids and attention_mask keys in Chapter 2, but we put off talking about token_type_ids. In this example, this is what tells the model which part of the input is the first sentence and which is the second sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecbd2495-2409-4358-b014-9c422f0c0a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sentence 1: {'input_ids': [101, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2132, 1997, 1996, 2334, 7071, 3131, 1010, 2056, 1996, 2873, 2001, 4755, 4229, 5467, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "tokenized sentence 2: {'input_ids': [101, 1996, 2132, 1997, 1996, 2334, 7071, 3131, 1010, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2056, 1996, 2873, 4062, 2018, 3478, 2000, 18235, 2094, 2417, 2644, 4597, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "combined tokenized sentences: {'input_ids': [101, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2132, 1997, 1996, 2334, 7071, 3131, 1010, 2056, 1996, 2873, 2001, 4755, 4229, 5467, 1012, 102, 1996, 2132, 1997, 1996, 2334, 7071, 3131, 1010, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2056, 1996, 2873, 4062, 2018, 3478, 2000, 18235, 2094, 2417, 2644, 4597, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_15 = raw_train_dataset[14]\n",
    "sentence_1 = example_15[\"sentence1\"]\n",
    "sentence_2 = example_15[\"sentence2\"]\n",
    "\n",
    "tokenized_sentence_1 = tokenizer(sentence_1)\n",
    "tokenized_sentence_2 = tokenizer(sentence_2)\n",
    "\n",
    "print(f\"tokenized sentence 1: {tokenized_sentence_1}\\n\")\n",
    "print(f\"tokenized sentence 2: {tokenized_sentence_2}\\n\")\n",
    "\n",
    "combined_tokenized = tokenizer(sentence_1, sentence_2)\n",
    "print(f\"combined tokenized sentences: {combined_tokenized}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4614ec75-8192-4ce1-a2e0-0ed04542289e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2132, 1997, 1996, 2334, 7071, 3131, 1010, 2056, 1996, 2873, 2001, 4755, 4229, 5467, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "612f3452-cd43-49f0-aba2-c9d5534b0327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'g', '##yo', '##rgy', 'he', '##iz', '##ler', ',', 'head', 'of', 'the', 'local', 'disaster', 'unit', ',', 'said', 'the', 'coach', 'was', 'carrying', '38', 'passengers', '.', '[SEP]']\n",
      "['[CLS]', 'the', 'head', 'of', 'the', 'local', 'disaster', 'unit', ',', 'g', '##yo', '##rgy', 'he', '##iz', '##ler', ',', 'said', 'the', 'coach', 'driver', 'had', 'failed', 'to', 'hee', '##d', 'red', 'stop', 'lights', '.', '[SEP]']\n",
      "['[CLS]', 'g', '##yo', '##rgy', 'he', '##iz', '##ler', ',', 'head', 'of', 'the', 'local', 'disaster', 'unit', ',', 'said', 'the', 'coach', 'was', 'carrying', '38', 'passengers', '.', '[SEP]', 'the', 'head', 'of', 'the', 'local', 'disaster', 'unit', ',', 'g', '##yo', '##rgy', 'he', '##iz', '##ler', ',', 'said', 'the', 'coach', 'driver', 'had', 'failed', 'to', 'hee', '##d', 'red', 'stop', 'lights', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokenized_sentence_1[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_sentence_2[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(combined_tokenized[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f50e6a3b-55ba-4359-8c1e-d3be25a9fa38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'sentence',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'second',\n",
       " 'one',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e47f932-e045-4d56-b743-bf196717c71b",
   "metadata": {},
   "source": [
    "As you can see, the parts of the input corresponding to [CLS] sentence1 [SEP] all have a token type ID of 0, while the other parts, corresponding to sentence2 [SEP], all have a token type ID of 1.\n",
    "\n",
    "Note that if you select a different checkpoint, you wonâ€™t necessarily have the token_type_ids in your tokenized inputs (for instance, theyâ€™re not returned if you use a DistilBERT model). They are only returned when the model will know what to do with them, because it has seen them during its pretraining.\n",
    "\n",
    "Here, BERT is pretrained with token type IDs, and on top of the masked language modeling objective we talked about in Chapter 1, it has an additional objective called next sentence prediction. The goal with this task is to model the relationship between pairs of sentences.\n",
    "\n",
    "With next sentence prediction, the model is provided pairs of sentences (with randomly masked tokens) and asked to predict whether the second sentence follows the first. To make the task non-trivial, half of the time the sentences follow each other in the original document they were extracted from, and the other half of the time the two sentences come from two different documents.\n",
    "\n",
    "In general, you donâ€™t need to worry about whether or not there are token_type_ids in your tokenized inputs: as long as you use the same checkpoint for the tokenizer and the model, everything will be fine as the tokenizer knows what to provide to its model.\n",
    "\n",
    "Now that we have seen how our tokenizer can deal with one pair of sentences, we can use it to tokenize our whole dataset: like in the previous chapter, we can feed the tokenizer a list of pairs of sentences by giving it the list of first sentences, then the list of second sentences. This is also compatible with the padding and truncation options we saw in Chapter 2. So, one way to preprocess the training dataset is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44aa4afd-9595-46dd-b2b7-ccd754f2adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbaa2db6-3cc8-466e-8907-9795ccb07360",
   "metadata": {},
   "source": [
    "This works well, but it has the disadvantage of returning a dictionary (with our keys, input_ids, attention_mask, and token_type_ids, and values that are lists of lists). It will also only work if you have enough RAM to store your whole dataset during the tokenization (whereas the datasets from the ðŸ¤— Datasets library are Apache Arrow files stored on the disk, so you only keep the samples you ask for loaded in memory).\n",
    "\n",
    "To keep the data as a dataset, we will use the Dataset.map() method. This also allows us some extra flexibility, if we need more preprocessing done than just tokenization. The map() method works by applying a function on each element of the dataset, so letâ€™s define a function that tokenizes our inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48fe0839-6735-4759-88fc-c3236cbdbb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "954a000c-e6f4-4c12-889d-c204253b82ab",
   "metadata": {},
   "source": [
    "This function takes a dictionary (like the items of our dataset) and returns a new dictionary with the keys input_ids, attention_mask, and token_type_ids. Note that it also works if the example dictionary contains several samples (each key as a list of sentences) since the tokenizer works on lists of pairs of sentences, as seen before. This will allow us to use the option batched=True in our call to map(), which will greatly speed up the tokenization. The tokenizer is backed by a tokenizer written in Rust from the ðŸ¤— Tokenizers library. This tokenizer can be very fast, but only if we give it lots of inputs at once.\n",
    "\n",
    "Note that weâ€™ve left the padding argument out in our tokenization function for now. This is because padding all the samples to the maximum length is not efficient: itâ€™s better to pad the samples when weâ€™re building a batch, as then we only need to pad to the maximum length in that batch, and not the maximum length in the entire dataset. This can save a lot of time and processing power when the inputs have very variable lengths!\n",
    "\n",
    "Here is how we apply the tokenization function on all our datasets at once. Weâ€™re using batched=True in our call to map so the function is applied to multiple elements of our dataset at once, and not on each element separately. This allows for faster preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35e4069b-b7ee-4a21-aa20-e588e3718259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1725/1725 [00:00<00:00, 3105.70 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77e331fd-7843-4be4-8607-216c2d11ed25",
   "metadata": {},
   "source": [
    "The way the ðŸ¤— Datasets library applies this processing is by adding new fields to the datasets, one for each key in the dictionary returned by the preprocessing function:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8810322-2347-4a61-989f-509ab022c56a",
   "metadata": {},
   "source": [
    "You can even use multiprocessing when applying your preprocessing function with map() by passing along a num_proc argument. We didnâ€™t do this here because the ðŸ¤— Tokenizers library already uses multiple threads to tokenize our samples faster, but if you are not using a fast tokenizer backed by this library, this could speed up your preprocessing.\n",
    "\n",
    "Our tokenize_function returns a dictionary with the keys input_ids, attention_mask, and token_type_ids, so those three fields are added to all splits of our dataset. Note that we could also have changed existing fields if our preprocessing function returned a new value for an existing key in the dataset to which we applied map().\n",
    "\n",
    "The last thing we will need to do is pad all the examples to the length of the longest element when we batch elements together â€” a technique we refer to as dynamic padding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdab2f6e-b061-4f48-86ea-592874a92650",
   "metadata": {},
   "source": [
    "##### Test adding custom column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae1a8215-84fe-4c5a-b71c-0271bf15fa23",
   "metadata": {},
   "source": [
    "Below does now work. Why it's not working will be explained in chapter 5 of the course ([The ðŸ¤— Datasets library - Hugging Face Course](https://huggingface.co/learn/nlp-course/chapter5/3?fw=pt#the-%3Ccode%3Emap()%3C/code%3E-method%E2%80%99s-superpowers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08cdeb47-d95d-4660-b292-cd0dfa3ce102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_test(example):\n",
    "    val = tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "    val[\"testtest\"] = [3]\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f861890-2d64-4f9e-8ab9-ed522a331a89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenized_datasets_test = raw_datasets.map(tokenize_function_test, batched=True)\n",
    "# tokenized_datasets_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41fb3e86-bbaa-4765-85b9-37717e648a71",
   "metadata": {},
   "source": [
    "#### Dynamic padding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3396490-50da-4cb7-a876-0ebd45b20d2e",
   "metadata": {},
   "source": [
    "The function that is responsible for putting together samples inside a batch is called a collate function. Itâ€™s an argument you can pass when you build a DataLoader, the default being a function that will just convert your samples to PyTorch tensors and concatenate them (recursively if your elements are lists, tuples, or dictionaries). This wonâ€™t be possible in our case since the inputs we have wonâ€™t all be of the same size. We have deliberately postponed the padding, to only apply it as necessary on each batch and avoid having over-long inputs with a lot of padding. This will speed up training by quite a bit, but note that if youâ€™re training on a TPU it can cause problems â€” TPUs prefer fixed shapes, even when that requires extra padding.\n",
    "\n",
    "To do this in practice, we have to define a collate function that will apply the correct amount of padding to the items of the dataset we want to batch together. Fortunately, the ðŸ¤— Transformers library provides us with such a function via DataCollatorWithPadding. It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a0a06fc-d6a1-4c81-9ed4-af464cebb7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dab73f9d-8187-43e6-9af6-d60d72a607f9",
   "metadata": {},
   "source": [
    "To test this new toy, letâ€™s grab a few samples from our training set that we would like to batch together. Here, we remove the columns idx, sentence1, and sentence2 as they wonâ€™t be needed and contain strings (and we canâ€™t create tensors with strings) and have a look at the lengths of each entry in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f1b1999-1ef3-48c4-a935-1739207e6418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "342b33e8-a5dd-480d-88ef-e76876515f3a",
   "metadata": {},
   "source": [
    "No surprise, we get samples of varying length, from 32 to 67. Dynamic padding means the samples in this batch should all be padded to a length of 67, the maximum length inside the batch. Without dynamic padding, all of the samples would have to be padded to the maximum length in the whole dataset, or the maximum length the model can accept. Letâ€™s double-check that our data_collator is dynamically padding the batch properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e68274b1-49a8-4bbc-8ca9-381253d6da8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7beb3e6-edcf-467f-ac01-3a2bb7a75942",
   "metadata": {},
   "source": [
    "Looking good! Now that weâ€™ve gone from raw text to batches our model can deal with, weâ€™re ready to fine-tune it!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a741d56-c103-449d-a3bf-5a8c25515fa8",
   "metadata": {},
   "source": [
    "### Fine-tuning a model with the Trainer API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5776c305-a6d4-4f7a-a551-3671c7e0f2d2",
   "metadata": {},
   "source": [
    "ðŸ¤— Transformers provides a Trainer class to help you fine-tune any of the pretrained models it provides on your dataset. Once youâ€™ve done all the data preprocessing work in the last section, you have just a few steps left to define the Trainer. The hardest part is likely to be preparing the environment to run Trainer.train(), as it will run very slowly on a CPU. If you donâ€™t have a GPU set up, you can get access to free GPUs or TPUs on Google Colab.\n",
    "\n",
    "The code examples below assume you have already executed the examples in the previous section. Here is a short summary recapping what you need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d4b6160-c9b8-4f28-a17d-87d123e8c405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1725/1725 [00:00<00:00, 5820.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4089d9a6-30e8-42a1-8f29-7fc37095ced6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f17b69c-d3de-485d-bfce-accb913f3313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bb4afc9-732b-4f90-9fd3-f2bf8e1567c4",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "737afac8-836c-4187-a478-620bac381ce0",
   "metadata": {},
   "source": [
    "The first step before we can define our Trainer is to define a TrainingArguments class that will contain all the hyperparameters the Trainer will use for training and evaluation. The only argument you have to provide is a directory where the trained model will be saved, as well as the checkpoints along the way. For all the rest, you can leave the defaults, which should work pretty well for a basic fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4cf7700-fbff-4f32-8ca1-82710e69f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "460c71d0-a165-47cb-8600-d719a2b9797c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dir': 'test-trainer',\n",
       " 'overwrite_output_dir': False,\n",
       " 'do_train': False,\n",
       " 'do_eval': False,\n",
       " 'do_predict': False,\n",
       " 'evaluation_strategy': <IntervalStrategy.NO: 'no'>,\n",
       " 'prediction_loss_only': False,\n",
       " 'per_device_train_batch_size': 8,\n",
       " 'per_device_eval_batch_size': 8,\n",
       " 'per_gpu_train_batch_size': None,\n",
       " 'per_gpu_eval_batch_size': None,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'eval_accumulation_steps': None,\n",
       " 'eval_delay': 0,\n",
       " 'learning_rate': 5e-05,\n",
       " 'weight_decay': 0.0,\n",
       " 'adam_beta1': 0.9,\n",
       " 'adam_beta2': 0.999,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'max_grad_norm': 1.0,\n",
       " 'num_train_epochs': 3.0,\n",
       " 'max_steps': -1,\n",
       " 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>,\n",
       " 'warmup_ratio': 0.0,\n",
       " 'warmup_steps': 0,\n",
       " 'log_level': 'passive',\n",
       " 'log_level_replica': 'warning',\n",
       " 'log_on_each_node': True,\n",
       " 'logging_dir': 'test-trainer/runs/Aug03_17-51-09_605cce1fc11b',\n",
       " 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
       " 'logging_first_step': False,\n",
       " 'logging_steps': 500,\n",
       " 'logging_nan_inf_filter': True,\n",
       " 'save_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
       " 'save_steps': 500,\n",
       " 'save_total_limit': None,\n",
       " 'save_safetensors': False,\n",
       " 'save_on_each_node': False,\n",
       " 'no_cuda': False,\n",
       " 'use_mps_device': False,\n",
       " 'seed': 42,\n",
       " 'data_seed': None,\n",
       " 'jit_mode_eval': False,\n",
       " 'use_ipex': False,\n",
       " 'bf16': False,\n",
       " 'fp16': False,\n",
       " 'fp16_opt_level': 'O1',\n",
       " 'half_precision_backend': 'auto',\n",
       " 'bf16_full_eval': False,\n",
       " 'fp16_full_eval': False,\n",
       " 'tf32': None,\n",
       " 'local_rank': 0,\n",
       " 'ddp_backend': None,\n",
       " 'tpu_num_cores': None,\n",
       " 'tpu_metrics_debug': False,\n",
       " 'debug': [],\n",
       " 'dataloader_drop_last': False,\n",
       " 'eval_steps': None,\n",
       " 'dataloader_num_workers': 0,\n",
       " 'past_index': -1,\n",
       " 'run_name': 'test-trainer',\n",
       " 'disable_tqdm': False,\n",
       " 'remove_unused_columns': True,\n",
       " 'label_names': None,\n",
       " 'load_best_model_at_end': False,\n",
       " 'metric_for_best_model': None,\n",
       " 'greater_is_better': None,\n",
       " 'ignore_data_skip': False,\n",
       " 'sharded_ddp': [],\n",
       " 'fsdp': [],\n",
       " 'fsdp_min_num_params': 0,\n",
       " 'fsdp_config': {'fsdp_min_num_params': 0,\n",
       "  'xla': False,\n",
       "  'xla_fsdp_grad_ckpt': False},\n",
       " 'fsdp_transformer_layer_cls_to_wrap': None,\n",
       " 'deepspeed': None,\n",
       " 'label_smoothing_factor': 0.0,\n",
       " 'optim': <OptimizerNames.ADAMW_HF: 'adamw_hf'>,\n",
       " 'optim_args': None,\n",
       " 'adafactor': False,\n",
       " 'group_by_length': False,\n",
       " 'length_column_name': 'length',\n",
       " 'report_to': ['tensorboard'],\n",
       " 'ddp_find_unused_parameters': None,\n",
       " 'ddp_bucket_cap_mb': None,\n",
       " 'ddp_broadcast_buffers': None,\n",
       " 'dataloader_pin_memory': True,\n",
       " 'skip_memory_metrics': True,\n",
       " 'use_legacy_prediction_loop': False,\n",
       " 'push_to_hub': False,\n",
       " 'resume_from_checkpoint': None,\n",
       " 'hub_model_id': None,\n",
       " 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>,\n",
       " 'hub_token': None,\n",
       " 'hub_private_repo': False,\n",
       " 'gradient_checkpointing': False,\n",
       " 'include_inputs_for_metrics': False,\n",
       " 'fp16_backend': 'auto',\n",
       " 'push_to_hub_model_id': None,\n",
       " 'push_to_hub_organization': None,\n",
       " 'push_to_hub_token': None,\n",
       " 'mp_parameters': '',\n",
       " 'auto_find_batch_size': False,\n",
       " 'full_determinism': False,\n",
       " 'torchdynamo': None,\n",
       " 'ray_scope': 'last',\n",
       " 'ddp_timeout': 1800,\n",
       " 'torch_compile': False,\n",
       " 'torch_compile_backend': None,\n",
       " 'torch_compile_mode': None,\n",
       " 'xpu_backend': None,\n",
       " 'distributed_state': Distributed environment: NO\n",
       " Num processes: 1\n",
       " Process index: 0\n",
       " Local process index: 0\n",
       " Device: cpu,\n",
       " '_n_gpu': 0,\n",
       " '__cached__setup_devices': device(type='cpu'),\n",
       " 'deepspeed_plugin': None}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "563744b9-9724-4301-a7a7-1a190aa1c0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d53eeea6-f3fd-48c7-999d-ade2e2f9fe3e",
   "metadata": {},
   "source": [
    "You will notice that unlike in Chapter 2, you get a warning after instantiating this pretrained model. This is because BERT has not been pretrained on classifying pairs of sentences, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been added instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head). It concludes by encouraging you to train the model, which is exactly what we are going to do now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1139ae0b-5ca8-4003-b5f7-2265930621f6",
   "metadata": {},
   "source": [
    "Once we have our model, we can define a Trainer by passing it all the objects constructed up to now â€” the model, the training_args, the training and validation datasets, our data_collator, and our tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc708fb9-3155-4825-ab46-b0d8a3db8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c4a7ade-5b64-4e0b-82da-ceb92d24c7a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': TrainingArguments(\n",
       " _n_gpu=0,\n",
       " adafactor=False,\n",
       " adam_beta1=0.9,\n",
       " adam_beta2=0.999,\n",
       " adam_epsilon=1e-08,\n",
       " auto_find_batch_size=False,\n",
       " bf16=False,\n",
       " bf16_full_eval=False,\n",
       " data_seed=None,\n",
       " dataloader_drop_last=False,\n",
       " dataloader_num_workers=0,\n",
       " dataloader_pin_memory=True,\n",
       " ddp_backend=None,\n",
       " ddp_broadcast_buffers=None,\n",
       " ddp_bucket_cap_mb=None,\n",
       " ddp_find_unused_parameters=None,\n",
       " ddp_timeout=1800,\n",
       " debug=[],\n",
       " deepspeed=None,\n",
       " disable_tqdm=False,\n",
       " do_eval=False,\n",
       " do_predict=False,\n",
       " do_train=False,\n",
       " eval_accumulation_steps=None,\n",
       " eval_delay=0,\n",
       " eval_steps=None,\n",
       " evaluation_strategy=no,\n",
       " fp16=False,\n",
       " fp16_backend=auto,\n",
       " fp16_full_eval=False,\n",
       " fp16_opt_level=O1,\n",
       " fsdp=[],\n",
       " fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
       " fsdp_min_num_params=0,\n",
       " fsdp_transformer_layer_cls_to_wrap=None,\n",
       " full_determinism=False,\n",
       " gradient_accumulation_steps=1,\n",
       " gradient_checkpointing=False,\n",
       " greater_is_better=None,\n",
       " group_by_length=False,\n",
       " half_precision_backend=auto,\n",
       " hub_model_id=None,\n",
       " hub_private_repo=False,\n",
       " hub_strategy=every_save,\n",
       " hub_token=<HUB_TOKEN>,\n",
       " ignore_data_skip=False,\n",
       " include_inputs_for_metrics=False,\n",
       " jit_mode_eval=False,\n",
       " label_names=None,\n",
       " label_smoothing_factor=0.0,\n",
       " learning_rate=5e-05,\n",
       " length_column_name=length,\n",
       " load_best_model_at_end=False,\n",
       " local_rank=0,\n",
       " log_level=passive,\n",
       " log_level_replica=warning,\n",
       " log_on_each_node=True,\n",
       " logging_dir=test-trainer/runs/Aug01_19-10-09_605cce1fc11b,\n",
       " logging_first_step=False,\n",
       " logging_nan_inf_filter=True,\n",
       " logging_steps=500,\n",
       " logging_strategy=steps,\n",
       " lr_scheduler_type=linear,\n",
       " max_grad_norm=1.0,\n",
       " max_steps=-1,\n",
       " metric_for_best_model=None,\n",
       " mp_parameters=,\n",
       " no_cuda=False,\n",
       " num_train_epochs=3.0,\n",
       " optim=adamw_hf,\n",
       " optim_args=None,\n",
       " output_dir=test-trainer,\n",
       " overwrite_output_dir=False,\n",
       " past_index=-1,\n",
       " per_device_eval_batch_size=8,\n",
       " per_device_train_batch_size=8,\n",
       " prediction_loss_only=False,\n",
       " push_to_hub=False,\n",
       " push_to_hub_model_id=None,\n",
       " push_to_hub_organization=None,\n",
       " push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       " ray_scope=last,\n",
       " remove_unused_columns=True,\n",
       " report_to=['tensorboard'],\n",
       " resume_from_checkpoint=None,\n",
       " run_name=test-trainer,\n",
       " save_on_each_node=False,\n",
       " save_safetensors=False,\n",
       " save_steps=500,\n",
       " save_strategy=steps,\n",
       " save_total_limit=None,\n",
       " seed=42,\n",
       " sharded_ddp=[],\n",
       " skip_memory_metrics=True,\n",
       " tf32=None,\n",
       " torch_compile=False,\n",
       " torch_compile_backend=None,\n",
       " torch_compile_mode=None,\n",
       " torchdynamo=None,\n",
       " tpu_metrics_debug=False,\n",
       " tpu_num_cores=None,\n",
       " use_ipex=False,\n",
       " use_legacy_prediction_loop=False,\n",
       " use_mps_device=False,\n",
       " warmup_ratio=0.0,\n",
       " warmup_steps=0,\n",
       " weight_decay=0.0,\n",
       " xpu_backend=None,\n",
       " ),\n",
       " 'hp_name': None,\n",
       " 'deepspeed': None,\n",
       " 'is_in_train': False,\n",
       " 'accelerator': <accelerate.accelerator.Accelerator at 0x7f4ef8b6eb30>,\n",
       " 'is_deepspeed_enabled': False,\n",
       " 'is_fsdp_enabled': False,\n",
       " '_memory_tracker': <transformers.trainer_utils.TrainerMemoryTracker at 0x7f5038201d20>,\n",
       " 'model_init': None,\n",
       " 'is_model_parallel': False,\n",
       " 'sharded_ddp': None,\n",
       " 'fsdp': None,\n",
       " 'place_model_on_device': True,\n",
       " 'data_collator': DataCollatorWithPadding(tokenizer=BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt'),\n",
       " 'train_dataset': Dataset({\n",
       "     features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 3668\n",
       " }),\n",
       " 'eval_dataset': Dataset({\n",
       "     features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 408\n",
       " }),\n",
       " 'tokenizer': BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),\n",
       " 'model_wrapped': BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "       (position_embeddings): Embedding(512, 768)\n",
       "       (token_type_embeddings): Embedding(2, 768)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       " ),\n",
       " 'model': BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "       (position_embeddings): Embedding(512, 768)\n",
       "       (token_type_embeddings): Embedding(2, 768)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       " ),\n",
       " 'compute_metrics': None,\n",
       " 'preprocess_logits_for_metrics': None,\n",
       " 'optimizer': None,\n",
       " 'lr_scheduler': None,\n",
       " 'callback_handler': <transformers.trainer_callback.CallbackHandler at 0x7f4efbc0fd60>,\n",
       " '_loggers_initialized': False,\n",
       " '_signature_columns': None,\n",
       " 'use_apex': False,\n",
       " 'use_cuda_amp': False,\n",
       " 'use_cpu_amp': False,\n",
       " 'do_grad_scaling': False,\n",
       " 'label_smoother': None,\n",
       " 'state': TrainerState(epoch=None, global_step=0, max_steps=0, num_train_epochs=0, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None),\n",
       " 'control': TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False),\n",
       " 'current_flos': 0,\n",
       " 'hp_search_backend': None,\n",
       " 'use_tune_checkpoints': False,\n",
       " 'label_names': ['labels'],\n",
       " 'can_return_loss': False,\n",
       " '_train_batch_size': 8,\n",
       " '_created_lr_scheduler': False}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(trainer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "402de2c9-ab7d-4311-b6e4-224e368d4059",
   "metadata": {},
   "source": [
    "Note that when you pass the tokenizer as we did here, the default data_collator used by the Trainer will be a DataCollatorWithPadding as defined previously, so you can skip the line data_collator=data_collator in this call. It was still important to show you this part of the processing in section 2!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac431512-10af-4e54-b9c6-4e1d87331789",
   "metadata": {},
   "source": [
    "To fine-tune the model on our dataset, we just have to call the train() method of our Trainer:\n",
    "**However, this takes too long time!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac4ed1b9-7618-4a8c-8de9-7b924bef101c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='271' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 271/1377 13:10 < 54:08, 0.34 it/s, Epoch 0.59/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1809\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2665\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2664\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2665\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/accelerator.py:1853\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1853\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d02294d-2421-4a44-877d-f39b699d29c8",
   "metadata": {},
   "source": [
    "This will start the fine-tuning (which should take a couple of minutes on a GPU) and report the training loss every 500 steps. It wonâ€™t, however, tell you how well (or badly) your model is performing. This is because:\n",
    "\n",
    "1. We didnâ€™t tell the Trainer to evaluate during training by setting evaluation_strategy to either \"steps\" (evaluate every eval_steps) or \"epoch\" (evaluate at the end of each epoch).\n",
    "2. We didnâ€™t provide the Trainer with a compute_metrics() function to calculate a metric during said evaluation (otherwise the evaluation would just have printed the loss, which is not a very intuitive number).r)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa57547b-4505-42e3-86b6-e46e61677b53",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3e80276-d883-4c2d-b713-4cb018f780ec",
   "metadata": {},
   "source": [
    "Letâ€™s see how we can build a useful compute_metrics() function and use it the next time we train. The function must take an EvalPrediction object (which is a named tuple with a predictions field and a label_ids field) and will return a dictionary mapping strings to floats (the strings being the names of the metrics returned, and the floats their values). To get some predictions from our model, we can use the Trainer.predict() command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2763ffd0-e45d-483f-b336-8e54b6b71dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed558a34-76a5-4808-a7f0-a659ac7c38ef",
   "metadata": {},
   "source": [
    "The output of the predict() method is another named tuple with three fields: predictions, label_ids, and metrics. The metrics field will just contain the loss on the dataset passed, as well as some time metrics (how long it took to predict, in total and on average). Once we complete our compute_metrics() function and pass it to the Trainer, that field will also contain the metrics returned by compute_metrics()."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62748667-6ddf-462f-9223-da3dc4d9fa32",
   "metadata": {},
   "source": [
    "As you can see, predictions is a two-dimensional array with shape 408 x 2 (408 being the number of elements in the dataset we used). Those are the logits for each element of the dataset we passed to predict() (as you saw in the previous chapter, all Transformer models return logits). To transform them into predictions that we can compare to our labels, we need to take the index with the maximum value on the second axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213550f-611f-41b0-bd89-1ca40992c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f4bec28-3564-49b2-a8d0-c0017c0f1d3f",
   "metadata": {},
   "source": [
    "We can now compare those preds to the labels. To build our compute_metric() function, we will rely on the metrics from the ðŸ¤— Evaluate library. We can load the metrics associated with the MRPC dataset as easily as we loaded the dataset, this time with the evaluate.load() function. The object returned has a compute() method we can use to do the metric calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d2f33-b635-44e9-8f64-338d90c00957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0f3c4bd-c77d-4089-9346-2272b0fbbd89",
   "metadata": {},
   "source": [
    "The exact results you get may vary, as the random initialization of the model head might change the metrics it achieved. Here, we can see our model has an accuracy of 85.78% on the validation set and an F1 score of 89.97. Those are the two metrics used to evaluate results on the MRPC dataset for the GLUE benchmark. The table in the BERT paper reported an F1 score of 88.9 for the base model. That was the uncased model while we are currently using the cased model, which explains the better result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45b86994-576b-46ed-a175-e031e59d82e1",
   "metadata": {},
   "source": [
    "Wrapping everything together, we get our compute_metrics() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6828ca37-9301-4161-a6c6-c7ea20b6376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a423b72-82f2-4e87-9a18-274a3cf87d40",
   "metadata": {},
   "source": [
    "And to see it used in action to report metrics at the end of each epoch, here is how we define a new Trainer with this compute_metrics() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f009ec82-1a61-47f0-b3e5-61905b62ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4316dbcb-84c8-4252-9b49-ee861d0dd56e",
   "metadata": {},
   "source": [
    "Note that we create a new TrainingArguments with its evaluation_strategy set to \"epoch\" and a new model â€” otherwise, we would just be continuing the training of the model we have already trained. To launch a new training run, we execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ecbff-1570-45c4-8bf1-f6e8f7b45b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d28099c-81b6-4adb-ab3a-397f4bd8948a",
   "metadata": {},
   "source": [
    "This time, it will report the validation loss and metrics at the end of each epoch on top of the training loss. Again, the exact accuracy/F1 score you reach might be a bit different from what we found, because of the random head initialization of the model, but it should be in the same ballpark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f8217c5-5d89-4307-bf12-ae52382ddb4d",
   "metadata": {},
   "source": [
    "The Trainer will work out of the box on multiple GPUs or TPUs and provides lots of options, like mixed-precision training (use fp16 = True in your training arguments). We will go over everything it supports in Chapter 10."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7bfb8ce-e40f-4819-b035-7e7fc88b05b4",
   "metadata": {},
   "source": [
    "This concludes the introduction to fine-tuning using the Trainer API. An example of doing this for most common NLP tasks will be given in Chapter 7, but for now letâ€™s look at how to do the same thing in pure PyTorch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b2c9f98-8632-45e0-82d0-a110937b1910",
   "metadata": {},
   "source": [
    "### A full training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee2dd657-79d6-4c4c-b240-807cc8070a6a",
   "metadata": {},
   "source": [
    "Now weâ€™ll see how to achieve the same results as we did in the last section without using the Trainer class. Again, we assume you have done the data processing in section 2. Here is a short summary covering everything you will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5dd170f8-9059-4611-b291-bcbd9a72329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e2195c2-5a48-4b5e-9e6e-61b3d26f79c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b2fd189-39f0-47a6-a3de-7364fdfc93ff",
   "metadata": {},
   "source": [
    "#### Prepare for training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3b821fd-e6e9-4457-96fd-7e7fe1fe2e75",
   "metadata": {},
   "source": [
    "Before actually writing our training loop, we will need to define a few objects. The first ones are the dataloaders we will use to iterate over batches. But before we can define those dataloaders, we need to apply a bit of postprocessing to our tokenized_datasets, to take care of some things that the Trainer did for us automatically. Specifically, we need to:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a02f5bb-47f3-4b5f-9e7b-c4f1f0dc26ab",
   "metadata": {},
   "source": [
    "* Remove the columns corresponding to values the model does not expect (like the sentence1 and sentence2 columns).\n",
    "* \n",
    "Rename the column label to labels (because the model expects the argument to be named labels)\n",
    "* \n",
    "Set the format of the datasets so they return PyTorch tensors instead of lists."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cf4934a-504c-4ef2-80ba-5498088ad545",
   "metadata": {},
   "source": [
    "Our tokenized_datasets has one method for each of those steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8f28872-bf22-433c-ab7f-ab2ad09146c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "840d8e87-5844-4306-916e-f6bae9f7ab83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe013ce2-e2fe-4789-8c57-925608ffaa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e30f3d4-cbc7-4ce3-9803-911b13151e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([8]),\n",
       " 'input_ids': torch.Size([8, 72]),\n",
       " 'token_type_ids': torch.Size([8, 72]),\n",
       " 'attention_mask': torch.Size([8, 72])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "286fb38e-9f44-48c7-90a0-f391823f325a",
   "metadata": {},
   "source": [
    "Note that the actual shapes will probably be slightly different for you since we set shuffle=True for the training dataloader and we are padding to the maximum length inside the batch.\n",
    "\n",
    "Now that weâ€™re completely finished with data preprocessing (a satisfying yet elusive goal for any ML practitioner), letâ€™s turn to the model. We instantiate it exactly as we did in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49c5ded1-d80b-4b42-893c-f8e714b59d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb128cdd-0da3-485d-b321-de95b3060ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6494, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd59d24b-ae78-404e-8a12-094b55641311",
   "metadata": {},
   "source": [
    "Weâ€™re almost ready to write our training loop! Weâ€™re just missing two things: an optimizer and a learning rate scheduler. Since we are trying to replicate what the Trainer was doing by hand, we will use the same defaults. The optimizer used by the Trainer is AdamW, which is the same as Adam, but with a twist for weight decay regularization (see â€œDecoupled Weight Decay Regularizationâ€ by Ilya Loshchilov and Frank Hutter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb6bd707-9df7-484a-b370-7b54faa4b897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c237c7ee-44ad-4514-ab5d-733afd2b681c",
   "metadata": {},
   "source": [
    "Finally, the learning rate scheduler used by default is just a linear decay from the maximum value (5e-5) to 0. To properly define it, we need to know the number of training steps we will take, which is the number of epochs we want to run multiplied by the number of training batches (which is the length of our training dataloader). The Trainer uses three epochs by default, so we will follow that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69f1dffa-4006-4915-a183-b5bb1edb0042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1377\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50f8de01-7377-4275-a5a7-04ae50eda661",
   "metadata": {},
   "source": [
    "The training loop\n",
    "One last thing: we will want to use the GPU if we have access to one (on a CPU, training might take several hours instead of a couple of minutes). To do this, we define a device we will put our model and our batches on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88108b66-a0d9-4cea-8d8f-066f9ae2a4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "444cf76d-07f0-45fe-a937-a673e14efc63",
   "metadata": {},
   "source": [
    "We are now ready to train! To get some sense of when training will be finished, we add a progress bar over our number of training steps, using the tqdm library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f860d68-4325-4f75-a8ee-614a0c29bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78621f81-5a91-468f-81b7-00245df38088",
   "metadata": {},
   "source": [
    "You can see that the core of the training loop looks a lot like the one in the introduction. We didnâ€™t ask for any reporting, so this training loop will not tell us anything about how the model fares. We need to add an evaluation loop for that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc267a8f-3118-4ef7-8a8f-78daa013e8f4",
   "metadata": {},
   "source": [
    "#### The evaluation loop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d44cc1b0-463f-4fe1-8510-b9761b06c589",
   "metadata": {},
   "source": [
    "As we did earlier, we will use a metric provided by the ðŸ¤— Evaluate library. Weâ€™ve already seen the metric.compute() method, but metrics can actually accumulate batches for us as we go over the prediction loop with the method add_batch(). Once we have accumulated all the batches, we can get the final result with metric.compute(). Hereâ€™s how to implement all of this in an evaluation loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a174c60f-e7c8-46fb-9eec-3f54c91da791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4198ea05-daa4-47f7-973f-67cb5eb78778",
   "metadata": {},
   "source": [
    "Again, your results will be slightly different because of the randomness in the model head initialization and the data shuffling, but they should be in the same ballpark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbdd3813",
   "metadata": {},
   "source": [
    "### Supercharge your training loop with ðŸ¤— Accelerate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "192c80b1",
   "metadata": {},
   "source": [
    "The training loop we defined earlier works fine on a single CPU or GPU. But using the ðŸ¤— Accelerate library, with just a few adjustments we can enable distributed training on multiple GPUs or TPUs. Starting from the creation of the training and validation dataloaders, here is what our manual training loop looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf20382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18dabebf",
   "metadata": {},
   "source": [
    "And here are the changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f86617b1",
   "metadata": {},
   "source": [
    "The first line to add is the import line. The second line instantiates an Accelerator object that will look at the environment and initialize the proper distributed setup. ðŸ¤— Accelerate handles the device placement for you, so you can remove the lines that put the model on the device (or, if you prefer, change them to use accelerator.device instead of device).\n",
    "\n",
    "Then the main bulk of the work is done in the line that sends the dataloaders, the model, and the optimizer to accelerator.prepare(). This will wrap those objects in the proper container to make sure your distributed training works as intended. The remaining changes to make are removing the line that puts the batch on the device (again, if you want to keep this you can just change it to use accelerator.device) and replacing loss.backward() with accelerator.backward(loss)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9491446",
   "metadata": {},
   "source": [
    "âš ï¸ In order to benefit from the speed-up offered by Cloud TPUs, we recommend padding your samples to a fixed length with the `padding=\"max_length\"` and `max_length` arguments of the tokenizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acb70897",
   "metadata": {},
   "source": [
    "If youâ€™d like to copy and paste it to play around, hereâ€™s what the complete training loop looks like with ðŸ¤— Accelerate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e8757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "train_dl, eval_dl, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dl:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af7054b2",
   "metadata": {},
   "source": [
    "Putting this in a train.py script will make that script runnable on any kind of distributed setup. To try it out in your distributed setup, run the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a062e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b888438",
   "metadata": {},
   "source": [
    "which will prompt you to answer a few questions and dump your answers in a configuration file used by this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b45ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate launch train.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec003084",
   "metadata": {},
   "source": [
    "which will launch the distributed training.\n",
    "\n",
    "If you want to try this in a Notebook (for instance, to test it with TPUs on Colab), just paste the code in a training_function() and run a last cell with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
