{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "984d4ffc",
   "metadata": {},
   "source": [
    "# Lesson 2: Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d21427",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06744c6f",
   "metadata": {},
   "source": [
    "* In this classroom, the libraries have been already installed for you.\n",
    "* If you would like to run this code on your own machine, you need to install the following:\n",
    "    ```\n",
    "    !pip install ultralytics torch\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150a0374",
   "metadata": {},
   "source": [
    "### Load the sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c6b40-97fb-4f08-a8e0-4054e29d42e7",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "raw_image = Image.open(\"dogs.jpg\")\n",
    "raw_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9b59e0",
   "metadata": {},
   "source": [
    ">Note: the images referenced in this notebook have already been uploaded to the Jupyter directory, in this classroom, for your convenience. For further details, please refer to the **Appendix** section located at the end of the lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d2d941",
   "metadata": {},
   "source": [
    "* Resize the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517328e-612e-4320-8d09-e15cbf37daa7",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from utils import resize_image\n",
    "resized_image = resize_image(raw_image, input_size=1024)\n",
    "resized_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba1103c",
   "metadata": {},
   "source": [
    "### Import and prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0714f3-c394-49c8-8673-5f005a8ce49f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48635d4",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7496aabd",
   "metadata": {},
   "source": [
    "Info about [torch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ecc9ea-b834-4ad9-adb2-7d0c853d8d73",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "model = YOLO('./FastSAM-x.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893be50b",
   "metadata": {},
   "source": [
    "Info about ['FastSAM'](https://docs.ultralytics.com/models/fast-sam/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f43d87",
   "metadata": {},
   "source": [
    "### Use the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3373596e",
   "metadata": {},
   "source": [
    ">Note: ```utils``` is an additional file containing the methods that have been already developed for you to be used in this classroom. \n",
    "For further details, please refer to the **Appendix** section located at the end of the lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a83331-2775-4d0d-abb1-53648e1e6a64",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import show_points_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278e50f-5b08-4f14-b62b-3b6c670096da",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Define the coordinates for the point in the image\n",
    "# [x_axis, y_axis]\n",
    "input_points = [ [350, 450 ] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e41f2-744a-4971-8610-0287a329341c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "input_labels = [1] # positive point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e724e32-4f35-4184-9dfc-4d3f8cafdc3c",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Function written in the utils file\n",
    "show_points_on_image(resized_image, input_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532bc8d-ff31-4de3-9d71-98ed494933d5",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Run the model\n",
    "results = model(resized_image, device=device, retina_masks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7761784b",
   "metadata": {},
   "source": [
    "* Filter the mask based on the point defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d79f594-0347-4cf9-b390-8223b553ed17",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import format_results, point_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d88f329-6ae2-41ee-8128-6fe9b739b0d2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "results = format_results(results[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb6dc8-8153-4eee-9e2d-9f3d330c2e52",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Generate the masks\n",
    "masks, _ = point_prompt(results, input_points, input_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc12d9-6d4c-48b5-b59f-a096530199bc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import show_masks_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d3f983-34c8-4382-9c00-5094693f26b3",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Visualize the generated masks\n",
    "show_masks_on_image(resized_image, [masks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7308c2",
   "metadata": {},
   "source": [
    "* Define 'semantic masks' - two points to be masked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bad722-c4ce-400e-bc2a-989842bd6e5e",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Specify two points in the same model\n",
    "# [x_axis, y_axis]\n",
    "input_points = [ [350, 450], [620, 450] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1136850-5661-4136-baec-58d00fbc4f74",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Specify both points as \"positive prompt\"\n",
    "input_labels = [1 , 1] # both positive points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0200106e-b203-4f1b-9ad0-d21a2efcc60d",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Visualize the points defined before\n",
    "show_points_on_image(resized_image, input_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e97e7bf-e286-490d-98b0-c4eec087000e",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Run the model\n",
    "results = model(resized_image, device=device, retina_masks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60fb80f-10c0-4ba9-8734-c06ae9409017",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "results = format_results(results[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b0b4c-49d0-4392-b479-2c8bf03b767c",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Generate the masks\n",
    "masks, _ = point_prompt(results, input_points, input_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2217634e-8f79-42ae-97ca-4b55021bf759",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Visualize the generated masks\n",
    "show_masks_on_image(resized_image, [masks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304fc0e",
   "metadata": {},
   "source": [
    ">Note: Please note that the results obtained from running this notebook may vary slightly from those demonstrated by the instructor in the video. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec6f9b8",
   "metadata": {},
   "source": [
    "* Identify subsections of the image by adding a **negative prompt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c9d1d-27aa-4225-a78e-1b2f74b1eb46",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Define the coordinates for the points to be masked\n",
    "# [x_axis, y_axis]\n",
    "input_points = [ [350, 450], [400, 300]  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de18e03-8de4-404c-be78-a753e411435c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "input_labels = [1, 0] # positive prompt, negative prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1ca7a-22f1-4b75-8b1c-e232614d8139",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Visualize the points defined before\n",
    "show_points_on_image(resized_image, input_points, input_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce6bd8c",
   "metadata": {},
   "source": [
    ">Note: From the image above, the red star indicates the negative prompt and the green star the positive prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6f408a-3b66-4b17-91ab-a03c5d77f57c",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Run the model\n",
    "results = model(resized_image, device=device, retina_masks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734d092-37ef-4570-8d32-7cdfc475b110",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "results = format_results(results[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf12e0f-142c-4c24-bd2f-6c5bbb976c21",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Generate the masks\n",
    "masks, _ = point_prompt(results, input_points, input_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c2b01b-dd4c-426e-9b0e-edd6330be670",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Visualize the generated masks\n",
    "show_masks_on_image(resized_image, [masks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a4807",
   "metadata": {},
   "source": [
    ">Note: From the image above, only the jacket, from the dog in the left, was segmented, so, it is following the command given by the positive prompt!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae4e7a2",
   "metadata": {},
   "source": [
    "### Prompting with bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a0557-42b8-4d3d-99f7-96ba22860b0c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import box_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8a106-4035-4e8f-a0b2-3cc65ae4afed",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Set the coordinates for the box\n",
    "# [xmin, ymin, xmax, ymax]\n",
    "input_boxes = [530, 180, 780, 600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b3975",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import show_boxes_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b80ee-c1a8-440c-b120-0adc416e2144",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Visualize the bounding box defined with the coordinates above\n",
    "show_boxes_on_image(resized_image, [input_boxes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124071f3",
   "metadata": {},
   "source": [
    "* Now, try to isolate the mask from the total output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b7378-97b7-42fe-b2d0-b52d30945311",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import box_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0324ec8-d2ae-46ea-9619-453471cca9db",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "results = model(resized_image, device=device, retina_masks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62055a75-7c2a-4b99-b473-2788d6be964e",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "#Generate the masks\n",
    "masks = results[0].masks.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52345d-3887-4da6-88bc-54303d4ef98c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95c252-8e98-467c-a73c-9fcdd2dff0cc",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Convert to True/False booleans\n",
    "masks = masks > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c0589c-d983-48b3-a69b-55ebd69ade8b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dd714c-1f3b-4532-a1d9-3361f53ce829",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "masks, _ = box_prompt(masks, input_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a8bb7-c2f4-4e1e-a653-734bb97db5f2",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Visualize the masks\n",
    "show_masks_on_image(resized_image, [masks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f8ae5-83d1-4695-a89b-f39fa62e4f3a",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Print the segmentation mask, but in its raw format\n",
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb1919a-59d5-43e8-a7ef-ac6bd12bb5cf",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# To visualize, import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d022c91-c2aa-4b93-8676-3d4739293c5f",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Plot the binary mask as an image\n",
    "plt.imshow(masks, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c368ba",
   "metadata": {},
   "source": [
    "### Try yourself! \n",
    "Try the image segmentation explained before with your own images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e00a69-72ff-45c2-a6da-1cb158c118f1",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "# Start opening images, we have sample images, for example: younes.png\n",
    "# The image younes.png is already uploaded in this classroom\n",
    "raw_image = Image.open('istockphoto.jpeg')\n",
    "raw_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53952bf6",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Resize image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e6885",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Define the coordinates for the point: [x_axis, y_axis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57087862",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Define the positive or negative prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d53ff",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# show_points_on_image(resized_image, input_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f774f",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "\n",
    "* For more on how to use Comet for experiment tracking, check out this [Quickstart Guide](https://colab.research.google.com/drive/1jj9BgsFApkqnpPMLCHSDH-5MoL_bjvYq?usp=sharing) and the [Comet Docs](https://www.comet.com/docs/v2/).\n",
    "* This course was based off a set of two blog articles from Comet. Explore them here for more on how to use newer versions of Stable Diffusion in this pipeline, additional tricks to improve your inpainting results, and a breakdown of the pipeline architecture:\n",
    "  * [SAM + Stable Diffusion for Text-to-Image Inpainting](https://www.comet.com/site/blog/sam-stable-diffusion-for-text-to-image-inpainting/)\n",
    "  * [Image Inpainting for SDXL 1.0 Base Model + Refiner](https://www.comet.com/site/blog/image-inpainting-for-sdxl-1-0-base-refiner/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df77d7f",
   "metadata": {},
   "source": [
    "# Lesson 3: Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d1b37f",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff2703b",
   "metadata": {},
   "source": [
    "* In this classroom, the libraries have been already installed for you.\n",
    "* If you would like to run this code on your own machine, you need to install the following:\n",
    "    ```\n",
    "    !pip install comet_ml --quiet\n",
    "    !pip install transformers\n",
    "    !pip install ultralytics torch\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b83e5",
   "metadata": {},
   "source": [
    "### Set up Comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29573cec-e264-41d6-b0b0-6f94d28feecb",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import comet_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e527f77",
   "metadata": {},
   "source": [
    "Info about ['Comet'](https://www.comet.com/site/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5edef5-2a8e-42b6-91d3-28b1da97e765",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "comet_ml.init(anonymous=True, project_name=\"3: OWL-ViT + SAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9678e1-4732-4b4e-b8fc-481d5b993afa",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "exp = comet_ml.Experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc143578",
   "metadata": {},
   "source": [
    "### Load the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c4aafd-55dc-4fb1-981a-a8f984ad3f36",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# To display the image\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d985060-1b7c-48d0-afd6-367b3bde7e74",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "logged_artifact = exp.get_artifact(\"L3-data\", \"anmorgan24\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef86770",
   "metadata": {},
   "source": [
    ">Note: the images referenced in this notebook have already been uploaded to the Jupyter directory, in this classroom, for your convenience. For further details, please refer to the **Appendix** section located at the end of the lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51236f01-5e57-4b3d-a3da-3bdb31af9a77",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "local_artifact = logged_artifact.download(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f65a28-3974-4b5e-8182-44537aef0a32",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Display the images\n",
    "raw_image = Image.open(\"L3_data/dogs.jpg\")\n",
    "raw_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019139e8-11d1-4b37-b258-9eafffe2f5fd",
   "metadata": {},
   "source": [
    "### Get bounding boxes with OWL-ViT object detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2dab86",
   "metadata": {},
   "source": [
    ">Note: `pipeline` is already installed for you in this classroom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c886b6d-dc91-4f2b-93e0-5a3178df04f2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d80f5ee-6b24-47f1-8f65-74dafe0de335",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "OWL_checkpoint = \"google/owlvit-base-patch32\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120a550b",
   "metadata": {},
   "source": [
    "Info about ['google/owlvit-base-patch32'](https://huggingface.co/google/owlvit-base-patch32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0047c123",
   "metadata": {},
   "source": [
    "* Build the pipeline for the detector model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cc2939-79d6-4f0d-8d7d-c462c5e27e55",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "detector = pipeline(\n",
    "    model= OWL_checkpoint,\n",
    "    task=\"zero-shot-object-detection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a3cc8-697a-4dab-bfaa-9e9428dd39e0",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# What you want to identify in the image\n",
    "text_prompt = \"dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00737c4e-4a35-4e13-8c61-cddc5a239c83",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "output = detector(\n",
    "    raw_image,\n",
    "    candidate_labels = [text_prompt]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b82fe-a74a-4479-9542-7d4ffe0982d3",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Print the output to identify the bounding boxes detected\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382b6a1",
   "metadata": {},
   "source": [
    "* Use the **util**'s function to prompt boxes in top of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7074fe92",
   "metadata": {},
   "source": [
    ">Note: ```utils``` is an additional file containing the methods that have been already developed for you to be used in this classroom. \n",
    "For further details, please refer to the **Appendix** section located at the end of the lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26cea3b-65fd-412e-8ce6-a9c92c0ff1a3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import preprocess_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dc1018-df22-4237-9739-594003c7ea2b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "input_scores, input_labels, input_boxes = preprocess_outputs(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df173e1c-0e61-47a6-8ffd-a4729b6770e3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import show_boxes_and_labels_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a715c552-0c39-4766-96d5-9125ffaf8a60",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# Show the image with the bounding boxes\n",
    "show_boxes_and_labels_on_image(\n",
    "    raw_image,\n",
    "    input_boxes[0],\n",
    "    input_labels,\n",
    "    input_scores\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd750f67-a674-44b2-9c2e-d7d734ddf057",
   "metadata": {},
   "source": [
    "### Get segmentation masks using Mobile SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a4509-ffdd-4792-a6cb-0cbd1b469ee3",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Load the SAM model from the imported ultralytics library\n",
    "from ultralytics import SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dcdcb9-2a97-486c-a4c3-9299f166bf12",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "SAM_version = \"mobile_sam.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f09dd",
   "metadata": {},
   "source": [
    "Info about [mobile_sam.pt](https://docs.ultralytics.com/models/mobile-sam/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c536491-2a1d-43c0-b800-db60611f3e77",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model = SAM(SAM_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32055e82",
   "metadata": {},
   "source": [
    "* Generate an array using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebfce2-30ef-41c3-a90c-ba8837bbe9bf",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924c0f1-343b-4e5a-846d-ec9b3071e561",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "labels = np.repeat(1, len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a026253-1288-4c5a-8d40-59c159a3e9d5",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Print the number of bounding boxes\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64130f-25fb-4408-84d6-34842fad1adb",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "result = model.predict(\n",
    "    raw_image,\n",
    "    bboxes=input_boxes[0],\n",
    "    labels=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d3de5-46ed-4a54-b611-e75061d2bb3a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90de4c0-d0de-44a1-be51-335b30f7ad59",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "masks = result[0].masks.data\n",
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ed1bf-aa03-44cf-a3b3-6535d155ebc5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import show_masks_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba93891-f826-4c8d-9447-6ac061141ada",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# Visualize the masks\n",
    "show_masks_on_image(\n",
    "    raw_image,\n",
    "    masks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9517b557",
   "metadata": {},
   "source": [
    ">Note: Please note that the results obtained from running this notebook may vary slightly from those demonstrated by the instructor in the video. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5ff161-f446-4aba-b4c1-d2163698289f",
   "metadata": {},
   "source": [
    "### Image Editing: blur out faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5d111",
   "metadata": {},
   "source": [
    "* Load the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e28c3-5290-48c3-80b0-d56a15fd8bdf",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d6309-2a05-4a85-9ed3-881cd0d95451",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "image_path = \"L3_data/people.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830fcb66",
   "metadata": {},
   "source": [
    ">Note: the images referenced in this notebook have already been uploaded to the Jupyter directory, in this classroom, for your convenience. For further details, please refer to the **Appendix** section located at the end of the lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de06864-3e4b-4bc5-8c43-a2a424067f24",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "raw_image = Image.open(image_path)\n",
    "raw_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd0fca-fd4b-4113-8324-043e662c426e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "raw_image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5a571a",
   "metadata": {},
   "source": [
    "* Resize the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a0d63b-38cc-434c-aca9-21fe34f8cdb2",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "# Width transformation\n",
    "mywidth = 600\n",
    "wpercent = mywidth / float(raw_image.size[0])\n",
    "wpercent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e95ff6-4cc2-4969-be4a-984e1d8972a2",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Height transformation\n",
    "hsize = int( float(raw_image.size[1]) * wpercent )\n",
    "hsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c70705-1b17-4ff8-b879-c83a0c737816",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Resize\n",
    "raw_image = raw_image.resize([mywidth, hsize])\n",
    "raw_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4d30d1-93bd-4989-97e6-8e98e4e9317a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "raw_image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ea037-4b8d-4566-9840-fca138475d1d",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Save the resized image\n",
    "image_path_resized = \"people_resized.jpg\"\n",
    "raw_image.save(image_path_resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8d963-6e65-4bee-9b6a-858a91759397",
   "metadata": {},
   "source": [
    "### Detect faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5957593-af98-4b2c-875c-f0c3a0d87310",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "candidate_labels = [\"human face\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5d2b3d-154e-4e66-8f3f-d9c51e350ed8",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Define a new Comet experiment for this new pipeline\n",
    "exp = comet_ml.Experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ddd2c-689f-412a-ac34-02ffb0ca4a11",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# Log raw image to the experiment\n",
    "_ = exp.log_image(\n",
    "    raw_image,\n",
    "    name = \"Raw image\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d6837-2b26-448c-a759-dc78ec41690a",
   "metadata": {},
   "source": [
    "* Create bounding boxes with OWL-ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17b064b-45ef-4fed-9355-0c757bfa4b8b",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# Apply detector model to the raw image\n",
    "output = detector(\n",
    "    raw_image,\n",
    "    candidate_labels=candidate_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec3c98-ac31-444e-8e07-8a5aaee6fae7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "input_scores, input_labels, input_boxes = preprocess_outputs(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae430e-fbbf-43c1-b375-143ddbefcf45",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Print values of the bounding boxes identified\n",
    "input_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a77851-67f4-459a-a970-edd6ae1cd75f",
   "metadata": {},
   "source": [
    "#### Log the images and bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707708fc-d786-49e7-8c17-29f9cef7d8a6",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"OWL prompt\": candidate_labels,\n",
    "    \"SAM version\": SAM_version,\n",
    "    \"OWL Version\": OWL_checkpoint\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb17873-319d-4e59-8bd6-1dc538250211",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import make_bbox_annots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33450381-477c-429a-81ff-47826df5a5bf",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "annotations = make_bbox_annots(\n",
    "    input_scores,\n",
    "    input_labels,\n",
    "    input_boxes,\n",
    "    metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc1832f-4664-4a52-a850-11cacbd02360",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "_ = exp.log_image(\n",
    "    raw_image,\n",
    "    annotations= annotations,\n",
    "    metadata=metadata,\n",
    "    name= \"OWL output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788de380-6ca9-4f15-9b6d-a0f4abdf8a41",
   "metadata": {},
   "source": [
    "### Segmentation masks using SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258417b7-2ca5-4260-9a9e-dc75685412ce",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "result = model.predict(\n",
    "    image_path_resized,\n",
    "    bboxes=input_boxes[0],\n",
    "    labels=np.repeat(1, len(input_boxes[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023b770d-4066-4525-9cca-4a64fb854c43",
   "metadata": {},
   "source": [
    "### Blur entire image first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13cb88-0d60-4eac-99d6-2da3b10807ce",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from PIL.ImageFilter import GaussianBlur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ecce2e-619a-4fb4-b8a4-eaac99ffc5c8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "blurred_img = raw_image.filter(GaussianBlur(radius=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49be80",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "blurred_img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8fdee9-5982-4f94-bbb2-18e79de00143",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "masks = result[0].masks.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684fafa9-0a76-4200-8a29-fdcfad821e36",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Obtain only a single mask\n",
    "total_mask = np.zeros(masks[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf8b958",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "for mask in masks:\n",
    "    total_mask = np.add(total_mask,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812df370-3d51-44e2-8366-d3d68f15cf28",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "output = np.where(\n",
    "    np.expand_dims(total_mask != 0, axis=2),\n",
    "    blurred_img,\n",
    "    raw_image\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fa9908-ceaf-4c52-8e87-d15daa0aeafb",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bf0f05-ba1a-47c2-a7a1-be0d3928ad76",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Print image with faces blured\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4638f0e3",
   "metadata": {},
   "source": [
    "* Log this image in the **Comet** platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af3bfab-2a10-4019-b420-87c0d791f487",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"OWL prompt\": candidate_labels,\n",
    "    \"SAM version\": SAM_version,\n",
    "    \"OWL version\": OWL_checkpoint\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc12097-032b-42af-a0ab-982f052fb8e7",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "_ = exp.log_image(\n",
    "    output,\n",
    "    name=\"Blurred masks\",\n",
    "    metadata = metadata,\n",
    "    annotations=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f18ad1-2785-4b1c-b1a1-371260de8e7d",
   "metadata": {},
   "source": [
    "### Blur just faces of those not wearing sunglasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6191d84-003e-4cc9-b2f8-7b5fd73419c8",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# New label\n",
    "candidate_labels = [\"a person without sunglasses\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5538c58b",
   "metadata": {},
   "source": [
    "* Re-run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8824b8a-8b4a-4746-9448-c4a68f248643",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "exp = comet_ml.Experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fd21ba-c3de-467a-9907-bbbdc4df98ef",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "_ = exp.log_image(raw_image, name=\"Raw image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ce773-2bc5-4200-843a-1fc4672098f5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "output = detector(raw_image, candidate_labels=candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da9c1f-a5d0-4568-82f9-ec61396f3602",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "input_scores, input_labels, input_boxes = preprocess_outputs(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f96bf-3702-413e-a93d-8e75611a94a9",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Print the number of bounding boxes\n",
    "input_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136b60fc",
   "metadata": {},
   "source": [
    "* Explore in the **Comet** platform what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fe8524-0771-4006-857b-880e66becec2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import make_bbox_annots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b733165-2c43-4f32-ab83-e377d618739d",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"OWL prompt\": candidate_labels,\n",
    "    \"SAM version\": SAM_version,\n",
    "    \"OWL version\": OWL_checkpoint,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86860adc-bac5-4ccc-b1f7-6172d006bde4",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "annotations = make_bbox_annots(\n",
    "    input_scores,\n",
    "    input_labels,\n",
    "    input_boxes,\n",
    "    metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a8c342-b343-4a23-ad29-31d0be1ec8c7",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "_ = exp.log_image(\n",
    "    raw_image,\n",
    "    annotations=annotations,\n",
    "    metadata=metadata,\n",
    "    name=\"OWL output no sunglasses\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a73b5f-9ec4-4c07-bf57-5fc30608a3d2",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "result = model.predict(\n",
    "    image_path_resized,\n",
    "    bboxes=input_boxes[0],\n",
    "    labels=np.repeat(1, len(input_boxes[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9902cc39-ed82-4c9d-89c6-6795db2ab15e",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from PIL.ImageFilter import GaussianBlur\n",
    "blurred_img = raw_image.filter(GaussianBlur(radius=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f3f04-de9b-469c-939f-c42f98b5bf4b",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "masks = result[0].masks.data.cpu().numpy()\n",
    "\n",
    "total_mask = np.zeros(masks[0].shape)\n",
    "for mask in masks:\n",
    "    total_mask = np.add(total_mask, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6689e4fe-c03a-4e85-bfec-f57b47162cec",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# Print the result\n",
    "output = np.where(\n",
    "    np.expand_dims(total_mask != 0, axis=2),\n",
    "    blurred_img,\n",
    "    raw_image\n",
    ")\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae623b0",
   "metadata": {},
   "source": [
    "* Analyze results in the **Comet** platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f034f7a-4037-4df9-995c-57cb6ce2b84a",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"OWL prompt\": candidate_labels,\n",
    "    \"SAM version\": SAM_version,\n",
    "    \"OWL version\": OWL_checkpoint,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a3715-db57-4425-b449-3bde7e028ccb",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "_ = exp.log_image(\n",
    "    output,\n",
    "    name=\"Blurred masks no sunglasses\",\n",
    "    metadata=metadata,\n",
    "    annotations=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed24b4",
   "metadata": {},
   "source": [
    "### Try yourself! \n",
    "Try the image editing with the following images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eef6d34",
   "metadata": {},
   "source": [
    ">Note: the images referenced in this notebook have already been uploaded to the Jupyter directory, in this classroom, for your convenience. For further details, please refer to the **Appendix** section located at the end of the lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23168ff-3a5c-4fff-864c-abc46669fda3",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "cafe_img = Image.open(\"L3_data/cafe.jpg\")\n",
    "cafe_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcedae9-7526-4c12-9c71-fe6582c8f2bf",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "crosswalk_img = Image.open(\"L3_data/crosswalk.jpg\")\n",
    "crosswalk_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77335554-7ec3-44c9-a583-efe644be4975",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "metro_img = Image.open(\"L3_data/metro.jpg\")\n",
    "metro_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf19de66-d5ac-4c00-9c75-34055e8fa1ad",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "friends_img = Image.open(\"L3_data/friends.jpg\")\n",
    "friends_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9941707",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "\n",
    "* For more on how to use Comet for experiment tracking, check out this [Quickstart Guide](https://colab.research.google.com/drive/1jj9BgsFApkqnpPMLCHSDH-5MoL_bjvYq?usp=sharing) and the [Comet Docs](https://www.comet.com/docs/v2/).\n",
    "* This course was based off a set of two blog articles from Comet. Explore them here for more on how to use newer versions of Stable Diffusion in this pipeline, additional tricks to improve your inpainting results, and a breakdown of the pipeline architecture:\n",
    "  * [SAM + Stable Diffusion for Text-to-Image Inpainting](https://www.comet.com/site/blog/sam-stable-diffusion-for-text-to-image-inpainting/)\n",
    "  * [Image Inpainting for SDXL 1.0 Base Model + Refiner](https://www.comet.com/site/blog/image-inpainting-for-sdxl-1-0-base-refiner/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283423aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6067b026",
   "metadata": {},
   "source": [
    "# Lesson 4: Image Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36956805",
   "metadata": {},
   "source": [
    "### Set up Comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b58221",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import  comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c0535",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "comet_ml.init(anonymous=True, project_name=\"4: Diffusion Prompting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a432479",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# Create the Comet Experiment for logging\n",
    "exp = comet_ml.Experiment()\n",
    "\n",
    "logged_artifact = exp.get_artifact(\"L4-data\", \"anmorgan24\")\n",
    "local_artifact = logged_artifact.download(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cdbff7",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce8994",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65afaf37",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "image=Image.open(\"L4_data/boy-with-kitten.jpg\").resize((256, 256))\n",
    "image_mask=Image.open(\"L4_data/cat_binary_mask.png\").resize((256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd376813",
   "metadata": {},
   "source": [
    ">Note: the images referenced in this notebook have already been uploaded to the Jupyter directory, in this classroom, for your convenience. For further details, please refer to the **Appendix** section located at the end of the lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db8862",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7072916c",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Print the image\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc293dd5",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Print the mask\n",
    "plt.imshow(image_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a9eeb",
   "metadata": {},
   "source": [
    "### Import and prepare the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7019a",
   "metadata": {},
   "source": [
    "#### Import [torch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6217ea50",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb4e7f",
   "metadata": {},
   "source": [
    "* Initialize the Stable Diffusion inpainting pipeline.\n",
    "  -- Note, if you'd like to learn more about the `float16` versus `bfloat16` data type and when you would use one or the other, please check out the short course [\"Quantization Fundamentals\" Lesson \"Data Types and Sizes](https://learn.deeplearning.ai/courses/quantization-fundamentals/lesson/3/data-types-and-sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1377d4fd",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "sd_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-inpainting\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.bfloat16,\n",
    "    low_cpu_mem_usage=False if torch.cuda.is_available() else True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ac1dc",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Set the value of seed manually for reproducibility of the results\n",
    "seed = 66733\n",
    "generator = torch.Generator(device).manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361904fe",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "prompt = \"a realistic green dragon\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efa84ed",
   "metadata": {},
   "source": [
    "### Note\n",
    "- Starting from this point, the code that generates the images will  take too long to run in the given classroom environment. \n",
    "  - So the code is left as markdown in the classroom.\n",
    "- In the classroom, and regardless of whether you have access to GPUs, you can still run the code that retrieves these image generation results using the experiment tracking tool.\n",
    "> - Thank you for your understanding as we try to make these courses free and accessible to as many people as possible. üíï üí´\n",
    "\n",
    "- **Hardware requirements:** \n",
    "To generate the images taught in this lab, a machine with at least 8 GB of CPU should suffice, and you can expect results within approximately 3 minutes for 3 inference steps. However, for tasks involving 10 steps, using a CPU may extend the execution time to about 10 minutes. Tasks involving 100 steps will significantly prolong the execution time on a CPU.\n",
    "Alternatively, consider utilizing a GPU for faster processing. With a GPU, such as a local one or through platforms like [Colab GPU](https://colab.research.google.com/), all three steps can be completed in under 1 second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91783a6f",
   "metadata": {},
   "source": [
    "#### Define new **Comet** experiment.\n",
    "- The following image generation code will takes hours without a GPU.\n",
    "- Its results are saved with an experiment tracking tool (Comet), so that you can retrieve them in this classroom environment (and on any computer, regardless of GPU access)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2dfec3",
   "metadata": {},
   "source": [
    "```Python\n",
    "exp = comet_ml.Experiment()\n",
    "\n",
    "output = sd_pipe(\n",
    "  image=image,\n",
    "  mask_image=image_mask,\n",
    "  prompt=prompt,\n",
    "  generator=generator,\n",
    "  num_inference_steps=3,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e730ac59",
   "metadata": {},
   "source": [
    "\n",
    "```Python\n",
    "generated_image = output.images[0]\n",
    "\n",
    "exp.log_image(\n",
    "    generated_image,\n",
    "    name=f\"{prompt}\",\n",
    "    metadata={\n",
    "        \"prompt\": prompt,\n",
    "        \"seed\": seed,\n",
    "        \"num_inference_steps\": 3\n",
    "    }\n",
    ")\n",
    "\n",
    "exp.end()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e788a4",
   "metadata": {},
   "source": [
    "#### Retrieve the experiment results\n",
    "- Regardless of the environment that you are running in, you can retrieve the results of the experiment using the experiment tracking tool (Comet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28f4b3",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "reference_experiment = comet_ml.APIExperiment(\n",
    "    workspace=\"ckaiser\",\n",
    "    project_name=\"4-diffusion-prompting\",\n",
    "    previous_experiment=\"b1b9e80bb0054b52a8914beec97d36a6\"\n",
    ")\n",
    "\n",
    "reference_image = reference_experiment.get_asset_by_name(f\"{prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7c082",
   "metadata": {},
   "source": [
    "-  Print the reference_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc2e75",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "plt.imshow(Image.open(io.BytesIO(reference_image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad46e9e",
   "metadata": {},
   "source": [
    "### Note: \n",
    "- We'll now explore different hyperparameters.\n",
    "- As before, running the image generation code would take hours in the classroom environment (or in any environment with GPUs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b665565",
   "metadata": {},
   "source": [
    "* Set up a different 'number of inference steps'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5cc39b",
   "metadata": {},
   "source": [
    "\n",
    "```Python\n",
    "exp = comet_ml.Experiment()\n",
    "\n",
    "prompt = \"a realistic green dragon\"\n",
    "\n",
    "exp.log_parameters({\n",
    "    \"seed\": seed,\n",
    "    \"num_inference_steps\": 100\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5236a5",
   "metadata": {},
   "source": [
    "```Python\n",
    "output = sd_pipe(\n",
    "  image=image,\n",
    "  mask_image=image_mask,\n",
    "  prompt=prompt,\n",
    "  generator=generator,\n",
    "  num_inference_steps=100,\n",
    ")\n",
    "\n",
    "generated_image = output.images[0]\n",
    "\n",
    "exp.log_image(\n",
    "    generated_image,\n",
    "    name=f\"{prompt}\",\n",
    "    metadata={\n",
    "        \"prompt\": prompt,\n",
    "        \"seed\": seed,\n",
    "        \"num_inference_steps\": 100\n",
    "    }\n",
    ")\n",
    "\n",
    "exp.end()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b147e3b2",
   "metadata": {},
   "source": [
    "#### Retrieve the experiment results\n",
    "- In the classroom or in any environment, you can retrieve the results of the image generation run by accessing the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729bfe71",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "reference_experiment = comet_ml.APIExperiment(\n",
    "    workspace=\"ckaiser\",\n",
    "    project_name=\"4-diffusion-prompting\",\n",
    "    previous_experiment=\"948c8e6cfd23420c86a0de5f65719955\"\n",
    ")\n",
    "\n",
    "reference_image = reference_experiment.get_asset_by_name(f\"{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c06a6c8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "plt.imshow(Image.open(io.BytesIO(reference_image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff1721",
   "metadata": {},
   "source": [
    "#### Set up the different 'Guidance Scale' values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cce2ce8",
   "metadata": {},
   "source": [
    "- This code is best run on a GPU.  It's left as markdown in the classroom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd0a69",
   "metadata": {},
   "source": [
    "```Python\n",
    "import numpy as np\n",
    "guidance_scale_values = [x for x in np.arange(0, 21, 10)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa66c0d",
   "metadata": {},
   "source": [
    "```Python\n",
    "exp = comet_ml.Experiment()\n",
    "\n",
    "prompt = \"a realistic green dragon\"\n",
    "\n",
    "num_inference_steps = 100 #if torch.cuda.is_available() else 10\n",
    "\n",
    "exp.log_parameters({\n",
    "    \"seed\": seed,\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b3c87",
   "metadata": {},
   "source": [
    "- Pass the guidance_scale to this pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca441a84",
   "metadata": {},
   "source": [
    "```Python\n",
    "for guidance_scale in guidance_scale_values:\n",
    "\n",
    "    output = sd_pipe(\n",
    "      image=image,\n",
    "      mask_image=image_mask,\n",
    "      prompt=prompt,\n",
    "      generator=generator,\n",
    "      num_inference_steps=num_inference_steps,\n",
    "      guidance_scale=guidance_scale\n",
    "    )\n",
    "\n",
    "    generated_image = output.images[0]\n",
    "\n",
    "    exp.log_image(\n",
    "        generated_image,\n",
    "        name=f\"{prompt}\",\n",
    "        metadata={\n",
    "            \"prompt\": prompt,\n",
    "            \"seed\": seed,\n",
    "            \"num_inference_steps\": num_inference_steps,\n",
    "            \"guidance_scale\": guidance_scale\n",
    "        }\n",
    "    )\n",
    "\n",
    "exp.end()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb85e82",
   "metadata": {},
   "source": [
    "#### Retrieve the experiment results\n",
    "- As before, regardless of whether you have access to GPUs or not, you can retrieve the results of the image generation code from the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d1a0a",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "reference_experiment = comet_ml.APIExperiment(\n",
    "    workspace=\"ckaiser\",\n",
    "    project_name=\"4-diffusion-prompting\",\n",
    "    previous_experiment=\"b34b94f94c594802b7090b6f2f1224f2\"\n",
    ")\n",
    "\n",
    "reference_experiment.display(tab=\"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1740fc",
   "metadata": {},
   "source": [
    "#### Set up another hyperparameter: 'strength'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5850b6e7",
   "metadata": {},
   "source": [
    "- Add the strength hyperparameter\n",
    "- This code is best run on a GPU.  It's left as markdown in the classroom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0104c28d",
   "metadata": {},
   "source": [
    "```Python\n",
    "strength_values = [x for x in np.arange(0.1, 1.1, 0.2)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c6af6",
   "metadata": {},
   "source": [
    "```Python\n",
    "exp = comet_ml.Experiment()\n",
    "\n",
    "prompt = \"a realistic green dragon\"\n",
    "\n",
    "num_inference_steps = 200 if torch.cuda.is_available() else 10\n",
    "\n",
    "exp.log_parameters({\n",
    "    \"seed\": seed,\n",
    "})\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f14455d",
   "metadata": {},
   "source": [
    "```Python\n",
    "for strength in strength_values:\n",
    "\n",
    "    output = sd_pipe(\n",
    "      image=image,\n",
    "      mask_image=image_mask,\n",
    "      prompt=prompt,\n",
    "      generator=generator,\n",
    "      num_inference_steps=num_inference_steps,\n",
    "      strength=strength\n",
    "    )\n",
    "\n",
    "    generated_image = output.images[0]\n",
    "\n",
    "    exp.log_image(\n",
    "        generated_image,\n",
    "        name=f\"{prompt}\",\n",
    "        metadata={\n",
    "            \"prompt\": prompt,\n",
    "            \"seed\": seed,\n",
    "            \"num_inference_steps\": num_inference_steps,\n",
    "            \"strength\": strength\n",
    "        }\n",
    "    )\n",
    "\n",
    "exp.end()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf42165",
   "metadata": {},
   "source": [
    "#### Retrieve the experiment results\n",
    "- With experiment tracking, you can compare the most recent run with the earlier ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ca88a",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "reference_experiment = comet_ml.APIExperiment(\n",
    "    workspace=\"ckaiser\",\n",
    "    project_name=\"4-diffusion-prompting\",\n",
    "    previous_experiment=\"2964615a382d46f09c3a36c50c74deef\"\n",
    ")\n",
    "\n",
    "reference_experiment.display(tab=\"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a39605",
   "metadata": {},
   "source": [
    "### Try adding a Negative Prompt.\n",
    "- If you set the negative prompt to \"cartoon\", this is asking the image generation model to not generate an image that looks like a cartoon.\n",
    "- Again, the image generation code is best run on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f137def",
   "metadata": {},
   "source": [
    "```Python\n",
    "exp = comet_ml.Experiment()\n",
    "\n",
    "prompt = \"a realistic green dragon\"\n",
    "negative_prompt = \"cartoon\"\n",
    "\n",
    "num_inference_steps = 100 if torch.cuda.is_available() else 10\n",
    "\n",
    "exp.log_parameters({\n",
    "    \"seed\": seed,\n",
    "})\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197fce49",
   "metadata": {},
   "source": [
    "```Python\n",
    "output = sd_pipe(\n",
    "  image=image,\n",
    "  mask_image=image_mask,\n",
    "  prompt=prompt,\n",
    "  negative_prompt=negative_prompt,\n",
    "  generator=generator,\n",
    "  num_inference_steps=num_inference_steps,\n",
    "  guidance_scale=10\n",
    ")\n",
    "\n",
    "generated_image = output.images[0]\n",
    "\n",
    "exp.log_image(\n",
    "    generated_image,\n",
    "    name=f\"{prompt}\",\n",
    "    metadata={\n",
    "        \"prompt\": prompt,\n",
    "        \"seed\": seed,\n",
    "        \"num_inference_steps\": num_inference_steps,\n",
    "        \"guidance_scale\": 10\n",
    "    }\n",
    ")\n",
    "\n",
    "exp.end()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613c9f14",
   "metadata": {},
   "source": [
    "#### Retrieve the experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106604dc",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "reference_experiment = comet_ml.APIExperiment(\n",
    "    workspace=\"ckaiser\",\n",
    "    project_name=\"4-diffusion-prompting\",\n",
    "    previous_experiment=\"f05b04ac203a4f9aa606ea6cf9417fa3\"\n",
    ")\n",
    "\n",
    "reference_image = reference_experiment.get_asset_by_name(f\"{prompt}\")\n",
    "\n",
    "plt.imshow(Image.open(io.BytesIO(reference_image)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1964f",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "* For more on how to use Comet for experiment tracking, check out this [Quickstart Guide](https://colab.research.google.com/drive/1jj9BgsFApkqnpPMLCHSDH-5MoL_bjvYq?usp=sharing) and the [Comet Docs](https://www.comet.com/docs/v2/).\n",
    "* This course was based off a set of two blog articles from Comet. Explore them here for more on how to use newer versions of Stable Diffusion in this pipeline, additional tricks to improve your inpainting results, and a breakdown of the pipeline architecture:\n",
    "  * [SAM + Stable Diffusion for Text-to-Image Inpainting](https://www.comet.com/site/blog/sam-stable-diffusion-for-text-to-image-inpainting/)\n",
    "  * [Image Inpainting for SDXL 1.0 Base Model + Refiner](https://www.comet.com/site/blog/image-inpainting-for-sdxl-1-0-base-refiner/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c7ba5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19ce6491",
   "metadata": {},
   "source": [
    "# Lesson 5: Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ea0a2",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7760ebe",
   "metadata": {},
   "source": [
    "* In this classroom, the libraries have been already installed for you.\n",
    "* If you would like to run this code on your own machine, you need to install the following:\n",
    "    ```\n",
    "    !pip install -q accelerate torch diffusers transformers comet_ml\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e6cdf0",
   "metadata": {},
   "source": [
    "### Set up Comet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666c298b",
   "metadata": {},
   "source": [
    "* Here you will use the [HuggingFace DreamBooth](https://huggingface.co/docs/diffusers/en/training/dreambooth) training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199255bb",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b7e53d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "comet_ml.init(anonymous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8523b3",
   "metadata": {},
   "source": [
    "### Import and prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0bd06c",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_name = 'stabilityai/stable-diffusion-xl-base-1.0'\n",
    "else:\n",
    "    model_name = './models/runwayml/stable-diffusion-v1-5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e4de7a",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "hyperparameters = {\n",
    "    \"instance_prompt\": \"a photo of a [V] man\",\n",
    "    \"class_prompt\": \"a photo of a man\",\n",
    "    \"seed\": 4329,\n",
    "    \"pretrained_model_name_or_path\": model_name,\n",
    "    \"resolution\": 1024 if torch.cuda.is_available() else 512,\n",
    "    \"num_inference_steps\": 50,\n",
    "    \"guidance_scale\": 5.0,\n",
    "    \"num_class_images\": 200,\n",
    "    \"prior_loss_weight\": 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7a913",
   "metadata": {},
   "source": [
    "* Set new **Comet** experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf780014",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "experiment = comet_ml.Experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd023517",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae20fa",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import DreamBoothTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df2c5f2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "trainer = DreamBoothTrainer(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff278d",
   "metadata": {},
   "source": [
    "#### Note\n",
    "- The code that generates images requires a GPU to run.\n",
    "- The code is left here in markdown, but if you have access to GPUs outside of the classroom, you can run it there.\n",
    "- In the classroom, you'll still be able to follow along by retrieving the generated images from the experiment tracking tool (Comet)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565e45d8",
   "metadata": {},
   "source": [
    "```Python\n",
    "# To run the training pipeline\n",
    "trainer.generate_class_images()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ed3c2",
   "metadata": {},
   "source": [
    "```Python\n",
    "# To see the content of generate_class_image\n",
    "??trainer.generate_class_images\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafe80d6",
   "metadata": {},
   "source": [
    "#### Get class images (using artifacts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea17339a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6eea9a",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Get images\n",
    "class_artifact = experiment.get_artifact('ckaiser/class-images-15')\n",
    "class_artifact.download('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc806c27",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "shutil.unpack_archive('./class.zip', './class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbd0a57",
   "metadata": {},
   "source": [
    ">Note: the images referenced in this notebook have already been uploaded to the Jupyter directory, in this classroom, for your convenience. For further details, please refer to the **Appendix** section located at the end of the lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361cfcb0",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Print some images\n",
    "trainer.display_images(\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682ec14d",
   "metadata": {},
   "source": [
    "* Get the instance dataset (images of Andrew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e94be",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "andrew_artifact = experiment.get_artifact('ckaiser/andrew-dataset')\n",
    "andrew_artifact.download('./')\n",
    "\n",
    "shutil.unpack_archive('./andrew-dataset.zip', './instance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5091ab7f",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Print some images\n",
    "trainer.display_images(\"instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16dd42",
   "metadata": {},
   "source": [
    "### Initialize the model\n",
    "- It will take some time (several minutes) to initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332beed7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "tokenizer, text_encoder, vae, unet = trainer.initialize_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d82144f",
   "metadata": {},
   "source": [
    "> Note: see the video lesson for the LoRA explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0cabbf",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# Add noise to generate images in Stable Diffusion\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "    trainer.hyperparameters.pretrained_model_name_or_path,\n",
    "    subfolder=\"scheduler\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1734b5a5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "unet = trainer.initialize_lora(unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8f01e5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "optimizer, params_to_optimize = trainer.initialize_optimizer(unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5204f",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Initialize the datasets\n",
    "train_dataset, train_dataloader = trainer.prepare_dataset(tokenizer, text_encoder)\n",
    "lr_scheduler = trainer.initialize_scheduler(train_dataloader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64faff81",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "unet, optimizer, train_dataloader, lr_scheduler = trainer.accelerator.prepare(\n",
    "    unet, optimizer, train_dataloader, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10251293",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "total_batch_size = \\\n",
    "    trainer.hyperparameters.train_batch_size * \\\n",
    "    trainer.hyperparameters.gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5dcd11",
   "metadata": {},
   "source": [
    "#### Note\n",
    "- Starting from this point, the code demonstrated by the instructor will not execute in this notebook due to computational resource constraints. However, we provide the code here for you to run if you have access to a GPU or similar resources.\n",
    "- Thank you for your understanding as we work to provide free and accessible courses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2da075",
   "metadata": {},
   "source": [
    "```Python\n",
    "from tqdm import tqdm\n",
    "\n",
    "global_step = 0\n",
    "epoch = 0\n",
    "\n",
    "progress_bar = tqdm(\n",
    "    range(0, trainer.hyperparameters.max_train_steps),\n",
    "    desc=\"Steps\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50067704",
   "metadata": {},
   "source": [
    "```Python\n",
    "for epoch in range(0, trainer.hyperparameters.num_train_epochs):\n",
    "    unet.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        with trainer.accelerator.accumulate(unet):\n",
    "            pixel_values = batch[\"pixel_values\"].to(dtype=vae.dtype)\n",
    "            model_input = vae.encode(pixel_values).latent_dist.sample()\n",
    "            model_input = model_input * vae.config.scaling_factor\n",
    "\n",
    "            noise = torch.randn_like(model_input)\n",
    "            bsz, channels, height, width = model_input.shape\n",
    "\n",
    "            timesteps = torch.randint(\n",
    "                0,\n",
    "                noise_scheduler.config_num_train_timesteps,\n",
    "                (bsz,),\n",
    "                device=model_input.device\n",
    "            )\n",
    "\n",
    "            timesteps = timesteps.long()\n",
    "            noisy_model_input = noise_scheduler.add_noise(\n",
    "                model_input,\n",
    "                noise,\n",
    "                timesteps\n",
    "            )\n",
    "\n",
    "            encoder_hidden_states = batch[\"input_ids\"]\n",
    "\n",
    "            model_predict = unet(\n",
    "                noisy_model_input,\n",
    "                timesteps,\n",
    "                encoder_hidden_states,\n",
    "                return_dic=False,\n",
    "            )[0]\n",
    "\n",
    "            target = noise\n",
    "\n",
    "            model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\n",
    "            target, target_prior = torch.chunk(target, 2, dim=0)\n",
    "\n",
    "            instance_loss = \\\n",
    "                F.mse_loss(\n",
    "                    model_pred.float(),\n",
    "                    target.float(),\n",
    "                    reduction=\"mean\"\n",
    "                )\n",
    "            \n",
    "            prior_loss = \\\n",
    "                F.mse_loss(\n",
    "                    model_pred_prior.float(),\n",
    "                    target_prior.float(),\n",
    "                    eduction=\"mean\"\n",
    "                )\n",
    "            \n",
    "            loss = \\\n",
    "                instance_loss + \\\n",
    "                trainer.hyperparameters.prior_loss_weight * \\\n",
    "                prior_loss\n",
    "            \n",
    "            trainer.accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step +=1\n",
    "\n",
    "        loss_metrics = {\n",
    "            \"loss\": loss.detach().item,\n",
    "            \"prior_loss\": prior_loss.detach().item,\n",
    "            \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "        }\n",
    "\n",
    "        experiment.log_metrics(loss_metrics, step=global_step)\n",
    "\n",
    "        progress_bar.set_postfix(**loss_metrics)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "\n",
    "        if global_step >= trainer.hyperparameters.max_train_steps:\n",
    "            break\n",
    "\n",
    "    trainer.save_lora_weights(unet)\n",
    "experiment.add_tag(f\"dreambooth-training\")\n",
    "experiment.log_parameteres(trainer.hyperparameters)\n",
    "trainer.accelerator.end_training()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0158a95",
   "metadata": {},
   "source": [
    "#### Retrieve the training results\n",
    "- You can get the training results using the experiment tracking tool, Comet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e3cce0",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "training_experiment = \\\n",
    "    comet_ml.APIExperiment(\n",
    "        previous_experiment=\"d92519b1f657497e8569a2c8e989b457\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d88aae",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# See the experiment\n",
    "training_experiment.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de92c27",
   "metadata": {},
   "source": [
    "* Prompts to generate images of Andrew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3cce35",
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"a photo of a [V] man playing basketball\",\n",
    "    \"a photo of a [V] man riding a horse\",\n",
    "    \"a photo of a [V] man at the summit of a mountain\",\n",
    "    \"a photo of a [V] man driving a convertible\",\n",
    "    \"a photo of a [V] man riding a skateboard on a huge halfpipe\",\n",
    "    \"a mural of a [V] man, painted by graffiti artists\"\n",
    "]\n",
    "\n",
    "validation_prompts = [\n",
    "    \"a photo of a man playing basketball\",\n",
    "    \"a photo of a man riding a horse\",\n",
    "    \"a photo of a man at the summit of a mountain\",\n",
    "    \"a photo of a man driving a convertible\",\n",
    "    \"a photo of a man riding a skateboard on a huge halfpipe\",\n",
    "    \"a mural of a man, painted by graffiti artists\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a7568d",
   "metadata": {},
   "source": [
    "#### Note\n",
    "- The folowing code requires GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12482248",
   "metadata": {},
   "source": [
    "```Python\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "pipeline.load_lora_weights(\"./andrew-model\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    with torch.no_grad():\n",
    "        images = pipeline(\n",
    "            prompt = prompt,\n",
    "        ).images\n",
    "\n",
    "        experiment.log_image(images[0], metadata={\n",
    "            \"prompt\": prompt,\n",
    "            \"model\": hyperparameters.pretrained_model_name_or_path,\n",
    "        })\n",
    "\n",
    "for prompt in validation_prompts:\n",
    "    with torch.no_grad():\n",
    "        images = pipeline(\n",
    "            prompt=prompt,\n",
    "        ).images\n",
    "\n",
    "    experiment.log_image(images[0], metadata={\n",
    "            \"prompt\": prompt,\n",
    "            \"model\": hyperparameters.pretrained_model_name_or_path,\n",
    "        })\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbaa179",
   "metadata": {},
   "source": [
    "#### Retrieve the image generation results\n",
    "- You can view the results of image generation regardless of whether you have access to GPUs, using the experiment tracking tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddce7323",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "inference_experiment = comet_ml.APIExperiment(\n",
    "        previous_experiment=\"0eb292126ab5476ab0c863061a400bdc\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea9d18",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# See the experiment\n",
    "inference_experiment.display(tab=\"images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c91f30",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "* For more on how to use Comet for experiment tracking, check out this [Quickstart Guide](https://colab.research.google.com/drive/1jj9BgsFApkqnpPMLCHSDH-5MoL_bjvYq?usp=sharing) and the [Comet Docs](https://www.comet.com/docs/v2/).\n",
    "* This course was based off a set of two blog articles from Comet. Explore them here for more on how to use newer versions of Stable Diffusion in this pipeline, additional tricks to improve your inpainting results, and a breakdown of the pipeline architecture:\n",
    "  * [SAM + Stable Diffusion for Text-to-Image Inpainting](https://www.comet.com/site/blog/sam-stable-diffusion-for-text-to-image-inpainting/)\n",
    "  * [Image Inpainting for SDXL 1.0 Base Model + Refiner](https://www.comet.com/site/blog/image-inpainting-for-sdxl-1-0-base-refiner/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9f48e7",
   "metadata": {},
   "source": [
    "## Did you like this course?\n",
    "\n",
    "- If you liked this course, could you consider giving a rating and share what you liked? üíï\n",
    "- If you did not like this course, could you also please share what you think could have made it better? üôè\n",
    "\n",
    "#### A note about the \"Course Review\" page.\n",
    "The rating options are from 0 to 10, and used to calculate the \"Net Promoter Score\"\n",
    "- A score of 9 or 10 means you like the course.üí´ üíï\n",
    "- A score of 7 or 8 means you feel neutral about the course (neither like nor dislike). üôÑ\n",
    "- A score of 0,1,2,3,4,5 or 6 all mean that you do not like the course. üò≠ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50cae24",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
