{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3dd4226",
   "metadata": {},
   "source": [
    "# 2: Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed49ed9",
   "metadata": {},
   "source": [
    "### Build the `chatbot` pipeline using ðŸ¤— Transformers Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3925a5",
   "metadata": {},
   "source": [
    "- Here is some code that suppresses warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4818781",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1482b1a0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53450efb",
   "metadata": {},
   "source": [
    "- Define the conversation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d21d64",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "chatbot = pipeline(task=\"conversational\",\n",
    "                   model=\"facebook/blenderbot-400M-distill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d32739",
   "metadata": {},
   "source": [
    "Info about ['blenderbot-400M-distill'](https://huggingface.co/facebook/blenderbot-400M-distill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e3332",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "user_message = \"\"\"\n",
    "What are some fun activities I can do in the winter?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dfe694",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from transformers import Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73acaa0a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "conversation = Conversation(user_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b454f37",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef28ce",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "conversation = chatbot(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893cd34",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e75ae",
   "metadata": {},
   "source": [
    "- You can continue the conversation with the chatbot with:\n",
    "```\n",
    "print(chatbot(Conversation(\"What else do you recommend?\")))\n",
    "```\n",
    "- However, the chatbot may provide an unrelated response because it does not have memory of any prior conversations.\n",
    "\n",
    "- To include prior conversations in the LLM's context, you can add a 'message' to include the previous chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e97aa",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "conversation.add_message(\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": \"\"\"\n",
    "What else do you recommend?\n",
    "\"\"\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e57d64f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc526d1",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "conversation = chatbot(conversation)\n",
    "\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6ec9d7",
   "metadata": {},
   "source": [
    "- [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
    "- [LMSYS Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36019e1e",
   "metadata": {},
   "source": [
    "### Try it yourself! \n",
    "- Try chatting with the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6685771",
   "metadata": {},
   "source": [
    "# 3: Translation and Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e3e7c9-1437-4784-8a21-cd200bc609a5",
   "metadata": {},
   "source": [
    "- Here is some code that suppresses warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782af222-1bea-449a-8dd4-655ad7a7b8ea",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea43ec1",
   "metadata": {},
   "source": [
    "### Build the `translation` pipeline using ðŸ¤— Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d46ac9-d665-4690-99a4-43b625e02114",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e1c26-df35-406c-8ac2-9789b011c86b",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "translator = pipeline(task=\"translation\",\n",
    "                      model=\"facebook/nllb-200-distilled-600M\",\n",
    "                      torch_dtype=torch.bfloat16) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8f7a5",
   "metadata": {},
   "source": [
    "NLLB: No Language Left Behind: ['nllb-200-distilled-600M'](https://huggingface.co/facebook/nllb-200-distilled-600M).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095bd1c5-a96f-4b20-8e9c-601b0b158fd8",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\\\n",
    "My puppy is adorable, \\\n",
    "Your kitten is cute.\n",
    "Her panda is friendly.\n",
    "His llama is thoughtful. \\\n",
    "We all have nice pets!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9ebdf-86d8-493b-8757-74b3d1010442",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "text_translated = translator(text,\n",
    "                             src_lang=\"eng_Latn\",\n",
    "                             tgt_lang=\"fra_Latn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711052f5",
   "metadata": {},
   "source": [
    "To choose other languages, you can find the other language codes on the page: [Languages in FLORES-200](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)\n",
    "\n",
    "For example:\n",
    "- Afrikaans: afr_Latn\n",
    "- Chinese: zho_Hans\n",
    "- Egyptian Arabic: arz_Arab\n",
    "- French: fra_Latn\n",
    "- German: deu_Latn\n",
    "- Greek: ell_Grek\n",
    "- Hindi: hin_Deva\n",
    "- Indonesian: ind_Latn\n",
    "- Italian: ita_Latn\n",
    "- Japanese: jpn_Jpan\n",
    "- Korean: kor_Hang\n",
    "- Persian: pes_Arab\n",
    "- Portuguese: por_Latn\n",
    "- Russian: rus_Cyrl\n",
    "- Spanish: spa_Latn\n",
    "- Swahili: swh_Latn\n",
    "- Thai: tha_Thai\n",
    "- Turkish: tur_Latn\n",
    "- Vietnamese: vie_Latn\n",
    "- Zulu: zul_Latn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ba07e3-4a5e-4bf2-86a9-498781828eca",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "text_translated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7517649",
   "metadata": {},
   "source": [
    "## Free up some memory before continuing\n",
    "- In order to have enough free memory to run the rest of the code, please run the following to free up memory on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e5dad-dac0-42e4-9a87-8128a1d49b44",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cafb3a-51b8-4aae-929c-31d524dec530",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "del translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d698a7-8ae2-475e-ac46-d768c282b17c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fac55f",
   "metadata": {},
   "source": [
    "### Build the `summarization` pipeline using ðŸ¤— Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b132c646-0c6a-4c57-939a-b3015ea4b76f",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "summarizer = pipeline(task=\"summarization\",\n",
    "                      model=\"facebook/bart-large-cnn\",\n",
    "                      torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca6f847",
   "metadata": {},
   "source": [
    "Model info: ['bart-large-cnn'](https://huggingface.co/facebook/bart-large-cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98276d66-4274-4a2f-b6a7-b4fb839b94f7",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Paris is the capital and most populous city of France, with\n",
    "          an estimated population of 2,175,601 residents as of 2018,\n",
    "          in an area of more than 105 square kilometres (41 square\n",
    "          miles). The City of Paris is the centre and seat of\n",
    "          government of the region and province of ÃŽle-de-France, or\n",
    "          Paris Region, which has an estimated population of\n",
    "          12,174,880, or about 18 percent of the population of France\n",
    "          as of 2017.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856f193-cbf7-450b-8ae3-42287096e56f",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "summary = summarizer(text,\n",
    "                     min_length=10,\n",
    "                     max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c79f81-6baf-4f6b-95ee-b1a2072ec073",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca56abc0",
   "metadata": {},
   "source": [
    "### Try it yourself! \n",
    "- Try this model with your own texts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c0464-7d37-4b50-b3fe-4099101e2c8b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93de5736",
   "metadata": {},
   "source": [
    "# 4: Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f7aac-6786-4ea6-8fa3-25a6cebbd2e5",
   "metadata": {},
   "source": [
    "- Here is some code that suppresses warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058015a6-19cf-4f80-940d-f4af86cb589c",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35a8e72",
   "metadata": {},
   "source": [
    "### Build the `sentence embedding` pipeline using ðŸ¤— Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed9cec8-803a-4d7e-99d9-c4c84682901c",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5dbb50-8c36-456c-ac0e-f724429c4b7f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad701ed",
   "metadata": {},
   "source": [
    "More info on [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d23e0-5e68-4537-8dd8-eb125e1a6820",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "sentences1 = ['The cat sits outside',\n",
    "              'A man is playing guitar',\n",
    "              'The movies are awesome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33db645-edd8-4a28-a06f-e0fd8200f27f",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de3f4a-bd8e-41d6-847b-9a3a043adeeb",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "embeddings1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5136886d-80d4-4a3a-a68e-692c25496b51",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "sentences2 = ['The dog plays in the garden',\n",
    "              'A woman watches TV',\n",
    "              'The new movie is so great']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0c68f",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "embeddings2 = model.encode(sentences2, \n",
    "                           convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a213124-ea97-4706-bbf4-737490e94244",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(embeddings2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3d585",
   "metadata": {},
   "source": [
    "* Calculate the cosine similarity between two sentences as a measure of how similar they are to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3b38a5-9b35-49de-9f85-c62583d6287d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1f4f3-94ad-4b5e-a40d-c4ba8277815b",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "cosine_scores = util.cos_sim(embeddings1,embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6859d46-15a7-4f61-8a9f-06a15baeff40",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(cosine_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae8571e-2dea-4872-b244-342731b949de",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i],\n",
    "                                                 sentences2[i],\n",
    "                                                 cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49863f4c",
   "metadata": {},
   "source": [
    "### Try it yourself! \n",
    "- Try this model with your own sentences!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0187c",
   "metadata": {},
   "source": [
    "# 5: Zero-Shot Audio Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49adf368",
   "metadata": {},
   "source": [
    "The `librosa` library may need to have [ffmpeg](https://www.ffmpeg.org/download.html) installed. \n",
    "- This page on [librosa](https://pypi.org/project/librosa/) provides installation instructions for ffmpeg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a19033-a44c-48c0-b627-6f31512bbf32",
   "metadata": {},
   "source": [
    "- Here is some code that suppresses warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641dcda-9522-4dbc-b9b9-388c5a061748",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a28d3",
   "metadata": {},
   "source": [
    "### Prepare the dataset of audio recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d422f6b-d6ec-4f54-9022-6268c89a071c",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "# This dataset is a collection of different sounds of 5 seconds\n",
    "dataset = load_dataset(\"ashraq/esc50\",\n",
    "                      split=\"train[0:10]\")\n",
    "# dataset = load_from_disk(\"ashraq/esc50/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa364c-3ef9-4528-aa80-2ee61f96df55",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "audio_sample = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3175cab3-61b5-41dd-bf0d-843322e79ffe",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "audio_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afeafff-182b-4fe4-9ab6-4f3a3acc80a0",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio as IPythonAudio\n",
    "IPythonAudio(audio_sample[\"audio\"][\"array\"],\n",
    "             rate=audio_sample[\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167ed318",
   "metadata": {},
   "source": [
    "### Build the `audio classification` pipeline using ðŸ¤— Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a5820",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5c027-2990-4687-b560-3a4db3099c3c",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "zero_shot_classifier = pipeline(\n",
    "    task=\"zero-shot-audio-classification\",\n",
    "    model=\"laion/clap-htsat-unfused\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb02a6f0",
   "metadata": {},
   "source": [
    "More info on [laion/clap-htsat-unfused](https://huggingface.co/laion/clap-htsat-unfused)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea87bb3-6500-4558-9fee-a20bc557f753",
   "metadata": {},
   "source": [
    "### Sampling Rate for Transformer Models\n",
    "- How long does 1 second of high resolution audio (192,000 Hz) appear to the Whisper model (which is trained to expect audio files at 16,000 Hz)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c904ec-2b9a-424b-bf1d-baad182d4b52",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "(1 * 192000) / 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370a1cdb-e824-4d93-a846-58e13f6756c5",
   "metadata": {},
   "source": [
    "- The 1 second of high resolution audio appears to the model as if it is 12 seconds of audio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89285b-c79d-422a-a9cf-f323205277db",
   "metadata": {},
   "source": [
    "- How about 5 seconds of audio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa97b744-6a6a-49b9-ae26-ba0ff259c0d8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "(5 * 192000) / 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1cb81b-79c3-4852-a8d6-e8e38f6a6b9b",
   "metadata": {},
   "source": [
    "- 5 seconds of high resolution audio appears to the model as if it is 60 seconds of audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da863a69-37f0-4719-9914-d49d3b25c445",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "zero_shot_classifier.feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1574ba2-b1e3-4905-8f6d-7efaac7b5196",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "audio_sample[\"audio\"][\"sampling_rate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff8ea7e",
   "metadata": {},
   "source": [
    "* Set the correct sampling rate for the input and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53b982",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from datasets import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0162d7-bc3f-4de9-816d-23531df64640",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\n",
    "    \"audio\",\n",
    "     Audio(sampling_rate=48_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb2c5b8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "audio_sample = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9991d3-2cc6-4289-ad51-3e6a3cf8860b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "audio_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ad36e-6c2b-4ec5-bf52-11d28f2654e4",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "candidate_labels = [\"Sound of a dog\",\n",
    "                    \"Sound of vacuum cleaner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a7e3f-55f2-4a09-b73b-b5a03d393e63",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "zero_shot_classifier(audio_sample[\"audio\"][\"array\"],\n",
    "                     candidate_labels=candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373d97e-c03d-4a5d-9ca5-061f82dbacc6",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "candidate_labels = [\"Sound of a child crying\",\n",
    "                    \"Sound of vacuum cleaner\",\n",
    "                    \"Sound of a bird singing\",\n",
    "                    \"Sound of an airplane\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a04c213-7ab0-42b6-966b-c83c89e81b2d",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "zero_shot_classifier(audio_sample[\"audio\"][\"array\"],\n",
    "                     candidate_labels=candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d7834f",
   "metadata": {},
   "source": [
    "### Try it yourself! \n",
    "- Try this model with some other labels and audio files!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6c51b1",
   "metadata": {},
   "source": [
    "# 6: Automatic Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac90500c",
   "metadata": {},
   "source": [
    "The `librosa` library may need to have [ffmpeg](https://www.ffmpeg.org/download.html) installed. \n",
    "- This page on [librosa](https://pypi.org/project/librosa/) provides installation instructions for ffmpeg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d654d27-1e91-416c-be34-37e830800585",
   "metadata": {},
   "source": [
    "- Here is some code that suppresses warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f4054-9a14-4f41-820f-e07197890390",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61707b14",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e11e596",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0a5065-8210-435d-a04a-1bda7e8bf596",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"librispeech_asr\",\n",
    "                       split=\"train.clean.100\",\n",
    "                       streaming=True,\n",
    "                       trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38a0be-db12-46f3-ba59-6a6f990423c4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "example = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e08d6-4b68-4d87-8a8c-f28b5981a854",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "dataset_head = dataset.take(5)\n",
    "list(dataset_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba094634-00a2-469b-8862-ee7d68e72d86",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "list(dataset_head)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2932356-2ea0-4797-a1eb-d0cd30cacedc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff63caa-aa7b-4da2-9317-4ec30999c0af",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio as IPythonAudio\n",
    "\n",
    "IPythonAudio(example[\"audio\"][\"array\"],\n",
    "             rate=example[\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda63f6",
   "metadata": {},
   "source": [
    "### Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb518e06",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15499e9e-d3c1-4010-997a-2be716c241c7",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "asr = pipeline(task=\"automatic-speech-recognition\",\n",
    "               model=\"distil-whisper/distil-small.en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2755a2f",
   "metadata": {},
   "source": [
    "Info about [distil-whisper/distil-small.en](https://huggingface.co/distil-whisper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea81f743-6e8b-4021-b9b1-1d1873b88fd0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "asr.feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ae7edb-d852-4fd7-adcf-c0274b494a4b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "example['audio']['sampling_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786cca9b-1fe0-404c-baf3-26e03a6d1ab1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "asr(example[\"audio\"][\"array\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5a0ef-81b8-4e4d-8a15-cd063860a9de",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "example[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac81083e",
   "metadata": {},
   "source": [
    "### Build a shareable app with Gradio\n",
    "\n",
    "### Troubleshooting Tip\n",
    "- Note, in the classroom, you may see the code for creating the Gradio app run indefinitely.\n",
    "  - This is specific to this classroom environment when it's serving many learners at once, and you won't wouldn't experience this issue if you run this code on your own machine.\n",
    "- To fix this, please restart the kernel (Menu Kernel->Restart Kernel) and re-run the code in the lab from the beginning of the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe9264",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d17e293-4ade-4f9a-891b-177abd659206",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "demo = gr.Blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff9b2c-563a-45c2-8ed2-d65a1c281ffa",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "def transcribe_speech(filepath):\n",
    "    if filepath is None:\n",
    "        gr.Warning(\"No audio found, please retry.\")\n",
    "        return \"\"\n",
    "    output = asr(filepath)\n",
    "    return output[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6df902-8d7b-47c6-ac23-745d4ff1f942",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "mic_transcribe = gr.Interface(\n",
    "    fn=transcribe_speech,\n",
    "    inputs=gr.Audio(sources=\"microphone\",\n",
    "                    type=\"filepath\"),\n",
    "    outputs=gr.Textbox(label=\"Transcription\",\n",
    "                       lines=3),\n",
    "    allow_flagging=\"never\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8c807",
   "metadata": {},
   "source": [
    "To learn more about building apps with Gradio, you can check out the short course: [Building Generative AI Applications with Gradio](https://www.deeplearning.ai/short-courses/building-generative-ai-applications-with-gradio/), also taught by Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11982cc9-e13d-4ef8-87c6-de529ea9d45f",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "file_transcribe = gr.Interface(\n",
    "    fn=transcribe_speech,\n",
    "    inputs=gr.Audio(sources=\"upload\",\n",
    "                    type=\"filepath\"),\n",
    "    outputs=gr.Textbox(label=\"Transcription\",\n",
    "                       lines=3),\n",
    "    allow_flagging=\"never\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06a929-8f7b-4c1d-a224-4fbcd807b55a",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "with demo:\n",
    "    gr.TabbedInterface(\n",
    "        [mic_transcribe,\n",
    "         file_transcribe],\n",
    "        [\"Transcribe Microphone\",\n",
    "         \"Transcribe Audio File\"],\n",
    "    )\n",
    "\n",
    "demo.launch(inbrowser=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764e5a0-c8d0-4801-8ee9-a26e0b40135f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4befa9",
   "metadata": {},
   "source": [
    "## Note: Please stop the demo before continuing with the rest of the lab.\n",
    "- The app will continue running unless you run\n",
    "  ```Python\n",
    "  demo.close()\n",
    "  ```\n",
    "- If you run another gradio app (later in this lesson) without first closing this appp, you'll see an error message:\n",
    "  ```Python\n",
    "  OSError: Cannot find empty port in range\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca98d74",
   "metadata": {},
   "source": [
    "* Testing with a longer audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26fe227-f981-4399-8ff3-ce99647168cf",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adbc33f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "audio, sampling_rate = sf.read('narration_example.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cad2ecc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82821e61",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "asr.feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6083dd-d874-4e9b-9a7d-26a2f1d154f1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "asr(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbdc7bd",
   "metadata": {},
   "source": [
    "_Note:_ Running the cell above will return:\n",
    "```\n",
    "ValueError: We expect a single channel audio input for AutomaticSpeechRecognitionPipeline\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c10b2f",
   "metadata": {},
   "source": [
    "* Convert the audio from stereo to mono (Using librosa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1751973f-aeee-497a-8c21-c51d971c7fc4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce73debe-ebdb-403d-bd23-608f5823a496",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "audio_transposed = np.transpose(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622f2cea-255a-4b7b-b322-688ada953c3b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "audio_transposed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d63aee-d3f4-48d6-b4d3-c0249c4e8732",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae60766f-880c-41fd-8ace-8d79d4bf3747",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "audio_mono = librosa.to_mono(audio_transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c0cb4-64d9-4d55-814e-6b0893e5001c",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "IPythonAudio(audio_mono,\n",
    "             rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d73d951-9955-497a-b6b5-a908cde5e4f5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "asr(audio_mono)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ccb846",
   "metadata": {},
   "source": [
    "_Warning:_ The cell above might throw a warning because the sample rate of the audio sample is not the same of the sample rate of the model.\n",
    "\n",
    "Let's check and fix this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf91f6-48b3-405b-8b3c-1aa9d07120db",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7cb71f-52f0-4bfa-9a63-c99793310755",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "asr.feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce1909-96b9-41bc-9e3f-da14c9568401",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "audio_16KHz = librosa.resample(audio_mono,\n",
    "                               orig_sr=sampling_rate,\n",
    "                               target_sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40415a12-e2c6-4bb3-8bc1-8108c57ac7b3",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "asr(\n",
    "    audio_16KHz,\n",
    "    chunk_length_s=30, # 30 seconds\n",
    "    batch_size=4,\n",
    "    return_timestamps=True,\n",
    ")[\"chunks\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713cb23a",
   "metadata": {},
   "source": [
    "* Build the Gradio interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891c095-e8f3-4917-8d91-4f8c5f94fcfb",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "demo = gr.Blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f27d78-de74-45a1-96e8-ebd9b0d367f4",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "def transcribe_long_form(filepath):\n",
    "    if filepath is None:\n",
    "        gr.Warning(\"No audio found, please retry.\")\n",
    "        return \"\"\n",
    "    output = asr(\n",
    "      filepath,\n",
    "      max_new_tokens=256,\n",
    "      chunk_length_s=30,\n",
    "      batch_size=8,\n",
    "    )\n",
    "    return output[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec7801-aab9-4014-85f4-5b8c01ab4edd",
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "mic_transcribe = gr.Interface(\n",
    "    fn=transcribe_long_form,\n",
    "    inputs=gr.Audio(sources=\"microphone\",\n",
    "                    type=\"filepath\"),\n",
    "    outputs=gr.Textbox(label=\"Transcription\",\n",
    "                       lines=3),\n",
    "    allow_flagging=\"never\")\n",
    "\n",
    "file_transcribe = gr.Interface(\n",
    "    fn=transcribe_long_form,\n",
    "    inputs=gr.Audio(sources=\"upload\",\n",
    "                    type=\"filepath\"),\n",
    "    outputs=gr.Textbox(label=\"Transcription\",\n",
    "                       lines=3),\n",
    "    allow_flagging=\"never\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e0b2d-8745-406c-b9c3-32b8e76f9512",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "with demo:\n",
    "    gr.TabbedInterface(\n",
    "        [mic_transcribe,\n",
    "         file_transcribe],\n",
    "        [\"Transcribe Microphone\",\n",
    "         \"Transcribe Audio File\"],\n",
    "    )\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0484ebb",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b5aa7",
   "metadata": {
    "height": 166
   },
   "source": [
    "## Note: Please stop the demo before continuing with the rest of the lab.\n",
    "- The app will continue running unless you run\n",
    "  ```Python\n",
    "  demo.close()\n",
    "  ```\n",
    "- If you run another gradio app (later in this lesson) without first closing this appp, you'll see an error message:\n",
    "  ```Python\n",
    "  OSError: Cannot find empty port in range\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfaa6fb",
   "metadata": {},
   "source": [
    "### Try it yourself!\n",
    "- Try this model with your own audio files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20cb142-ec22-4c55-adc7-17d81ded715f",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import io\n",
    "\n",
    "audio, sampling_rate = sf.read('narration_example.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9279ef-654b-4f8a-9be0-15cbef926b90",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341c526-c5aa-4ccf-a426-98ed5efeaeac",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "asr.feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6b75a",
   "metadata": {},
   "source": [
    "# 7: Text to Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec00b62",
   "metadata": {},
   "source": [
    "**Note:**  `py-espeak-ng` is only available Linux operating systems.\n",
    "\n",
    "To run locally in a Linux machine, follow these commands:\n",
    "```\n",
    "    sudo apt-get update\n",
    "    sudo apt-get install espeak-ng\n",
    "    pip install py-espeak-ng\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2602e381",
   "metadata": {},
   "source": [
    "### Build the `text-to-speech` pipeline using the ðŸ¤— Transformers Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb33f1",
   "metadata": {},
   "source": [
    "- Here is some code that suppresses warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293796cf",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb9737",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "narrator = pipeline(\"text-to-speech\",\n",
    "                    model=\"kakao-enterprise/vits-ljs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9edfd0",
   "metadata": {},
   "source": [
    "Info about [kakao-enterprise/vits-ljs](https://huggingface.co/kakao-enterprise/vits-ljs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413caac",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Researchers at the Allen Institute for AI, \\\n",
    "HuggingFace, Microsoft, the University of Washington, \\\n",
    "Carnegie Mellon University, and the Hebrew University of \\\n",
    "Jerusalem developed a tool that measures atmospheric \\\n",
    "carbon emitted by cloud servers while training machine \\\n",
    "learning models. After a modelâ€™s size, the biggest variables \\\n",
    "were the serverâ€™s location and time of day it was active.\n",
    "\"\"\"\n",
    "text = \"\"\"\n",
    "Maybe I said so Javiera, I really dont remember. I could have said it. Because you said it \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a83fdc",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "narrated_text = narrator(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e56ae",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio as IPythonAudio\n",
    "\n",
    "IPythonAudio(narrated_text[\"audio\"][0],\n",
    "             rate=narrated_text[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7217c",
   "metadata": {},
   "source": [
    "### Try it yourself! \n",
    "- Try this model with your own text to speech examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c2b231",
   "metadata": {
    "id": "BNm6JThea320"
   },
   "source": [
    "# 8: Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f95180",
   "metadata": {},
   "source": [
    "**Note:**  `py-espeak-ng` is only available Linux operating systems.\n",
    "\n",
    "To run locally in a Linux machine, follow these commands:\n",
    "```\n",
    "    sudo apt-get update\n",
    "    sudo apt-get install espeak-ng\n",
    "    pip install py-espeak-ng\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b690a",
   "metadata": {
    "id": "kkkJorDBd8oY"
   },
   "source": [
    "### Build the `object-detection` pipeline using ðŸ¤— Transformers Library\n",
    "\n",
    "- This model was release with the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) from Carion et al. (2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b9409a",
   "metadata": {
    "height": 30,
    "id": "BwlBLWY5dLhB"
   },
   "outputs": [],
   "source": [
    "from helper import load_image_from_url, render_results_in_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846b6c7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7ccaf4",
   "metadata": {},
   "source": [
    "- Here is some code that suppresses warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0180ed",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from helper import ignore_warnings\n",
    "ignore_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd823e6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "od_pipe = pipeline(\"object-detection\", \"facebook/detr-resnet-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8ef790",
   "metadata": {},
   "source": [
    "Info about [facebook/detr-resnet-50](https://huggingface.co/facebook/detr-resnet-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d0ec1",
   "metadata": {},
   "source": [
    "Explore more of the [Hugging Face Hub for more object detection models](https://huggingface.co/models?pipeline_tag=object-detection&sort=trending)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c65bcd",
   "metadata": {},
   "source": [
    "### Use the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0cd7ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391,
     "referenced_widgets": [
      "7978d52d2e0247189ff62b5f03a07255",
      "d09488557e204ab082d9e737181ed034",
      "c3a6dcba63ae4d55ae55a79bd6849f05",
      "20c1146b99e64cccb95f015f18aebb2c",
      "0747d89099364a22b089f6600df6515a",
      "f701db0a2b5443938ebef1b1c383d930",
      "ad6ac2565c4f4338a4a8200c95148753",
      "1bdfd1aa1f3b47a5b50740dbe9108416",
      "05a17f14f1ec4018b13286761bc6140f",
      "20092ac47a794151aa0a324df76c80f7",
      "7bbf39a730f147b1875ba952c34b8ed5",
      "e31526c5288344a7a34404f3d237ee2b",
      "e445744a2c724f7b92f20b11c299aa33",
      "86dc5c1797204ff086a892b7458825a7",
      "311bd5df491447178d80efd07140deb1",
      "518e75567a79481eb3d1bdd2dd022212",
      "43d68a32193f47e19e41918189dee22a",
      "202a8e5017ad43d286dec7c9453857e1",
      "8a1cdd27284c4d5f87cc87e14d438cc0",
      "935a5464b920485c8941ff3d93e2c185",
      "d9a1e78d3ea44343bb8e15c768421540",
      "a4895ef524b04edd9523eb323b51b1e8",
      "de905191365243cfab8e04d8845ec797",
      "5871cb766d244506841d2b3899349346",
      "e092e341c7ca4168baf60a25514ac57e",
      "bcac20c050e34bc1a875b005d0f029c8",
      "86f6a4da1ee3481abfaaafe0c1b5bad8",
      "fb1c4da5a415471595d3ea0fee505a45",
      "a3fe7b7089f542abb40203ae635c695d",
      "67f88f95edb5408785639fbc92e2c3d9",
      "1c674ddbf01440028edb634f7dbfb2ef",
      "468ce2d389184f87bb47a0ab93e8cbf7",
      "8db974a8664044a091fa696f5bf66e77",
      "87fab1dbc0ae40a285d44b8e1d2ea024",
      "1beb4e6b36304d2d990aa4750b2e8f73",
      "44b430e61b03475d95d8480d2b126cd3",
      "385dc6f1834449f18dc87760ed9fa6d1",
      "4874f2fae5d145699a66ee9e0ded3051",
      "6a024b3605914ea3bd7f366d678603b7",
      "921cd39a80ce4e9281a180e175e381c1",
      "5b3b3d3ea7ee4f5b8c7d46644ac024c7",
      "10535ccdbe114df6bb0e41d80c149d20",
      "98848407973241e18b826e0f3e31123c",
      "d685478810a848a3b13bd745961189c4"
     ]
    },
    "height": 30,
    "id": "yHWwzv3qemBi",
    "outputId": "38fc2acf-b935-4698-e154-64aef6d651a7"
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ebee3",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "raw_image = Image.open('huggingface_friends.jpg')\n",
    "raw_image.resize((569, 491))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc802d",
   "metadata": {
    "height": 30,
    "id": "7fcJtShhfkf1"
   },
   "outputs": [],
   "source": [
    "pipeline_output = od_pipe(raw_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52c6174",
   "metadata": {},
   "source": [
    "- Return the results from the pipeline using the helper function `render_results_in_image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c93e7e0",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "processed_image = render_results_in_image(\n",
    "    raw_image, \n",
    "    pipeline_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff1e0b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "processed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a909617",
   "metadata": {},
   "source": [
    "### Using `Gradio` as a Simple Interface\n",
    "\n",
    "- Use [Gradio](https://www.gradio.app) to create a demo for the object detection app.\n",
    "- The demo makes it look friendly and easy to use.\n",
    "- You can share the demo with your friends and colleagues as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2999216",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a95459",
   "metadata": {
    "height": 132,
    "id": "HPfWT6d8mTEL"
   },
   "outputs": [],
   "source": [
    "def get_pipeline_prediction(pil_image):\n",
    "    \n",
    "    pipeline_output = od_pipe(pil_image)\n",
    "    \n",
    "    processed_image = render_results_in_image(pil_image,\n",
    "                                            pipeline_output)\n",
    "    return processed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4fc701",
   "metadata": {
    "height": 132,
    "id": "3onzDMlzl7uj"
   },
   "outputs": [],
   "source": [
    "demo = gr.Interface(\n",
    "  fn=get_pipeline_prediction,\n",
    "  inputs=gr.Image(label=\"Input image\", \n",
    "                  type=\"pil\"),\n",
    "  outputs=gr.Image(label=\"Output image with predicted instances\",\n",
    "                   type=\"pil\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15936d4",
   "metadata": {},
   "source": [
    "- `share=True` will provide an online link to access to the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc28ac4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "height": 30,
    "id": "QcE4p98UmGOc",
    "outputId": "fdf28840-e2c1-488c-da58-8dc605c6f42e"
   },
   "outputs": [],
   "source": [
    "demo.launch(share=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be2017b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27aab82",
   "metadata": {
    "height": 30
   },
   "source": [
    "### Close the app\n",
    "- Remember to call `.close()` on the Gradio app when you're done using it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98657736",
   "metadata": {
    "id": "6LA1Wm6bl724"
   },
   "source": [
    "### Make an AI Powered Audio Assistant\n",
    "\n",
    "- Combine the object detector with a text-to-speech model that will help dictate what is inside the image.\n",
    "\n",
    "- Inspect the output of the object detection pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea875091",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "pipeline_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a431ee",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "od_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fafdea2",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "raw_image = Image.open('huggingface_friends.jpg')\n",
    "raw_image.resize((284, 245))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f964da",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from helper import summarize_predictions_natural_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8218d24",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "text = summarize_predictions_natural_language(pipeline_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5253c1f1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e079239",
   "metadata": {},
   "source": [
    "### Generate Audio Narration of an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d20907",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "tts_pipe = pipeline(\"text-to-speech\",\n",
    "                    model=\"kakao-enterprise/vits-ljs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096db4bc",
   "metadata": {},
   "source": [
    "More info about [kakao-enterprise/vits-ljs](https://huggingface.co/kakao-enterprise/vits-ljs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1380cc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "narrated_text = tts_pipe(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15328303",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284,
     "referenced_widgets": [
      "02c8da7c19e945c6b126cc9d1a9cde1c",
      "f35cd6e475b946dfaa75c607e19bce5e",
      "34c27cfda6be44a89ee39ddf3a2df540",
      "48b7aa27448c47b9b14250b2c42a9fa3",
      "2d128cd857e5440dad33c855a012b035",
      "66a4bde858f34d29a388c35428c35fe3",
      "ea500d795c61486db100416ba6992139",
      "9f22f318547a420ab078e742e65f9bd1",
      "a453054e3b5b429687329fbe270f557d",
      "baa1c7c610894119aafde921b82f7643",
      "3730a68cfc964336a635b1eb3a742e31",
      "0b6e5c370fcd43ddadb88a98e22bffd0",
      "e30d036f6a5841229538bed0e1bf5d97",
      "8909effa410d4d4faeadb56b41aefd5d",
      "12a74e65fe284f92a40418a05ad642b6",
      "66aa281ede8d43669d2c7c406d3c5d56",
      "742ba6b6a8584c81b10625038e631912",
      "45084206664b4979a70e7624b6f5d626",
      "5a2d20c7c00342c6812bb78ad6623161",
      "8f37058c0fe8474fb450795f1d2e5634",
      "f154bf741b11437b8dd2444189c9a25d",
      "a306dbc71ae84dc29d995a9a9ceaa580",
      "9cc867d7bbb146338b7d469118751bcc",
      "149bc68382624ab3b6a7a979b3b751cf",
      "0731a596dc6a43b4a939cee4be03a55d",
      "9b1bd6117a2c49a6877f2fd04dc40225",
      "40927e1dd8944889804e35e7142937dd",
      "f563a4e558c2447b821703df532ab512",
      "361cacd7377f457289fdbe3e9651ae6a",
      "ed091afe5c6749a18a3b742fca3d3757",
      "68e662024cf24cc2b192d64eb780915a",
      "5050fddbb01249c08ba5265bbcefc80f",
      "f892a9e557874920afbbeeb2f4cb07b9",
      "334bd6ab29ff4877be882204753a2dab",
      "f79b3126388a49e3b440e950209653d9",
      "6bbdec2ed932485b996ae63bce15d160",
      "5edb96689b70479faf1c724680d20681",
      "dba9d6bdeada4fbfa0032b30526544fe",
      "a82677a81c794b3abef26cef36f43d62",
      "bfa2b026aea74553b5e4a4beba8f6b17",
      "75dc8815d1284e71852d77d529065aba",
      "1fbb32d50e204f74acef47498849cf81",
      "4fe39d60cb9f455fa69082909f321187",
      "510e3d27bb114856af72081103ebeca4",
      "92e2fbc1f51c4dd9b667adafa567f653",
      "2aa7641a805642af9cf5f36b31396548",
      "75cc12c80c734242905a529b710ca4b3",
      "d5b6f479413c4699b2673235a5f9e8de",
      "1a4d4bfd56754fa6b25f16c39d7ee7f9",
      "0f4f2bc83a7f43c0a8d62160dc4b66b6",
      "c47d0b8edde648c3b324e656d155c70a",
      "3176a68d23e04e8b90e9aef2d158b00a",
      "80a41207a8e447a1a99e8bf108946b7a",
      "84df015bbed14f27be58c7eb42906f85",
      "9c2a0fef00124559a9c37d3a23841bdd"
     ]
    },
    "id": "A6EV19RCmK4i",
    "outputId": "a792cc08-39d4-47dc-96b5-f520386a9fec"
   },
   "source": [
    "### Play the Generated Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880c1c78",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio as IPythonAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f569eec7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76
    },
    "height": 47,
    "id": "M-l4sMDaqUJM",
    "outputId": "a0bbdce2-216c-40c3-85da-2dd20d5676e8"
   },
   "outputs": [],
   "source": [
    "IPythonAudio(narrated_text[\"audio\"][0],\n",
    "             rate=narrated_text[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc3231",
   "metadata": {},
   "source": [
    "### Try it yourself! \n",
    "- Try these models with other images!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c6855-70f7-4738-ae2c-50c9c80e1894",
   "metadata": {},
   "source": [
    "# 9: Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad498c8-c553-4afd-9ece-373829881db5",
   "metadata": {},
   "source": [
    "- Here is some code that suppresses warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1cfbb-6550-4639-ac92-339f63ca81de",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe32afb-1dbf-4a02-9cbe-497ed5c5ac97",
   "metadata": {},
   "source": [
    "### Mask Generation with SAM\n",
    "\n",
    "The [Segment Anything Model (SAM)](https://segment-anything.com) model was released by Meta AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61871b2-812e-4862-8418-ccbb513a737f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a9a3f0-a5c6-430a-acfc-86a77ca95ebd",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "sam_pipe = pipeline(\"mask-generation\",\n",
    "    \"Zigeng/SlimSAM-uniform-77\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f073c4d9",
   "metadata": {},
   "source": [
    "Info about [Zigeng/SlimSAM-uniform-77](https://huggingface.co/Zigeng/SlimSAM-uniform-77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8be7ed",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e113a30f-5f10-418c-8775-d0866cc0da64",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "raw_image = Image.open('meta_llamas.jpg')\n",
    "raw_image.resize((720, 375))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd34a5-7312-4945-bb8e-19a474f92503",
   "metadata": {},
   "source": [
    "- Running this will take some time\n",
    "- The higher the value of 'points_per_batch', the more efficient pipeline inference will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e2f3f-27cc-4f9a-b9d8-8799280822ee",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "output = sam_pipe(raw_image, points_per_batch=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd701210-253c-4e6c-a4f8-2a22778c306f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from helper import show_pipe_masks_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3b40b-c1d4-44b8-ad3a-5a85c77faef2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "show_pipe_masks_on_image(raw_image, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4d8f65-0224-4981-a057-741de8aad6ec",
   "metadata": {},
   "source": [
    "_Note:_ The colors of segmentation, that you will get when running this code, might be different than the ones you see in the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c0dc35-a5aa-4f85-9fb0-b21c310b9c3c",
   "metadata": {},
   "source": [
    "### Faster Inference: Infer an Image and a Single Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7d88bc-e9aa-4e1a-a1c3-96884ced17bd",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391efe26-8706-44eb-9cc0-1573609107eb",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "model = SamModel.from_pretrained(\n",
    "    \"Zigeng/SlimSAM-uniform-77\")\n",
    "\n",
    "processor = SamProcessor.from_pretrained(\n",
    "    \"Zigeng/SlimSAM-uniform-77\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8bdbb3-150c-4e45-9d8d-a03743f4c990",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "raw_image.resize((720, 375))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34928701-d890-4aee-b2dd-8e1814024d34",
   "metadata": {},
   "source": [
    "- Segment the blue shirt Andrew is wearing.\n",
    "- Give any single 2D point that would be in that region (blue shirt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ba89e-e3e3-4c69-8eb8-a6683939f276",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "input_points = [[[1600, 700]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77ac5b0-cfac-4ba1-888b-076dc6c2db25",
   "metadata": {},
   "source": [
    "- Create the input using the image and the single point.\n",
    "- `return_tensors=\"pt\"` means to return PyTorch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8388fb7-52b1-4a8e-b40f-f0e5e474e52b",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "inputs = processor(raw_image,\n",
    "                 input_points=input_points,\n",
    "                 return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea8e307-3890-4adc-a2a7-fd9ffc14859d",
   "metadata": {},
   "source": [
    "- Given the inputs, get the output from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7d328-55f4-4b53-ab10-1d6f6160ea49",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a807d4-077e-4c19-8e3f-7fac5c561c18",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd87d475-a4f0-4a44-bea7-7decebb2abc3",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "predicted_masks = processor.image_processor.post_process_masks(\n",
    "    outputs.pred_masks,\n",
    "    inputs[\"original_sizes\"],\n",
    "    inputs[\"reshaped_input_sizes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c931e-fe87-4c42-a4ac-08fb19c1e117",
   "metadata": {},
   "source": [
    " Length of `predicted_masks` corresponds to the number of images that are used in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a988a82-fb2a-400b-9859-e4a13af3f279",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "len(predicted_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99828ca1-7a9f-4438-81f9-d48381e25d31",
   "metadata": {},
   "source": [
    "- Inspect the size of the first ([0]) predicted mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27c7cd-82af-46ac-84ee-a0a7291adc03",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "predicted_mask = predicted_masks[0]\n",
    "predicted_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b66f617-b45a-42b6-b799-a99f58807fbe",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "outputs.iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d76624-28c2-4736-82de-174497b8d330",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from helper import show_mask_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c561702-3846-4640-93b1-43ca96e2325d",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    show_mask_on_image(raw_image, predicted_mask[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbeaa3f-72a5-4a15-a82e-cbba2caf2167",
   "metadata": {},
   "source": [
    "## Depth Estimation with DPT\n",
    "\n",
    "- This model was introduced in the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by Ranftl et al. (2021) and first released in [isl-org/DPT](https://github.com/isl-org/DPT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1429d0-b8b5-4899-b468-6fc3a8c84c54",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "depth_estimator = pipeline(task=\"depth-estimation\",\n",
    "                        model=\"Intel/dpt-hybrid-midas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a05b882",
   "metadata": {},
   "source": [
    "Info about ['Intel/dpt-hybrid-midas'](https://huggingface.co/Intel/dpt-hybrid-midas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12a3c26-771d-4fe9-aee3-c72e83510edd",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "raw_image = Image.open('gradio_tamagochi_vienna.png')\n",
    "raw_image.resize((806, 621))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4475e-1146-4903-b6e4-2ca921e9854e",
   "metadata": {},
   "source": [
    "- If you'd like to generate this image or something like it, check out the short course on [Gradio](https://www.deeplearning.ai/short-courses/building-generative-ai-applications-with-gradio/) and go to the lesson \"Image Generation App\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc181af8-77b4-40cd-a59d-13d4d2e82d84",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "output = depth_estimator(raw_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70859cb-ed27-4961-9e82-0b8c51371c21",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8c341-6722-466b-b067-20f5a255061e",
   "metadata": {},
   "source": [
    "- Post-process the output image to resize it to the size of the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5127caa0-1cf3-4c91-ab7e-39cbc832c1ef",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "output[\"predicted_depth\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2357b661-ac23-4813-8b28-2801c138fa75",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "output[\"predicted_depth\"].unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e39fd9-17c6-42fe-b87e-b8db44632e6a",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "prediction = torch.nn.functional.interpolate(\n",
    "    output[\"predicted_depth\"].unsqueeze(1),\n",
    "    size=raw_image.size[::-1],\n",
    "    mode=\"bicubic\",\n",
    "    align_corners=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b3d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b129d1d-ed14-4cda-a2e0-5256a05769c8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac4235-6bb7-42d0-a38b-7893ce0ab72b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "raw_image.size[::-1],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638ab97-6935-4743-9439-03e2e3b822b3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3609337d-e61b-4d06-b986-8c472cf1c8e3",
   "metadata": {},
   "source": [
    "- Normalize the predicted tensors (between 0 and 255) so that they can be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca24bf3-f232-4a46-adfe-6c3d2a627a1e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e5d5bc-3fdf-4255-8025-51619843ece4",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "output = prediction.squeeze().numpy()\n",
    "formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "depth = Image.fromarray(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56452340-3cbc-427f-a12d-98c4c7a62305",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ada74-05c4-474b-907d-578b5f0d9d37",
   "metadata": {},
   "source": [
    "### Demo using Gradio\n",
    "\n",
    "### Troubleshooting Tip\n",
    "- Note, in the classroom, you may see the code for creating the Gradio app run indefinitely.\n",
    "  - This is specific to this classroom environment when it's serving many learners at once, and you won't wouldn't experience this issue if you run this code on your own machine.\n",
    "- To fix this, please restart the kernel (Menu Kernel->Restart Kernel) and re-run the code in the lab from the beginning of the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b04985-b3a4-43c8-94ed-5e8a956863c8",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939feab-b84b-4691-826c-afff2d477c60",
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "def launch(input_image):\n",
    "    out = depth_estimator(input_image)\n",
    "\n",
    "    # resize the prediction\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        out[\"predicted_depth\"].unsqueeze(1),\n",
    "        size=input_image.size[::-1],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    # normalize the prediction\n",
    "    output = prediction.squeeze().numpy()\n",
    "    formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "    depth = Image.fromarray(formatted)\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d7f891-9265-4afa-ab3a-9b5d2aed942e",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "iface = gr.Interface(launch, \n",
    "                     inputs=gr.Image(type='pil'), \n",
    "                     outputs=gr.Image(type='pil'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f08a4b-3228-4327-850a-3e2211be83d8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "iface.launch(share=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81555d70-12ec-4e7c-bb11-c1933935bf6d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057195e2-858b-4a3e-96bc-8148004128e3",
   "metadata": {},
   "source": [
    "### Close the app\n",
    "- Remember to call `.close()` on the Gradio app when you're done using it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b878fa",
   "metadata": {},
   "source": [
    "### Try it yourself! \n",
    "- Try this model with your own images!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef44f6-876d-4c92-bfa2-885dac873ad9",
   "metadata": {},
   "source": [
    "# 10: Image Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190858c7-792d-4695-ba6d-b502c1d25b03",
   "metadata": {},
   "source": [
    "- Here is some code that suppresses warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1faec1a-dcda-4b44-84b8-1631f0bb464e",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847fac54",
   "metadata": {},
   "source": [
    "- Load the model and the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da6b12-64f8-44b1-afd0-717e41346c84",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from transformers import BlipForImageTextRetrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417991ee-dbf7-48fe-83a6-b6d5700632be",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "model = BlipForImageTextRetrieval.from_pretrained(\n",
    "    \"Salesforce/blip-itm-base-coco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80ab891",
   "metadata": {},
   "source": [
    "More info about [Salesforce/blip-itm-base-coco](https://huggingface.co/Salesforce/blip-itm-base-coco)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478a6f97-a290-42cd-9f2c-284ea7e41d49",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50faf354-91e4-4739-8e2d-7547504e4c92",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Salesforce/blip-itm-base-coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e72f24c-86b8-48d3-af80-07434ea43522",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e17973f-f89a-40a4-9e4a-b46f46d4dfba",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d711df0-1f01-42e5-b397-8ffcb6ef4c12",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "raw_image =  Image.open(\n",
    "    requests.get(img_url, stream=True).raw).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0a6f43-1607-4364-a7d1-c9799c1a19af",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "raw_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db1166-f8c1-4f06-ad67-7083492b6ceb",
   "metadata": {},
   "source": [
    "### Test, if the image matches the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edee591-c891-4b4b-91ee-1e9f54642e97",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "text = \"an image of a woman and a dog on the beach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b41ec7-70d7-49c2-909a-e600ab7e76ca",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "inputs = processor(images=raw_image,\n",
    "                   text=text,\n",
    "                   return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f818f97-3b9a-4024-8cab-31b615d32632",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db51ff2e-74c2-4ea1-b271-48d8f15e7fd6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "itm_scores = model(**inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c962d20d-6091-4bba-9cc9-afa9a3bef79b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "itm_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7615e88d-a6c1-4363-bcab-0f11e93d20af",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a433f-d1f1-45c7-b776-1472b8a7cbcd",
   "metadata": {},
   "source": [
    "- Use a softmax layer to get the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0749cbf-be41-4b47-92ac-a0f6515ff31e",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "itm_score = torch.nn.functional.softmax(\n",
    "    itm_scores,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc3f239-f95e-4451-9a5b-b8b83011eeb4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "itm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f000eb6-2b6a-48c5-8481-a3bbdd5b3c5d",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"\\\n",
    "The image and text are matched \\\n",
    "with a probability of {itm_score[0][1]:.4f}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416bc237",
   "metadata": {},
   "source": [
    "### Try it yourself! \n",
    "- Try this model with your own images and texts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02012ee6-68f8-4b18-ba64-9596cdcd9f9d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72066422-7d11-4f63-9113-17b60626842b",
   "metadata": {},
   "source": [
    "# 11: Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffb83a-cd74-472c-ab75-be74944bdfd0",
   "metadata": {},
   "source": [
    "- Here is some code that suppresses warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5521ba88-d370-4c32-933d-455b2cbca604",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Using the model-agnostic default `max_length`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ceb84a-5362-4b12-a253-354fef68fd64",
   "metadata": {},
   "source": [
    "- Load the Model and the Processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e6e8f-1586-4727-a3d7-9046cb3ff438",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from transformers import BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6b34e0-4e0a-4ba6-8adc-bed69968c2fc",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "model = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f22c659",
   "metadata": {},
   "source": [
    "Info about [Salesforce/blip-image-captioning-base](https://huggingface.co/Salesforce/blip-image-captioning-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb7bdb-88a0-4c2a-9d78-b6b8d0c72c26",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b435c-dc46-4668-9cb6-f47eb5636412",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06b43b7-2b5e-4f03-90af-c124c3070f00",
   "metadata": {},
   "source": [
    "- Load the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ea0ac-d067-45ad-a5e5-8d21b907b910",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529cd3a3-4e00-46a2-8e41-432a7276cb39",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# image = Image.open(\"./beach.jpeg\")\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n",
    "image =  Image.open(\n",
    "    requests.get(img_url, stream=True).raw).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e94a9a-d27d-405f-ada5-045ad9581c31",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35906e2d-6ccb-485b-91e5-efd5fabc63a8",
   "metadata": {},
   "source": [
    "### Conditional Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2238435a-af4e-440a-9001-4a2ea01d6595",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "text = \"a photograph of\"\n",
    "inputs = processor(image, text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b179ea-c5d4-4cf4-b4db-a806c052a23a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a124287-b2fe-46bc-83cb-85354f7b2da1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "out = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f16848-ee99-498a-9972-0168bbfa3abc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50465206-d384-4a8e-b154-37a002cfa331",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d0e425-d5c0-4968-88b5-b47c2307b7fe",
   "metadata": {},
   "source": [
    "### Unconditional Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5ed22-9beb-4edc-b99d-3df8a5568d05",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "inputs = processor(image,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547b2ae9-76c6-45f5-b1b1-c2306986e177",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "out = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc75041-447b-45c2-95f5-fdd31a6d1e6f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe21dda9",
   "metadata": {},
   "source": [
    "### Try it yourself! \n",
    "- Try this model with your own images and texts!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1db53d-7034-4939-a348-00010503a791",
   "metadata": {},
   "source": [
    "# 12: Visual Question & Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c87fb-c19c-415f-9601-23b3df1407f1",
   "metadata": {},
   "source": [
    "- Here is some code that suppresses warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b6348-302b-4305-9832-17c627b4dfad",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Using the model-agnostic default `max_length`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d41c7-3a0d-4205-b47b-9a1e6215bc1e",
   "metadata": {},
   "source": [
    "* Load the Model and the Processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c2a05-19bf-4785-a1a0-72d282d7055b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from transformers import BlipForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee7551-f403-402e-96a2-1e2b4458b8ca",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "model = BlipForQuestionAnswering.from_pretrained(\n",
    "    \"Salesforce/blip-vqa-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b673bd3",
   "metadata": {},
   "source": [
    "Info about [Salesforce/blip-vqa-base](https://huggingface.co/Salesforce/blip-vqa-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117bb100-a107-419e-a43a-be34525a4ca8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806431d5-334c-4d17-90a7-2c9ac763e64f",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Salesforce/blip-vqa-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c130ee3-e6af-4f40-8cf8-00beb908427b",
   "metadata": {},
   "source": [
    "- Load the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a6ce6-34f3-4216-82dc-61adcf237a58",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb1551d-9962-4692-94a6-1611a51495ab",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n",
    "image =  Image.open(\n",
    "    requests.get(img_url, stream=True).raw).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a2d53-e80d-4df4-9cc1-e6b685978a18",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5224486-c9e8-4d9a-a8f3-779f03bc652e",
   "metadata": {},
   "source": [
    "- Write the `question` you want to ask to the model about the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0677c-44a8-4cc2-b2e5-b9f84c8451c3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "question = \"how many dogs are in the picture?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71480588-f901-47aa-8212-34bda635e38b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "inputs = processor(image, question, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca3645c-fa84-48ef-8fc0-938b6894e5b0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "out = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d2bfcf-e9c7-4b4e-9f54-5ea0526d7d41",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6550904f",
   "metadata": {},
   "source": [
    "### Try it yourself! \n",
    "- Try this model with your own images and questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b56dd1-61e0-4522-a508-abb0317e79a9",
   "metadata": {},
   "source": [
    "# 13: Zero-Shot Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb16fd2-2e18-4c0a-912b-c6cf737d8b45",
   "metadata": {},
   "source": [
    "- Here is some code that suppresses warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead1e9b-2273-4e90-b89e-a8bd997a7ba8",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df1e27",
   "metadata": {},
   "source": [
    "- Load the model and the processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d30213-b2f4-4fb9-988e-98b1799e524e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaefb689-245e-4b20-833d-8cf197fa1eb2",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\n",
    "    \"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce8a17-11f8-4a00-a6de-2f58f9cd020f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7654301-8267-4cbd-a6df-1c220e258d32",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a2210",
   "metadata": {},
   "source": [
    "More info about [openai/clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c780438c-f7d8-4c61-853a-695cb5ef0f16",
   "metadata": {},
   "source": [
    "- Load the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ae61e-6c98-4384-9d73-da8a2550aaba",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1a89e-ef13-44d3-b048-7f2ce2624313",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n",
    "image =  Image.open(\n",
    "    requests.get(img_url, stream=True).raw).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bcfa82-5d72-4883-b3fa-9d3fd101174b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433ffd0b-916b-41ec-a228-094c3bae6d1a",
   "metadata": {},
   "source": [
    "- Set the list of labels from which you want the model to classify the image (above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd646c-86a5-49bd-993a-0bd6293aa583",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "labels = [\"a photo of a cat\", \"a photo of a dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bce093-ff57-421d-b5e0-a261d250f280",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "inputs = processor(text=labels,\n",
    "                   images=image,\n",
    "                   return_tensors=\"pt\",\n",
    "                   padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b69f984-34e7-42f3-8f7d-137bf87eaebf",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff2a203-d24d-4bfa-8dfd-f948976bd756",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b8b1fd-f6bf-4af4-99a7-18b493dc6b97",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "outputs.logits_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bfc489-8d05-4405-bf84-16f0a1d41780",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "probs = outputs.logits_per_image.softmax(dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a6f52d-ae9b-4e14-8a01-828ff789b8d1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e9aed6-098c-424f-9818-ba497aa56586",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "probs = list(probs)\n",
    "for i in range(len(labels)):\n",
    "  print(f\"label: {labels[i]} - probability of {probs[i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ce3e4",
   "metadata": {},
   "source": [
    "### Try it yourself! \n",
    "- Try this model with your own images and labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a8d7a2-5a74-4076-a4a5-64580a6508fb",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7df7d478",
   "metadata": {
    "id": "Lyy5zEjDMY_L"
   },
   "source": [
    "# 14: Deploy ML Models on ðŸ¤— Hub using Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251bf195",
   "metadata": {
    "id": "H0CMZcSPRXni"
   },
   "source": [
    "- Welcome to the last lesson - ML deployment using ðŸ¤— Hub and Gradio libraries.\n",
    "- This lesson is optional.  You can watch the video first to see a walkthrough of how to deploy to Hugging Face Spaces.\n",
    "- If you would like to follow along or deploy to Hugging Face Spaces later, you can do so by creating a free account on https://huggingface.co/\n",
    "- You are not required to create an account to complete this lesson, as this lesson contains screenshots and instructions for how to deploy, but does not have any code that requires you to have a Hugging Face account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b9ad6",
   "metadata": {},
   "source": [
    "- In the classroom, the libraries are already installed for you.\n",
    "- If you would like to run this code on your own machine, you can install the following:\n",
    "\n",
    "```\n",
    "    !pip install transformers\n",
    "    !pip install gradio\n",
    "    !pip install gradio_client\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ee1a20",
   "metadata": {},
   "source": [
    "- Note that if you run into issues when making an API call to your own space, you can try to upgrade your version of gradio_client:\n",
    "\n",
    "```\n",
    "pip install -U gradio_client\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e0828",
   "metadata": {},
   "source": [
    "- Here is some code that suppresses warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231add0",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \n",
    "                        message=\"Using the model-agnostic default `max_length`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d9a69",
   "metadata": {
    "id": "tpBYPpSxNTyq"
   },
   "source": [
    "## ðŸ¤— Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f44465",
   "metadata": {},
   "source": [
    "- You can create an account on hugging face from [here](https://huggingface.co), to follow the instructions provided in the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4e01b2",
   "metadata": {},
   "source": [
    "### Deploying to Hugging Face Spaces\n",
    "\n",
    "- Go to [https://huggingface.co/spaces](https://huggingface.co/spaces)![]\n",
    "- Click the button \"create new space\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1199e1",
   "metadata": {},
   "source": [
    "- Give the space a name, such as \"blip-image-captioning\".\n",
    "- Choose a license, such as Apache 2.0\n",
    "- For \"Select the Space SDK\", click \"Gradio\".\n",
    "- For Hardware, choose the default free option: \"CPU Basic\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314db31c",
   "metadata": {},
   "source": [
    "- Leave it as \"public\"\n",
    "- Click \"create space\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3887e638",
   "metadata": {},
   "source": [
    "- You will see a new page with instructions for how to get started by cloning and updating a GitHub repo.\n",
    "- You can also add the required files directly in the web browser if you'd like to get a small app running quickly.  Click on \"Files\" at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adcaf87",
   "metadata": {},
   "source": [
    "- Click on \"+ Add file\"->\"Create new File\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e3dce2",
   "metadata": {},
   "source": [
    "### Add requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7c35b",
   "metadata": {},
   "source": [
    "- Add a file called requirements.txt.\n",
    "- Paste in the following:\n",
    "\n",
    "```\n",
    "transformers\n",
    "torch\n",
    "gradio\n",
    "```\n",
    "\n",
    "- Leave \"Commit Directly to the main branch\" selected.\n",
    "- Click \"commit new file to main\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b04781",
   "metadata": {},
   "source": [
    "### Add app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3567e75",
   "metadata": {},
   "source": [
    "- In the textbox \"Name Your File\", type \"app.py\"\n",
    "- In the textbox for your code, paste in the code that you ran above, or copy this block below:\n",
    "\n",
    "\n",
    "\n",
    "```Python\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-to-text\",\n",
    "                model=\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "\n",
    "def launch(input):\n",
    "    out = pipe(input)\n",
    "    return out[0]['generated_text']\n",
    "\n",
    "iface = gr.Interface(launch,\n",
    "                     inputs=gr.Image(type='pil'),\n",
    "                     outputs=\"text\")\n",
    "\n",
    "iface.launch()\n",
    "```\n",
    "- Notice that `iface.launch()` does not have `share=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e983a112",
   "metadata": {},
   "source": [
    "- Leave \"Commit Directly to the main branch\" selected.\n",
    "- Click \"Commit new file to main\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d55ae1",
   "metadata": {},
   "source": [
    "### View the app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce30e603",
   "metadata": {},
   "source": [
    "- You will see that the app is still \"Building\" for a few minutes.\n",
    "- You can click on the \"App\" menu to the left of the \"Files\" menu to see the console as the space is being built."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94d1f44",
   "metadata": {},
   "source": [
    "- When the build is done, you'll see your app!\n",
    "- At the bottom, you can click \"Use via API\" to see sample code that you can use to use your model with an API call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd82f71d",
   "metadata": {},
   "source": [
    "- You can run the pip install if you haven't already done so.\n",
    "- In the classroom, gradio_client should already be installed for you.\n",
    "- Copy the sample code, which will look something like this:\n",
    "\n",
    "```Python\n",
    "from gradio_client import Client\n",
    "\n",
    "client = Client(\"eddyS/blip-image-captioning-2\")\n",
    "result = client.predict(\n",
    "\t\t\"https://raw.githubusercontent.com/gradio-app/gradio/main/test/test_files/bus.png\",\t# filepath  in 'input' Image component\n",
    "\t\tapi_name=\"/predict\"\n",
    ")\n",
    "print(result)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79847123",
   "metadata": {},
   "source": [
    "- Note, you can replace the string within `client.predict()` with a string that points to a local file.\n",
    "- In the classroom, there are two image files that you can use.\n",
    "  - \"kittens.jpg\"\n",
    "  - \"huggingface_friends.jpg\"\n",
    "  - Feel free to upload your own to the file directory.\n",
    " \n",
    "So your code may look like this:\n",
    "```Python\n",
    "from gradio_client import Client\n",
    "\n",
    "client = Client(\"eddyS/blip-image-captioning-2\")\n",
    "result = client.predict(\n",
    "\t\t\"kittens.jpg\",\n",
    "\t\tapi_name=\"/predict\"\n",
    ")\n",
    "print(result)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2bb5b0",
   "metadata": {},
   "source": [
    "- Inspect the information in the API.\n",
    "\n",
    "```Python\n",
    "client.view_api()\n",
    "```\n",
    "- The output may look like this:\n",
    "\n",
    "\n",
    "```\n",
    "Client.predict() Usage Info\n",
    "---------------------------\n",
    "Named API endpoints: 1\n",
    "\n",
    " - predict(input, api_name=\"/predict\") -> output\n",
    "    Parameters:\n",
    "     - [Image] input: filepath \n",
    "    Returns:\n",
    "     - [Textbox] output: str \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efe3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradio_client import Client, file\n",
    "\n",
    "client = Client(src=\"hjerpe/blip-image-captioning\")\n",
    "result = client.predict(\n",
    "    file(\"https://cms.eichertrucksandbuses.com/uploads/truck/sub-category/a933e5958e4a354cfb8d22665bd244fd.png\"),\t# filepath  in 'parameter_1' Image component\n",
    "    api_name=\"/predict\"\n",
    "\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cfaddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.view_api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df43373a",
   "metadata": {},
   "source": [
    "You can modify the API call to include your access token.\n",
    "\n",
    "```Python\n",
    "from gradio_client import Client\n",
    "\n",
    "client = Client(\"eddyS/blip-image-captioning-2\",\n",
    "                hf_token=hf_access_token\n",
    "               )\n",
    "result = client.predict(\n",
    "\t\t\"kittens.jpg\",\n",
    "\t\tapi_name=\"/predict\"\n",
    ")\n",
    "print(result)\n",
    "# client = Client(\"abidlabs/whisper-large-v2\", \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d95b206",
   "metadata": {},
   "source": [
    "### Saving your access token securely\n",
    "- It's recommended that you not hard code the access token.\n",
    "\n",
    "```Python\n",
    "HF_TOKEN=\"abc1234\" # not recommended\n",
    "```\n",
    "\n",
    "- You can save your access token to a file \".env\"\n",
    "\n",
    "```\n",
    "HF_ACCESS_TOKEN=\"abc123\"\n",
    "```\n",
    "\n",
    "Then access that environment variable with the `dotenv` library\n",
    "\n",
    "```Python\n",
    "# !pip install python-dotenv # install library\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "_ = load_dotenv(find_dotenv())\n",
    "hf_access_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e22139",
   "metadata": {
    "id": "DH5WBFsETakU"
   },
   "source": [
    "### GPU Zero Space\n",
    "- [ZeroGPU Explorers](https://huggingface.co/zero-gpu-explorers): A place to spin free GPUs on demand for your spaces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default_venv",
   "language": "python",
   "name": "default_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
