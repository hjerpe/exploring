{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88535ead",
   "metadata": {},
   "source": [
    "# L1: NLP tasks with a simple interface üóûÔ∏è"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6faa43ba",
   "metadata": {},
   "source": [
    "Load your HF API key and relevant Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2698081-4deb-436a-a821-8ea48bdd6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from IPython.display import Image, display, HTML\n",
    "from PIL import Image\n",
    "import base64 \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import gradio as gr\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hf_api_key = os.environ['HF_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a106ab02-f248-4c03-9dd8-b1991db7f778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "import requests, json\n",
    "\n",
    "#Summarization endpoint\n",
    "def get_completion(inputs, parameters=None,ENDPOINT_URL=os.environ['HF_API_SUMMARY_BASE']): \n",
    "    headers = {\n",
    "      \"Authorization\": f\"Bearer {hf_api_key}\",\n",
    "      \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = { \"inputs\": inputs }\n",
    "    if parameters is not None:\n",
    "        data.update({\"parameters\": parameters})\n",
    "    response = requests.request(\"POST\",\n",
    "                                ENDPOINT_URL, headers=headers,\n",
    "                                data=json.dumps(data)\n",
    "                               )\n",
    "    return json.loads(response.content.decode(\"utf-8\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a97a06f9",
   "metadata": {},
   "source": [
    "## Building a text summarization app"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4794502",
   "metadata": {},
   "source": [
    "Here we are using an [Inference Endpoint](https://huggingface.co/inference-endpoints) for the `shleifer/distilbart-cnn-12-6`, a 306M parameter distilled model from `facebook/bart-large-cnn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d01136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "get_completion_local = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "def summarize_local(input):\n",
    "    output = get_completion_local(input)\n",
    "    return output[0]['summary_text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01892d1a",
   "metadata": {},
   "source": [
    "### How about running it locally?\n",
    "The code would look very similar if you were running it locally instead of from an API. The same is true for all the models in the rest of the course, make sure to check the [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) documentation page\n",
    "\n",
    "```py\n",
    "from transformers import pipeline\n",
    "\n",
    "get_completion = pipeline(\"summarization\", model=\"shleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "def summarize(input):\n",
    "    output = get_completion(input)\n",
    "    return output[0]['summary_text']\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f0fc58-91d6-48f2-a014-052192586be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ('''The tower is 324 metres (1,063 ft) tall, about the same height\n",
    "        as an 81-storey building, and the tallest structure in Paris. \n",
    "        Its base is square, measuring 125 metres (410 ft) on each side. \n",
    "        During its construction, the Eiffel Tower surpassed the Washington \n",
    "        Monument to become the tallest man-made structure in the world,\n",
    "        a title it held for 41 years until the Chrysler Building\n",
    "        in New York City was finished in 1930. It was the first structure \n",
    "        to reach a height of 300 metres. Due to the addition of a broadcasting \n",
    "        aerial at the top of the tower in 1957, it is now taller than the \n",
    "        Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the \n",
    "        Eiffel Tower is the second tallest free-standing structure in France \n",
    "        after the Millau Viaduct.''')\n",
    "\n",
    "get_completion(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f144593f",
   "metadata": {},
   "source": [
    "### Getting started with Gradio `gr.Interface` \n",
    "\n",
    "#### How about running it locally?\n",
    "The code would look very similar if you were running it locally.  Simply remove all the paramters in the launch method\n",
    "\n",
    "```py\n",
    "demo.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb11460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "def summarize(input):\n",
    "    output = get_completion(input)\n",
    "    return output[0]['summary_text']\n",
    "    \n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=summarize, inputs=\"text\", outputs=\"text\")\n",
    "demo.launch()\n",
    "#demo.launch(share=True, server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b310770",
   "metadata": {},
   "source": [
    "You can add `demo.launch(share=True)` to create a public link to share with your team or friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60684b55-c7ae-4c9e-88ea-bbc2e702ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def summarize(input):\n",
    "    output = get_completion(input)\n",
    "    return output[0]['summary_text']\n",
    "    \n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=summarize, \n",
    "                    inputs=[gr.Textbox(label=\"Text to summarize\", lines=6)],\n",
    "                    outputs=[gr.Textbox(label=\"Result\", lines=3)],\n",
    "                    title=\"Text summarization with distilbart-cnn\",\n",
    "                    description=\"Summarize any text using the `shleifer/distilbart-cnn-12-6` model under the hood!\"\n",
    "                   )\n",
    "demo.launch(share=True)\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a4f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.sto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b300d17",
   "metadata": {},
   "source": [
    "## Building a Named Entity Recognition app"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0d1043f",
   "metadata": {},
   "source": [
    "We are using this [Inference Endpoint](https://huggingface.co/inference-endpoints) for `dslim/bert-base-NER`, a 108M parameter fine-tuned BART model on the NER task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f663dcbb",
   "metadata": {},
   "source": [
    "### How about running it locally?\n",
    "\n",
    "```py\n",
    "from transformers import pipeline\n",
    "\n",
    "get_completion = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "\n",
    "def ner(input):\n",
    "    output = get_completion(input)\n",
    "    return {\"text\": input, \"entities\": output}\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4a922-b300-4dbc-8768-955b6a18dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = os.environ['HF_API_NER_BASE'] #NER endpoint\n",
    "text = \"My name is Andrew, I'm building DeepLearningAI and I live in California\"\n",
    "get_completion(text, parameters=None, ENDPOINT_URL= API_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c21254-128d-446c-b6dd-e30af26d436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(input):\n",
    "    output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)\n",
    "    return {\"text\": input, \"entities\": output}\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=ner,\n",
    "                    inputs=[gr.Textbox(label=\"Text to find entities\", lines=2)],\n",
    "                    outputs=[gr.HighlightedText(label=\"Text with entities\")],\n",
    "                    title=\"NER with dslim/bert-base-NER\",\n",
    "                    description=\"Find entities using the `dslim/bert-base-NER` model under the hood!\",\n",
    "                    allow_flagging=\"never\",\n",
    "                    #Here we introduce a new tag, examples, easy to use examples for your application\n",
    "                    examples=[\"My name is Andrew and I live in California\", \"My name is Poli and work at HuggingFace\"])\n",
    "demo.launch()\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14829721",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60f16ad4",
   "metadata": {},
   "source": [
    "### Adding a helper function to merge tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc278e9-87b4-420b-89e9-7120dc4be754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(tokens):\n",
    "    merged_tokens = []\n",
    "    for token in tokens:\n",
    "        if merged_tokens and token['entity'].startswith('I-') and merged_tokens[-1]['entity'].endswith(token['entity'][2:]):\n",
    "            # If current token continues the entity of the last one, merge them\n",
    "            last_token = merged_tokens[-1]\n",
    "            last_token['word'] += token['word'].replace('##', '')\n",
    "            last_token['end'] = token['end']\n",
    "            last_token['score'] = (last_token['score'] + token['score']) / 2\n",
    "        else:\n",
    "            # Otherwise, add the token to the list\n",
    "            merged_tokens.append(token)\n",
    "\n",
    "    return merged_tokens\n",
    "\n",
    "def ner(input):\n",
    "    output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)\n",
    "    merged_tokens = merge_tokens(output)\n",
    "    return {\"text\": input, \"entities\": merged_tokens}\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=ner,\n",
    "                    inputs=[gr.Textbox(label=\"Text to find entities\", lines=2)],\n",
    "                    outputs=[gr.HighlightedText(label=\"Text with entities\")],\n",
    "                    title=\"NER with dslim/bert-base-NER\",\n",
    "                    description=\"Find entities using the `dslim/bert-base-NER` model under the hood!\",\n",
    "                    allow_flagging=\"never\",\n",
    "                    examples=[\"My name is Andrew, I'm building DeeplearningAI and I live in California\", \"My name is Poli, I live in Vienna and work at HuggingFace\"])\n",
    "\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cccdb9b-0c3a-406e-95bc-106705aeb010",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef1d07f-ac8b-4f38-9c6c-ecdd5942ffb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5fcb3b6",
   "metadata": {},
   "source": [
    "# L2: Image captioning app üñºÔ∏èüìù"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90f08092",
   "metadata": {},
   "source": [
    "Load your HF API key and relevant Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c9d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import IPython.display\n",
    "from PIL import Image\n",
    "import base64 \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hf_api_key = os.environ['HF_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "import requests, json\n",
    "\n",
    "#Image-to-text endpoint\n",
    "def get_completion(inputs, parameters=None, ENDPOINT_URL=os.environ['HF_API_ITT_BASE']): \n",
    "    headers = {\n",
    "      \"Authorization\": f\"Bearer {hf_api_key}\",\n",
    "      \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = { \"inputs\": inputs }\n",
    "    if parameters is not None:\n",
    "        data.update({\"parameters\": parameters})\n",
    "    response = requests.request(\"POST\",\n",
    "                                ENDPOINT_URL,\n",
    "                                headers=headers,\n",
    "                                data=json.dumps(data))\n",
    "    return json.loads(response.content.decode(\"utf-8\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38eda7ce",
   "metadata": {},
   "source": [
    "## Building an image captioning app "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee01ea47",
   "metadata": {},
   "source": [
    "Here we'll be using an [Inference Endpoint](https://huggingface.co/inference-endpoints) for `Salesforce/blip-image-captioning-base` a 14M parameter captioning model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f8c6382",
   "metadata": {},
   "source": [
    "The code would look very similar if you were running it locally instead of from an API. You can check the [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) documentation page.\n",
    "\n",
    "```py\n",
    "from transformers import pipeline\n",
    "\n",
    "get_completion = pipeline(\"image-to-text\",model=\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "def summarize(input):\n",
    "    output = get_completion(input)\n",
    "    return output[0]['generated_text']\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b20f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The free images are available on: https://free-images.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c1bde-bf26-4e60-8f00-8ae2ccd7aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://free-images.com/sm/9596/dog_animal_greyhound_983023.jpg\"\n",
    "image_url = \"https://free-images.com/sm/d687/beach_children_vacation_fun.jpg\"\n",
    "image_url = \"https://free-images.com/md/8c2a/beach_vacation_summer_sea.jpg\"\n",
    "image_url = \"https://free-images.com/md/5978/funny_pug_dog_animal.jpg\"\n",
    "# image_url = \"https://free-images.com/md/5978/funny_pug_dog_animal.jpg\"\n",
    "# image_url = \"https://free-images.com/md/018d/frogs_funny_frog_animal.jpg\"\n",
    "# image_url = \"https://free-images.com/md/a869/giraffe_funny_tongue_funny.jpg\"\n",
    "# image_url = \"https://free-images.com/md/a869/giraffe_funny_tongue_funny.jpg\"\n",
    "# image_url = \"https://free-images.com/md/af55/sheep_box_funny_animal.jpg\"\n",
    "display(IPython.display.Image(url=image_url))\n",
    "get_completion(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe20258-b9f6-41d2-b174-dcb2955285d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Captioning with `gr.Interface()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a749a7e2-fa52-4898-a37e-9b9373c9503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr \n",
    "\n",
    "def image_to_base64_str(pil_image):\n",
    "    byte_arr = io.BytesIO()\n",
    "    pil_image.save(byte_arr, format='PNG')\n",
    "    byte_arr = byte_arr.getvalue()\n",
    "    return str(base64.b64encode(byte_arr).decode('utf-8'))\n",
    "\n",
    "def captioner(image):\n",
    "    base64_image = image_to_base64_str(image)\n",
    "    result = get_completion(base64_image)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=captioner,\n",
    "                    inputs=[gr.Image(label=\"Upload image\", type=\"pil\")],\n",
    "                    outputs=[gr.Textbox(label=\"Caption\")],\n",
    "                    title=\"Image Captioning with BLIP\",\n",
    "                    description=\"Caption any image using the BLIP model\",\n",
    "                    allow_flagging=\"never\",\n",
    "                    examples=[\"christmas_dog.jpeg\", \"bird_flight.jpeg\", \"cow.jpeg\"])\n",
    "\n",
    "#demo.launch(share=True, server_port=int(os.environ['PORT1']))\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f957fee-6b9b-48c4-999a-def344bad727",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c40c6-2154-44bb-9f8c-75c8763addd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "860619c4",
   "metadata": {},
   "source": [
    "# L3: Image generation app üé®"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6fc3b9e",
   "metadata": {},
   "source": [
    "Load your HF API key and relevant Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab976dd-e4ed-48a4-a55e-4682fdc5f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import IPython.display\n",
    "from PIL import Image\n",
    "import base64 \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hf_api_key = os.environ['HF_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a18bda-4802-4b9e-bfeb-a7f6181b09d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "import requests, json\n",
    "\n",
    "#Text-to-image endpoint\n",
    "def get_completion(inputs, parameters=None, ENDPOINT_URL=os.environ['HF_API_TTI_BASE']):\n",
    "    headers = {\n",
    "      \"Authorization\": f\"Bearer {hf_api_key}\",\n",
    "      \"Content-Type\": \"application/json\"\n",
    "    }   \n",
    "    data = { \"inputs\": inputs }\n",
    "    if parameters is not None:\n",
    "        data.update({\"parameters\": parameters})\n",
    "    response = requests.request(\"POST\",\n",
    "                                ENDPOINT_URL,\n",
    "                                headers=headers,\n",
    "                                data=json.dumps(data))\n",
    "    contents = response.content\n",
    "    return contents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7847d029-dee6-44f0-9af4-c9d3836cb3d5",
   "metadata": {},
   "source": [
    "## Building an image generation app "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d286683-3f4d-452a-a19e-21e8940be5bc",
   "metadata": {},
   "source": [
    "Here we are going to run `runwayml/stable-diffusion-v1-5` using the `üß® diffusers` library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8138e864-a85b-4d14-8a20-ef6e6adf9781",
   "metadata": {},
   "source": [
    "### How about running it locally?\n",
    "The code would look very similar if you were running it locally instead of from an API.\n",
    "```py\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "\n",
    "def get_completion(prompt):\n",
    "    return pipeline(prompt).images[0]    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e16d426-7ac9-4034-b098-d8015602bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a dog in a park\"\n",
    "prompt = \"A beautiful castle surronded by small pink birds and a crocodile swimming in the river\"\n",
    "result = get_completion(prompt)\n",
    "\n",
    "# Assuming 'result' contains the binary data of the image in PNG format\n",
    "image = Image.open(io.BytesIO(result))\n",
    "\n",
    "# Convert the image binary data to base64 encoding\n",
    "image_data = io.BytesIO()\n",
    "image.save(image_data, format=\"PNG\")\n",
    "image_base64 = base64.b64encode(image_data.getvalue()).decode(\"utf-8\")\n",
    "html_code = f'<img src=\"data:image/png;base64,{image_base64}\" />'\n",
    "IPython.display.HTML(html_code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44a17b02-90fc-4b79-827a-290e0bbb5b40",
   "metadata": {},
   "source": [
    "## Generating with `gr.Interface()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c85db4-520e-4e9c-80e7-9d716f4b0b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr \n",
    "\n",
    "#A helper function to convert the PIL image to base64\n",
    "#so you can send it to the API\n",
    "def base64_to_pil(img_base64):\n",
    "    base64_decoded = base64.b64decode(img_base64)\n",
    "    byte_stream = io.BytesIO(base64_decoded)\n",
    "    pil_image = Image.open(byte_stream)\n",
    "    return pil_image\n",
    "\n",
    "def generate(prompt):\n",
    "    output = get_completion(prompt)\n",
    "    print(output)\n",
    "    image = Image.open(io.BytesIO(output))\n",
    "    # Convert the image binary data to base64 encoding\n",
    "    image_data = io.BytesIO()\n",
    "    image.save(image_data, format=\"PNG\")\n",
    "    image_base64 = base64.b64encode(image_data.getvalue()).decode(\"utf-8\")\n",
    "    result_image = base64_to_pil(image_base64)\n",
    "    return result_image\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=generate,\n",
    "                    inputs=[gr.Textbox(label=\"Your prompt\")],\n",
    "                    outputs=[gr.Image(label=\"Result\")],\n",
    "                    title=\"Image Generation with Stable Diffusion\",\n",
    "                    description=\"Generate any image with Stable Diffusion\",\n",
    "                    allow_flagging=\"never\",\n",
    "                    examples=[\"the spirit of a tamagotchi wandering in the city of Vienna\",\"a mecha robot in a favela\"])\n",
    "\n",
    "demo.launch()\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72119b58-8055-4ff4-a0bf-341f3d5a01ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "baa14223-b999-4ccb-885a-aa0f3697be86",
   "metadata": {},
   "source": [
    "## Building a more advanced interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568e77fa-4f0a-4143-ba54-6a639bbe4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr \n",
    "\n",
    "#A helper function to convert the PIL image to base64 \n",
    "# so you can send it to the API\n",
    "def base64_to_pil(img_base64):\n",
    "    base64_decoded = base64.b64decode(img_base64)\n",
    "    byte_stream = io.BytesIO(base64_decoded)\n",
    "    pil_image = Image.open(byte_stream)\n",
    "    return pil_image\n",
    "\n",
    "def generate(prompt, negative_prompt, steps, guidance, width, height):\n",
    "    params = {\n",
    "        \"negative_prompt\": negative_prompt,\n",
    "        \"num_inference_steps\": steps,\n",
    "        \"guidance_scale\": guidance,\n",
    "        \"width\": width,\n",
    "        \"height\": height\n",
    "    }\n",
    "    \n",
    "    output = get_completion(prompt, params)\n",
    "    print(output)\n",
    "    image = Image.open(io.BytesIO(output))\n",
    "    # Convert the image binary data to base64 encoding\n",
    "    image_data = io.BytesIO()\n",
    "    image.save(image_data, format=\"PNG\")\n",
    "    image_base64 = base64.b64encode(image_data.getvalue()).decode(\"utf-8\")\n",
    "    pil_image = base64_to_pil(image_base64)\n",
    "    return pil_image\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=generate,\n",
    "                    inputs=[\n",
    "                        gr.Textbox(label=\"Your prompt\"),\n",
    "                        gr.Textbox(label=\"Negative prompt\"),\n",
    "                        gr.Slider(label=\"Inference Steps\", minimum=1, maximum=100, value=25,\n",
    "                                 info=\"In how many steps will the denoiser denoise the image?\"),\n",
    "                        gr.Slider(label=\"Guidance Scale\", minimum=1, maximum=20, value=7, \n",
    "                                  info=\"Controls how much the text prompt influences the result\"),\n",
    "                        gr.Slider(label=\"Width\", minimum=64, maximum=512, step=64, value=512),\n",
    "                        gr.Slider(label=\"Height\", minimum=64, maximum=512, step=64, value=512),\n",
    "                    ],\n",
    "                    outputs=[gr.Image(label=\"Result\")],\n",
    "                    title=\"Image Generation with Stable Diffusion\",\n",
    "                    description=\"Generate any image with Stable Diffusion\",\n",
    "                    allow_flagging=\"never\"\n",
    "                    )\n",
    "\n",
    "demo.launch()\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c222a0ca-0fa7-45b6-90c2-e4c55158ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7801c9f5-e0f2-40a8-8f62-cda3384ee1a4",
   "metadata": {},
   "source": [
    "## `gr.Blocks()` to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10776193-a7ec-4746-9049-bd03747aa3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Image Generation with Stable Diffusion\")\n",
    "    prompt = gr.Textbox(label=\"Your prompt\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            negative_prompt = gr.Textbox(label=\"Negative prompt\")\n",
    "            steps = gr.Slider(label=\"Inference Steps\", minimum=1, maximum=100, value=25,\n",
    "                      info=\"In many steps will the denoiser denoise the image?\")\n",
    "            guidance = gr.Slider(label=\"Guidance Scale\", minimum=1, maximum=20, value=7,\n",
    "                      info=\"Controls how much the text prompt influences the result\")\n",
    "            width = gr.Slider(label=\"Width\", minimum=64, maximum=512, step=64, value=512)\n",
    "            height = gr.Slider(label=\"Height\", minimum=64, maximum=512, step=64, value=512)\n",
    "            btn = gr.Button(\"Submit\")\n",
    "        with gr.Column():\n",
    "            output = gr.Image(label=\"Result\")\n",
    "\n",
    "    btn.click(fn=generate, inputs=[prompt,negative_prompt,steps,guidance,width,height], outputs=[output])\n",
    "gr.close_all()\n",
    "demo.launch()\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f42f3-ddcf-42d5-8192-3f1d514efa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Image Generation with Stable Diffusion\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            prompt = gr.Textbox(label=\"Your prompt\") #Give prompt some real estate\n",
    "        with gr.Column(scale=1, min_width=50):\n",
    "            btn = gr.Button(\"Submit\") #Submit button side by side!\n",
    "    with gr.Accordion(\"Advanced options\", open=False): #Let's hide the advanced options!\n",
    "            negative_prompt = gr.Textbox(label=\"Negative prompt\")\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    steps = gr.Slider(label=\"Inference Steps\", minimum=1, maximum=100, value=25,\n",
    "                      info=\"In many steps will the denoiser denoise the image?\")\n",
    "                    guidance = gr.Slider(label=\"Guidance Scale\", minimum=1, maximum=20, value=7,\n",
    "                      info=\"Controls how much the text prompt influences the result\")\n",
    "                with gr.Column():\n",
    "                    width = gr.Slider(label=\"Width\", minimum=64, maximum=512, step=64, value=512)\n",
    "                    height = gr.Slider(label=\"Height\", minimum=64, maximum=512, step=64, value=512)\n",
    "    output = gr.Image(label=\"Result\") #Move the output up too\n",
    "            \n",
    "    btn.click(fn=generate, inputs=[prompt,negative_prompt,steps,guidance,width,height], outputs=[output])\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch()\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c51cc7-e4a2-4fb7-85f1-6ac28b64b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fcbae6-34b1-4370-b459-daa9c0cabea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1b1984a",
   "metadata": {},
   "source": [
    "# Describe and Generate Game\n",
    "\n",
    "_(Content from L4_Describe_and_Generate_game.ipynb)_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "537a7b33",
   "metadata": {},
   "source": [
    "Load your HF API key and relevant Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d968c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from IPython.display import Image, display, HTML\n",
    "from PIL import Image\n",
    "import base64 \n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hf_api_key = os.environ['HF_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5b5556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "import requests, json\n",
    "\n",
    "#Here we are going to call multiple endpoints!\n",
    "def get_completion(inputs, parameters=None, ENDPOINT_URL=\"\"):\n",
    "    headers = {\n",
    "      \"Authorization\": f\"Bearer {hf_api_key}\",\n",
    "      \"Content-Type\": \"application/json\"\n",
    "    }   \n",
    "    data = { \"inputs\": inputs }\n",
    "    if parameters is not None:\n",
    "        data.update({\"parameters\": parameters})\n",
    "    response = requests.request(\"POST\",\n",
    "                                ENDPOINT_URL,\n",
    "                                headers=headers,\n",
    "                                data=json.dumps(data))\n",
    "    return json.loads(response.content.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c631a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text-to-image\n",
    "TTI_ENDPOINT = os.environ['HF_API_TTI_BASE']\n",
    "#image-to-text\n",
    "ITT_ENDPOINT = os.environ['HF_API_ITT_BASE']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f95c81a0",
   "metadata": {},
   "source": [
    "## Building your game with `gr.Blocks()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bringing the functions from lessons 3 and 4!\n",
    "def image_to_base64_str(pil_image):\n",
    "    byte_arr = io.BytesIO()\n",
    "    pil_image.save(byte_arr, format='PNG')\n",
    "    byte_arr = byte_arr.getvalue()\n",
    "    return str(base64.b64encode(byte_arr).decode('utf-8'))\n",
    "\n",
    "def base64_to_pil(img_base64):\n",
    "    base64_decoded = base64.b64decode(img_base64)\n",
    "    byte_stream = io.BytesIO(base64_decoded)\n",
    "    pil_image = Image.open(byte_stream)\n",
    "    return pil_image\n",
    "\n",
    "def captioner(image):\n",
    "    base64_image = image_to_base64_str(image)\n",
    "    result = get_completion(base64_image, None, ITT_ENDPOINT)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "def generate(prompt):\n",
    "    output = get_completion(prompt, None, TTI_ENDPOINT)\n",
    "    result_image = base64_to_pil(output)\n",
    "    return result_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6193cd46",
   "metadata": {},
   "source": [
    "### First attempt, just captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba41fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr \n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Describe-and-Generate game üñçÔ∏è\")\n",
    "    image_upload = gr.Image(label=\"Your first image\",type=\"pil\")\n",
    "    btn_caption = gr.Button(\"Generate caption\")\n",
    "    caption = gr.Textbox(label=\"Generated caption\")\n",
    "    \n",
    "    btn_caption.click(fn=captioner, inputs=[image_upload], outputs=[caption])\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch()\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b21e2de9",
   "metadata": {},
   "source": [
    "### Let's add generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Describe-and-Generate game üñçÔ∏è\")\n",
    "    image_upload = gr.Image(label=\"Your first image\",type=\"pil\")\n",
    "    btn_caption = gr.Button(\"Generate caption\")\n",
    "    caption = gr.Textbox(label=\"Generated caption\")\n",
    "    btn_image = gr.Button(\"Generate image\")\n",
    "    image_output = gr.Image(label=\"Generated Image\")\n",
    "    btn_caption.click(fn=captioner, inputs=[image_upload], outputs=[caption])\n",
    "    btn_image.click(fn=generate, inputs=[caption], outputs=[image_output])\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch()\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT2']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15313486",
   "metadata": {},
   "source": [
    "### Doing it all at once! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_and_generate(image):\n",
    "    caption = captioner(image)\n",
    "    image = generate(caption)\n",
    "    return [caption, image]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Describe-and-Generate game üñçÔ∏è\")\n",
    "    image_upload = gr.Image(label=\"Your first image\",type=\"pil\")\n",
    "    btn_all = gr.Button(\"Caption and generate\")\n",
    "    caption = gr.Textbox(label=\"Generated caption\")\n",
    "    image_output = gr.Image(label=\"Generated Image\")\n",
    "\n",
    "    btn_all.click(fn=caption_and_generate, inputs=[image_upload], outputs=[caption, image_output])\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch()\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913470cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a494a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5de28bc",
   "metadata": {},
   "source": [
    "# L5: Chat with any LLM! üí¨"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "196ab24c",
   "metadata": {},
   "source": [
    "Load your HF API key and relevant Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d88821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import IPython.display\n",
    "from PIL import Image\n",
    "import base64 \n",
    "import requests \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "requests.adapters.DEFAULT_TIMEOUT = 60\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hf_api_key = os.environ['HF_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6890e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "import requests, json\n",
    "from text_generation import Client\n",
    "\n",
    "#FalcomLM-instruct endpoint on the text_generation library\n",
    "#client = Client(os.environ['HF_API_FALCOM_BASE'], headers={\"Authorization\": f\"Basic {hf_api_key}\"}, timeout=120)\n",
    "client = Client(os.environ['HF_API_FALCOM_BASE'], headers={\"Authorization\": f\"Bearer {hf_api_key}\"}, timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4714014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# model = \"tiiuae/falcon-40b-instruct\"\n",
    "# model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     trust_remote_code=True,\n",
    "#     device_map=\"auto\",\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     load_in_half_prec=True,\n",
    "# )\n",
    "\n",
    "# def print_sequences(query):\n",
    "#     query = \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\"\n",
    "#     sequences = pipeline(\n",
    "#         query, \n",
    "#         max_length=200,\n",
    "#         do_sample=True,\n",
    "#         top_k=10,\n",
    "#         num_return_sequences=1,\n",
    "#         eos_token_id=tokenizer.eos_token_id,\n",
    "#     )\n",
    "#     for seq in sequences:\n",
    "#         print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef39a6ba",
   "metadata": {},
   "source": [
    "## Building an app to chat with any LLM!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "692007d0",
   "metadata": {},
   "source": [
    "Here we'll be using an [Inference Endpoint](https://huggingface.co/inference-endpoints) for `falcon-40b-instruct` , one of best ranking open source LLM on the [ü§ó Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). \n",
    "\n",
    "To run it locally, one can use the [Transformers library](https://huggingface.co/docs/transformers/index) or the [text-generation-inference](https://github.com/huggingface/text-generation-inference) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92816a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Has math been invented or discovered?\"\n",
    "client.generate(prompt, max_new_tokens=256).generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d3484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Back to Lesson 2, time flies!\n",
    "import gradio as gr\n",
    "def generate(input, slider):\n",
    "    output = client.generate(input, max_new_tokens=slider).generated_text\n",
    "    return output\n",
    "\n",
    "demo = gr.Interface(fn=generate, inputs=[gr.Textbox(label=\"Prompt\"), gr.Slider(label=\"Max new tokens\", value=20,  maximum=1024, minimum=1)], outputs=[gr.Textbox(label=\"Completion\")])\n",
    "gr.close_all()\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT1']))\n",
    "demo.launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78206396",
   "metadata": {},
   "source": [
    "## `gr.Chatbot()` to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f782097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def respond(message, chat_history):\n",
    "        #No LLM here, just respond with a random pre-made message\n",
    "        bot_message = random.choice([\"Tell me more about it\", \n",
    "                                     \"Cool, but I'm not interested\", \n",
    "                                     \"Hmmmm, ok then\"]) \n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "gr.close_all()\n",
    "demo.launch()\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28018731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_prompt(message, chat_history):\n",
    "    prompt = \"\"\n",
    "    for turn in chat_history:\n",
    "        user_message, bot_message = turn\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    return prompt\n",
    "\n",
    "def respond(message, chat_history):\n",
    "        formatted_prompt = format_chat_prompt(message, chat_history)\n",
    "        bot_message = client.generate(formatted_prompt,\n",
    "                                     max_new_tokens=1024,\n",
    "                                     stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"]).generated_text\n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "gr.close_all()\n",
    "demo.launch()\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c967c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding other advanced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d73f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_prompt(message, chat_history, instruction):\n",
    "    prompt = f\"System:{instruction}\"\n",
    "    for turn in chat_history:\n",
    "        user_message, bot_message = turn\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    return prompt\n",
    "\n",
    "def respond(message, chat_history, instruction, temperature=0.7):\n",
    "    prompt = format_chat_prompt(message, chat_history, instruction)\n",
    "    chat_history = chat_history + [[message, \"\"]]\n",
    "    stream = client.generate_stream(prompt,\n",
    "                                      max_new_tokens=1024,\n",
    "                                      stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"],\n",
    "                                      temperature=temperature)\n",
    "                                      #stop_sequences to not generate the user answer\n",
    "    acc_text = \"\"\n",
    "    #Streaming the tokens\n",
    "    for idx, response in enumerate(stream):\n",
    "            text_token = response.token.text\n",
    "\n",
    "            if response.details:\n",
    "                return\n",
    "\n",
    "            if idx == 0 and text_token.startswith(\" \"):\n",
    "                text_token = text_token[1:]\n",
    "\n",
    "            acc_text += text_token\n",
    "            last_turn = list(chat_history.pop(-1))\n",
    "            last_turn[-1] += acc_text\n",
    "            chat_history = chat_history + [last_turn]\n",
    "            yield \"\", chat_history\n",
    "            acc_text = \"\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    with gr.Accordion(label=\"Advanced options\",open=False):\n",
    "        system = gr.Textbox(label=\"System message\", lines=2, value=\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\")\n",
    "        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1)\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot]) #Press enter to submit\n",
    "gr.close_all()\n",
    "demo.queue().launch()\n",
    "# demo.queue().launch(share=True, server_port=int(os.environ['PORT4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf44635",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
