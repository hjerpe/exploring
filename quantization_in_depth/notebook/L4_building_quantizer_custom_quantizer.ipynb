{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L4-A - Building your own Quantizer: Custom Build an 8-Bit Quantizer\n",
    "\n",
    "In this lesson, you will learn how to compress any model in 8-bit precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: class `W8A16LinearLayer`\n",
    "\n",
    "- Build the target class, `W8A16LinearLayer()`, that will be responsible for quantizing your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - `w8_a16_forward` Function\n",
    "\n",
    "-\n",
    "```Python\n",
    "W8A16LinearLayer\n",
    "                    # 8-bit  # 16-bit         # optional\n",
    "* w8_a16_forward -> weights, input,   scales, bias=None\n",
    "                    \n",
    "```\n",
    "- Cast the 8-bit `weights` to the same data type as the `input`, \"casted weights\",\n",
    "- keeping the \"casted weights\" in the same range as before, [-128, 127]\n",
    "- Next, $$(({inputs} \\cdot \\text{``casted weights''}) * {scale}) + {bias}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_int8 = torch.randint(-128, 127, (32, 16)).to(torch.int8)\n",
    "random_hs = torch.randn((1, 16), dtype=torch.bfloat16)\n",
    "scales = torch.randn((1, 32), dtype=torch.bfloat16)\n",
    "bias = torch.randn((1, 32), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Since the values are random, what you see in the video might be different than what you will get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.linear(random_hs, random_int8.to(random_hs.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.linear(random_hs, random_int8.to(random_hs.dtype)) * scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(F.linear(random_hs, random_int8.to(random_hs.dtype)) * scales) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement all this as a function, `w8_a16_forward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w8_a16_forward(weight, input, scales, bias=None):\n",
    "    \n",
    "    casted_weights = weight.to(input.dtype)\n",
    "    output = F.linear(input, casted_weights) * scales\n",
    "    \n",
    "    if bias is not None:\n",
    "        output = output + bias\n",
    "      \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With bias:\\n\\n\", \n",
    "      w8_a16_forward(random_int8, random_hs, scales, bias))\n",
    "\n",
    "print(\"\\nWithout bias:\\n\\n\", \n",
    "      w8_a16_forward(random_int8, random_hs, scales))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - `init` Function of class `W8A16LinearLayer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is how the `init` is of [PyTorch Linear layer](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear):\n",
    "```Python\n",
    "def __init__(self, in_features, out_features, bias=True,\n",
    "             device=None, dtype=None)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### running this will result in an error\n",
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.int8_weights = nn.Parameter(torch.Tensor([0, 1]\n",
    "                                     ).to(dtype=torch.int8))\n",
    "\n",
    "try:\n",
    "    \n",
    "    W8A16LinearLayer(1, 1)\n",
    "    \n",
    "except Exception as error:\n",
    "    print(\"\\033[91m\", type(error).__name__, \": \", error, \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"int8_weights\",\n",
    "            torch.randint(\n",
    "                -128, 127, (out_features, in_features), dtype=torch.int8\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.register_buffer(\"scales\", \n",
    "                             torch.randn((out_features), dtype=dtype))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", \n",
    "                                 torch.randn((1, out_features), \n",
    "                                             dtype=dtype))\n",
    "        \n",
    "        else:\n",
    "            self.bias = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_instance = W8A16LinearLayer(16, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dummy_instance.int8_weights.shape)\n",
    "print(dummy_instance.scales.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - `forward` Function of class `W8A16LinearLayer`\n",
    "\n",
    "- Use the `w8_a16_forward` defined earlier (Step 1.1) to define the `forward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"int8_weights\",\n",
    "            torch.randint(\n",
    "                -128, 127, (out_features, in_features), dtype=torch.int8\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.register_buffer(\"scales\", \n",
    "                             torch.randn((out_features), dtype=dtype))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", \n",
    "                                 torch.randn((1, out_features), \n",
    "                                             dtype=dtype))\n",
    "        \n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return w8_a16_forward(self.int8_weights, \n",
    "                              input, self.scales, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = W8A16LinearLayer(16, 32)\n",
    "dummy_hidden_states = torch.randn(1, 6, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module(dummy_hidden_states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module(dummy_hidden_states).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - `quantize` Function of class `W8A16LinearLayer`\n",
    "\n",
    "- `quantize` function will dynamically quantize half-precision weights into `torch.int8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"int8_weights\",\n",
    "            torch.randint(\n",
    "                -128, 127, (out_features, in_features), dtype=torch.int8\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.register_buffer(\"scales\", \n",
    "                             torch.randn((out_features), dtype=dtype))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", \n",
    "                                 torch.randn((1, out_features), \n",
    "                                             dtype=dtype))\n",
    "        \n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def quantize(self, weights):\n",
    "        w_fp32 = weights.clone().to(torch.float32)\n",
    "\n",
    "        scales = w_fp32.abs().max(dim=-1).values / 127\n",
    "        scales = scales.to(weights.dtype)\n",
    "\n",
    "        int8_weights = torch.round(weights\n",
    "                        /scales.unsqueeze(1)).to(torch.int8)\n",
    "\n",
    "        self.int8_weights = int8_weights\n",
    "        self.scales = scales\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return w8_a16_forward(self.int8_weights, \n",
    "                              input, self.scales, self.bias)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = W8A16LinearLayer(4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weights before:\\n\" , module.int8_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_matrix = torch.randn((4, 8), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.quantize(random_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weights After:\\n\" , module.int8_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.scales.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.int8_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dequantized weights\n",
    "module.int8_weights * module.scales.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### original weights\n",
    "random_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(random_matrix - module.int8_weights \n",
    " * module.scales.unsqueeze(1)).abs().mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default_venv",
   "language": "python",
   "name": "default_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
