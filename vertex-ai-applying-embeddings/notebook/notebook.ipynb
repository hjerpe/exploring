{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "207942ee-1cad-4ff4-a64a-90aebc10d040",
   "metadata": {},
   "source": [
    "# Getting Started With Text Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed132594-2b03-449b-87e2-2529d60aa05a",
   "metadata": {},
   "source": [
    "#### Project environment setup\n",
    "\n",
    "- Load credentials and relevant Python Libraries\n",
    "- If you were running this notebook locally, you would first install Vertex AI.  In this classroom, this is already installed.\n",
    "```Python\n",
    "!pip install google-cloud-aiplatform\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2931ae6b-23c1-4d02-a724-1bc0b1b9a820",
   "metadata": {
    "height": 66
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "PROJECT_ID = os.environ['PROJECT_ID']\n",
    "REGION = os.environ['REGION']\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"REGION: {REGION}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3cfcd44-8eba-4eee-b7b4-a7be3c876685",
   "metadata": {},
   "source": [
    "#### Enter project details"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e448c9f-8311-420b-b408-a1fb712bbff4",
   "metadata": {},
   "source": [
    "#### Use the embeddings model\n",
    "- Import and load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681bfe86-6880-4afa-bdd6-77a8dfbc5f21",
   "metadata": {
    "height": 49
   },
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62b23a-b423-4651-88f1-b00509900be3",
   "metadata": {
    "height": 66
   },
   "outputs": [],
   "source": [
    "embedding_model = TextEmbeddingModel.from_pretrained(\n",
    "    \"textembedding-gecko\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce2289d4-882f-4336-a720-75e80cac6900",
   "metadata": {},
   "source": [
    "- Generate a word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05aea17-cb72-4c7f-9da2-df0ee954e663",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "embedding = embedding_model.get_embeddings(\n",
    "    [\"life\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "685a6bc7-455a-4783-93d5-fa491e82e828",
   "metadata": {},
   "source": [
    "- The returned object is a list with a single `TextEmbedding` object.\n",
    "- The `TextEmbedding.values` field stores the embeddings in a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea301db6-a439-4c0b-aae1-5e6ef943419c",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "vector = embedding[0].values\n",
    "print(f\"Length = {len(vector)}\")\n",
    "print(vector[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b803ae9-dd2e-421e-bc06-d6e7e0898b06",
   "metadata": {},
   "source": [
    "- Generate a sentence embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a4cd7-f89a-4931-ae16-519d1feb5197",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "embedding = embedding_model.get_embeddings(\n",
    "    [\"What is the meaning of life?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d300e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca0182e-1744-4fbb-b240-2d193ec907d4",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "vector = embedding[0].values\n",
    "print(f\"Length = {len(vector)}\")\n",
    "print(vector[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7179f3de-e405-4c97-aaf5-055a99c464b6",
   "metadata": {},
   "source": [
    "#### Similarity\n",
    "\n",
    "- Calculate the similarity between two sentences as a number between 0 and 1.\n",
    "- Try out your own sentences and check if the similarity calculations match your intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b2520b-9687-4eed-be07-9ff75f1fcd74",
   "metadata": {
    "height": 49
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04483bf2-4d5d-4fd2-9aec-27711f2c780c",
   "metadata": {
    "height": 236
   },
   "outputs": [],
   "source": [
    "emb_1 = embedding_model.get_embeddings(\n",
    "    [\"What is the meaning of life?\"]) # 42!\n",
    "\n",
    "emb_2 = embedding_model.get_embeddings(\n",
    "    [\"How does one spend their time well on Earth?\"])\n",
    "\n",
    "emb_3 = embedding_model.get_embeddings(\n",
    "    [\"Would you like a salad?\"])\n",
    "\n",
    "vec_1 = [emb_1[0].values]\n",
    "vec_2 = [emb_2[0].values]\n",
    "vec_3 = [emb_3[0].values]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0e70a55-21a4-4532-be59-1b9749d639b0",
   "metadata": {},
   "source": [
    "- Note: the reason we wrap the embeddings (a Python list) in another list is because the `cosine_similarity` function expects either a 2D numpy array or a list of lists.\n",
    "```Python\n",
    "vec_1 = [emb_1[0].values]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b370eaa-a3b7-44e4-90a2-6dc84e460122",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "print(cosine_similarity(vec_1,vec_2)) \n",
    "print(cosine_similarity(vec_2,vec_3))\n",
    "print(cosine_similarity(vec_1,vec_3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe4b69f0-e64b-477d-9466-72ba1be05043",
   "metadata": {},
   "source": [
    "#### From word to sentence embeddings\n",
    "- One possible way to calculate sentence embeddings from word embeddings is to take the average of the word embeddings.\n",
    "- This ignores word order and context, so two sentences with different meanings, but the same set of words will end up with the same sentence embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88fff09-a268-4a29-a04b-7c84a588db80",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "in_1 = \"The kids play in the park.\"\n",
    "in_2 = \"The play was for kids in the park.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40437875-53db-4c88-8882-a2d6266d43d3",
   "metadata": {},
   "source": [
    "- Remove stop words like [\"the\", \"in\", \"for\", \"an\", \"is\"] and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab3a74-ec19-48cc-b5fb-1f0bd520b2c2",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "in_pp_1 = [\"kids\", \"play\", \"park\"]\n",
    "in_pp_2 = [\"play\", \"kids\", \"park\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2202a83-a704-41aa-ae2a-ba9ee2afd351",
   "metadata": {},
   "source": [
    "- Generate one embedding for each word.  So this is a list of three lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412df546-e2e2-4a90-9cd4-86b4887ab908",
   "metadata": {
    "height": 49
   },
   "outputs": [],
   "source": [
    "embeddings_1 = [emb.values for emb in embedding_model.get_embeddings(in_pp_1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3ac14b1-3d55-4227-9014-88447d5a08b8",
   "metadata": {},
   "source": [
    "- Use numpy to convert this list of lists into a 2D array of 3 rows and 768 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bcf45a-fb96-4838-a676-fbf52afc0b32",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "emb_array_1 = np.stack(embeddings_1)\n",
    "print(emb_array_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d20a4e-c56d-4a23-83ff-97d98e86cf32",
   "metadata": {
    "height": 83
   },
   "outputs": [],
   "source": [
    "embeddings_2 = [emb.values for emb in embedding_model.get_embeddings(in_pp_2)]\n",
    "emb_array_2 = np.stack(embeddings_2)\n",
    "print(emb_array_2.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a83a56b-404d-4517-a666-c161f92ae48f",
   "metadata": {},
   "source": [
    "- Take the average embedding across the 3 word embeddings \n",
    "- You'll get a single embedding of length 768."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d9b65-a0ce-49eb-9627-b550053663c3",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "emb_1_mean = emb_array_1.mean(axis = 0) \n",
    "print(emb_1_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba79b5-ba81-4c7e-b1d8-642b428e16ec",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "emb_2_mean = emb_array_2.mean(axis = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdfe77e1-0b0c-407e-b179-862db2e2e454",
   "metadata": {},
   "source": [
    "- Check to see that taking an average of word embeddings results in two sentence embeddings that are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd51c8-cee8-46f1-b717-0a44ff8f1d1c",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "print(emb_1_mean[:4])\n",
    "print(emb_2_mean[:4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8014b28-ddfa-43e0-969b-5bd4ed0f6fdb",
   "metadata": {},
   "source": [
    "#### Get sentence embeddings from the model.\n",
    "- These sentence embeddings account for word order and context.\n",
    "- Verify that the sentence embeddings are not the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f69ff3c-dd02-45ae-b34c-a10f3d78614b",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "print(in_1)\n",
    "print(in_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dcfe77-013f-4cdc-a6c5-455c364b2507",
   "metadata": {
    "height": 66
   },
   "outputs": [],
   "source": [
    "embedding_1 = embedding_model.get_embeddings([in_1])\n",
    "embedding_2 = embedding_model.get_embeddings([in_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e998f-01b4-4d90-b509-cb03f26c106c",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "vector_1 = embedding_1[0].values\n",
    "print(vector_1[:4])\n",
    "vector_2 = embedding_2[0].values\n",
    "print(vector_2[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccfbb6f-c72b-4f5e-a86c-265bab07d9e1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34589b64-1a75-49b1-8e5e-1ea3cd857111",
   "metadata": {},
   "source": [
    "# Visualizing Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88adc7f3-52b6-432f-80c4-995605b556eb",
   "metadata": {},
   "source": [
    "#### Project environment setup\n",
    "\n",
    "- Load credentials and relevant Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeebfae-c08f-4c9d-a714-9090a04fbc5c",
   "metadata": {
    "height": 66
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "PROJECT_ID = os.environ['PROJECT_ID']\n",
    "REGION = os.environ['REGION']\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"REGION: {REGION}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dea550bf-d3fb-4ae2-80da-4f3d9179a9b9",
   "metadata": {},
   "source": [
    "## Embeddings capture meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e2623-ca37-43ed-90d5-d0e52cc6fba5",
   "metadata": {
    "height": 338
   },
   "outputs": [],
   "source": [
    "in_1 = \"Missing flamingo discovered at swimming pool\"\n",
    "\n",
    "in_2 = \"Sea otter spotted on surfboard by beach\"\n",
    "\n",
    "in_3 = \"Baby panda enjoys boat ride\"\n",
    "\n",
    "\n",
    "in_4 = \"Breakfast themed food truck beloved by all!\"\n",
    "\n",
    "in_5 = \"New curry restaurant aims to please!\"\n",
    "\n",
    "\n",
    "in_6 = \"Python developers are wonderful people\"\n",
    "\n",
    "in_7 = \"TypeScript, C++ or Java? All are great!\"\n",
    "\n",
    "\n",
    "input_text_lst_news = [in_1, in_2, in_3, in_4, in_5, in_6, in_7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46ec871",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_1 = \"These apples are always crisp and delicious! My family loves them.\"\n",
    "in_2 = \"I bought these apples, and they were mushy and tasted bad. Not happy at all.\"\n",
    "in_3 = \"The cereal selection here is fantastic. I can always find my favorite brands.\"\n",
    "in_4 = \"The cereal I bought was stale. It was like eating cardboard.\"\n",
    "\n",
    "in_5 = \"Just discovered the new organic section at my local grocery store! 🌿 So excited to shop for healthier options. Thanks, @GreenGroceryCo! #HealthyEating #OrganicFood\"\n",
    "in_6 = \"Visited @SuperMart today for my weekly fruit haul. Their produce section is always on point! 🍎🥭🍇 #FreshProduce #SuperMartLove\"\n",
    "in_7 = \"Seriously disappointed with my recent trip to @BudgetMart. Out of stock on half the items I needed. 😡 #PoorService #BudgetMartFail\"\n",
    "\n",
    "in_8 = \"In a recent survey of 1,000 consumers, 72% indicated a growing preference for organic and locally sourced products when shopping for groceries.\"\n",
    "in_9 = \"Our market research shows that plant-based meat alternatives have seen a 35% increase in sales over the past year, indicating a rising trend in health-conscious choices.\"\n",
    "in_10 = \"Analysis of competitor pricing strategies reveals that Grocery Chain A consistently offers lower prices on staple products, attracting cost-conscious shoppers.\"\n",
    "in_11 = \"Our research indicates that urban millennials are the fastest-growing segment of online grocery shoppers, with a 42% increase in the past two years.\"\n",
    "\n",
    "in_12 = \"I've noticed some safety hazards in the back storage area that need attention. It's crucial to address these issues to ensure the safety of our team and prevent accidents.\"\n",
    "in_13 = \"Our team has been working exceptionally hard lately, and it would be motivating to receive more recognition for our efforts, perhaps through an 'Employee of the Month' program.\"\n",
    "in_14 = \"I'd like to see more opportunities for professional development and training. It would benefit both employees and the company as we stay updated on industry trends.\"\n",
    "in_15 = \"Improving communication between shifts and departments would help streamline operations and reduce misunderstandings. Clearer communication channels are essential.\"\n",
    "\n",
    "\n",
    "input_text_lst_news = [\n",
    "    in_1,\n",
    "    in_2,\n",
    "    in_3,\n",
    "    in_4,\n",
    "    in_5,\n",
    "    in_6,\n",
    "    in_7,\n",
    "    in_8,\n",
    "    in_9,\n",
    "    in_10,\n",
    "    in_11,\n",
    "    in_12,\n",
    "    in_13,\n",
    "    in_14,\n",
    "    in_15,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ce640-0505-4d01-9620-94502af9eb70",
   "metadata": {
    "height": 117
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "\n",
    "embedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b270e703-62fd-47cf-893e-3fe7a1282f29",
   "metadata": {},
   "source": [
    "- Get embeddings for all pieces of text.\n",
    "- Store them in a 2D NumPy array (one row for each embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dbf38d-5f88-4ee0-804c-87099b5d6693",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for input_text in input_text_lst_news:\n",
    "    emb = embedding_model.get_embeddings([input_text])[0].values\n",
    "    embeddings.append(emb)\n",
    "\n",
    "embeddings_array = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129943c-d624-4ada-9dbf-8277fc12554d",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "print(\"Shape: \" + str(embeddings_array.shape))\n",
    "print(embeddings_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "deffaf3d-f57e-4b14-a391-620194c5d354",
   "metadata": {},
   "source": [
    "#### Reduce embeddings from 768 to 2 dimensions for visualization\n",
    "- We'll use principal component analysis (PCA).\n",
    "- You can learn more about PCA in [this video](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/73zWO/reducing-the-number-of-features-optional) from the Machine Learning Specialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa6ec2-fa3b-47da-831a-1892dd8560dc",
   "metadata": {
    "height": 134
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA for 2D visualization\n",
    "PCA_model = PCA(n_components=2)\n",
    "PCA_model.fit(embeddings_array)\n",
    "new_values = PCA_model.transform(embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a13b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "text = pd.DataFrame(input_text_lst_news, columns=[\"text\"])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1481b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(text.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b525cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import umap_plot\n",
    "umap_plot(emb=embeddings_array, text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8743daf4-31f3-467d-be38-fc5984857682",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "print(\"Shape: \" + str(new_values.shape))\n",
    "print(new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9328db18-bb41-4405-8139-b62f69850a2a",
   "metadata": {
    "height": 134
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "%matplotlib ipympl\n",
    "\n",
    "from utils import plot_2D\n",
    "plot_2D(new_values[:,0], new_values[:,1], input_text_lst_news)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20c789c6-7f12-41ab-96a0-15aea8082a1a",
   "metadata": {},
   "source": [
    "#### Embeddings and Similarity\n",
    "- Plot a heat map to compare the embeddings of sentences that are similar and sentences that are dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dcb75a-abd7-4a45-a78b-3800fb4cf559",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "in_1 = \"\"\"He couldn’t desert \n",
    "          his post at the power plant.\"\"\"\n",
    "\n",
    "in_2 = \"\"\"The power plant needed \n",
    "          him at the time.\"\"\"\n",
    "\n",
    "in_3 = \"\"\"Cacti are able to \n",
    "          withstand dry environments.\"\"\"\n",
    "\n",
    "in_4 = \"\"\"Desert plants can \n",
    "          survive droughts.\"\"\"\n",
    "\n",
    "input_text_lst_sim = [in_1, in_2, in_3, in_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c0c3d-87d9-4601-bc03-dd998bf12166",
   "metadata": {
    "height": 134
   },
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for input_text in input_text_lst_sim:\n",
    "    emb = embedding_model.get_embeddings([input_text])[0].values\n",
    "    embeddings.append(emb)\n",
    "\n",
    "embeddings_array = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5057cff-18dc-42a8-918a-86e54c2e7c0f",
   "metadata": {
    "height": 134
   },
   "outputs": [],
   "source": [
    "from utils import plot_heatmap\n",
    "\n",
    "y_labels = input_text_lst_sim\n",
    "\n",
    "# Plot the heatmap\n",
    "plot_heatmap(embeddings_array, y_labels=y_labels, title=\"Embeddings Heatmap\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b984ee27-fa85-4839-b799-748b9d82107e",
   "metadata": {},
   "source": [
    "Note: the heat map won't show everything because there are 768 columns to show.  To adjust the heat map with your mouse:\n",
    "- Hover your mouse over the heat map.  Buttons will appear on the left of the heatmap.  Click on the button that has a vertical and horizontal double arrow (they look like axes).\n",
    "- Left click and drag to move the heat map left and right.\n",
    "- Right click and drag up to zoom in.\n",
    "- Right click and drag down to zoom out."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e075dc41-18e8-40e1-8435-d0f972d0c8ab",
   "metadata": {},
   "source": [
    "#### Compute cosine similarity\n",
    "- The `cosine_similarity` function expects a 2D array, which is why we'll wrap each embedding list inside another list.\n",
    "- You can verify that sentence 1 and 2 have a higher similarity compared to sentence 1 and 4, even though sentence 1 and 4 both have the words \"desert\" and \"plant\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff11a0-ae2b-4898-b565-3be7439a27b0",
   "metadata": {
    "height": 49
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e99af9-2bf6-4ba5-ad72-809a12c6fca1",
   "metadata": {
    "height": 66
   },
   "outputs": [],
   "source": [
    "def compare(embeddings, idx1, idx2):\n",
    "    return cosine_similarity([embeddings[idx1]], [embeddings[idx2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8842d87c-1abf-4908-9ff7-ab8b3200532e",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "print(in_1)\n",
    "print(in_2)\n",
    "print(compare(embeddings, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35860466-0090-4c86-b39a-baf6926b613c",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "print(in_1)\n",
    "print(in_4)\n",
    "print(compare(embeddings, 0, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3f2115e-785b-45a6-acb0-431e015420d5",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a39f45f",
   "metadata": {},
   "source": [
    "# Applications of Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da4f2d23",
   "metadata": {},
   "source": [
    "#### Load Stack Overflow questions and answers from BigQuery\n",
    "- BigQuery is Google Cloud's serverless data warehouse.\n",
    "- We'll get the first 500 posts (questions and answers) for each programming language: Python, HTML, R, and CSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60da3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420839dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "PROJECT_ID = os.environ['PROJECT_ID']\n",
    "REGION = os.environ['REGION']\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"REGION: {REGION}\")\n",
    "print(f\"REGION: {os.environ['GOOGLE_APPLICATION_CREDENTIALS']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8518491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.service_account import Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c7f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your service account key file\n",
    "key_path = '../../.keys/alpine-inkwell-403410-78bfa7dd2500.json' #Path to the json key associated with your service account from google cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create credentials object\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    key_path,\n",
    "    scopes=['https://www.googleapis.com/auth/cloud-platform'])\n",
    "\n",
    "if credentials.expired:\n",
    "    credentials.refresh(Request())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dbcbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bq_query(sql):\n",
    "\n",
    "    # Create BQ client\n",
    "    bq_client = bigquery.Client(project = PROJECT_ID)\n",
    "\n",
    "    # Try dry run before executing query to catch any errors\n",
    "    job_config = bigquery.QueryJobConfig(dry_run=True, \n",
    "                                         use_query_cache=False)\n",
    "    bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    # If dry run succeeds without errors, proceed to run query\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    client_result = bq_client.query(sql, \n",
    "                                    job_config=job_config)\n",
    "\n",
    "    job_id = client_result.job_id\n",
    "\n",
    "    # Wait for query/job to finish running. then get & return data frame\n",
    "    df = client_result.result().to_arrow().to_pandas()\n",
    "    print(f\"Finished job_id: {job_id}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e498d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of programming language tags we want to query\n",
    "\n",
    "language_list = [\"python\", \"html\", \"r\", \"css\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f654c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_df = pd.DataFrame()\n",
    "\n",
    "for language in language_list:\n",
    "    \n",
    "    print(f\"generating {language} dataframe\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        CONCAT(q.title, q.body) as input_text,\n",
    "        a.body AS output_text\n",
    "    FROM\n",
    "        `bigquery-public-data.stackoverflow.posts_questions` q\n",
    "    JOIN\n",
    "        `bigquery-public-data.stackoverflow.posts_answers` a\n",
    "    ON\n",
    "        q.accepted_answer_id = a.id\n",
    "    WHERE \n",
    "        q.accepted_answer_id IS NOT NULL AND \n",
    "        REGEXP_CONTAINS(q.tags, \"{language}\") AND\n",
    "        a.creation_date >= \"2020-01-01\"\n",
    "    LIMIT \n",
    "        500\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    language_df = run_bq_query(query)\n",
    "    language_df[\"category\"] = language\n",
    "    so_df = pd.concat([so_df, language_df], \n",
    "                      ignore_index = True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3d363dc",
   "metadata": {},
   "source": [
    "- You can reuse the above code to run your own queries if you are using Google Cloud's BigQuery service.\n",
    "- In this classroom, if you run into any issues, you can load the same data from a csv file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d194e632",
   "metadata": {},
   "source": [
    "#### Generate text embeddings\n",
    "- To generate embeddings for a dataset of texts, we'll need to group the sentences together in batches and send batches of texts to the model.\n",
    "- The API currently can take batches of up to 5 pieces of text per API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af258d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae020549",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextEmbeddingModel.from_pretrained(\n",
    "    \"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ff6410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2aa013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator function to yield batches of sentences\n",
    "\n",
    "def generate_batches(sentences, batch_size = 5):\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        yield sentences[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ad6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_questions = so_df[0:200].input_text.tolist() \n",
    "batches = generate_batches(sentences = so_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b3974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(batches)\n",
    "len(batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62ef8fcb",
   "metadata": {},
   "source": [
    "#### Get embeddings on a batch of data\n",
    "- This helper function calls `model.get_embeddings()` on the batch of data, and returns a list containing the embeddings for each text in that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6f7c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts_to_embeddings(sentences):\n",
    "    try:\n",
    "        embeddings = model.get_embeddings(sentences)\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception:\n",
    "        return [None for _ in range(len(sentences))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a9964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_embeddings = encode_texts_to_embeddings(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e25c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{len(batch_embeddings)} embeddings of size \\\n",
    "{len(batch_embeddings[0])}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac389069",
   "metadata": {},
   "source": [
    "#### Code for getting data on an entire data set\n",
    "- Most API services have rate limits, so we've provided a helper function (in utils.py) that you could use to wait in-between API calls.\n",
    "- If the code was not designed to wait in-between API calls, you may not receive embeddings for all batches of text.\n",
    "- This particular service can handle 20 calls per minute.  In calls per second, that's 20 calls divided by 60 seconds, or `20/60`.\n",
    "\n",
    "```Python\n",
    "from utils import encode_text_to_embedding_batched\n",
    "\n",
    "so_questions = so_df.input_text.tolist()\n",
    "question_embeddings = encode_text_to_embedding_batched(\n",
    "                            sentences=so_questions,\n",
    "                            api_calls_per_second = 20/60, \n",
    "                            batch_size = 5)\n",
    "```\n",
    "\n",
    "In order to handle limits of this classroom environment, we're not going to run this code to embed all of the data. But you can adapt this code for your own projects and datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40a6278c",
   "metadata": {},
   "source": [
    "#### Load the data from file\n",
    "- We'll load the stack overflow questions, answers, and category labels (Python, HTML, R, CSS) from a .csv file.\n",
    "- We'll load the embeddings of the questions (which we've precomputed with batched calls to `model.get_embeddings()`), from a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03353163",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23df8bcc",
   "metadata": {},
   "source": [
    "#### Code takes ~ 20 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e20c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import encode_text_to_embedding_batched\n",
    "#\n",
    "#so_questions = so_df.input_text.tolist()\n",
    "#question_embeddings = encode_text_to_embedding_batched(\n",
    "#                            sentences=so_questions,\n",
    "#                            api_calls_per_second = 20/60, \n",
    "#                            batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_df = pd.read_csv('data/so_database_app.csv')\n",
    "so_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f77d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f08ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/question_embeddings_app.pkl', 'rb') as file:\n",
    "    question_embeddings = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b69d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \" + str(question_embeddings.shape))\n",
    "print(question_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd9ba97e",
   "metadata": {},
   "source": [
    "#### Cluster the embeddings of the Stack Overflow questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20978523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_dataset = question_embeddings[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2\n",
    "kmeans = KMeans(n_clusters=n_clusters, \n",
    "                random_state=0, \n",
    "                ).fit(clustering_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb9d5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19312dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_model = PCA(n_components=2)\n",
    "PCA_model.fit(clustering_dataset)\n",
    "new_values = PCA_model.transform(clustering_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f569752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a0083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import clusters_2D\n",
    "clusters_2D(x_values = new_values[:,0], y_values = new_values[:,1], \n",
    "            labels = so_df[:1000], kmeans_labels = kmeans_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc74655",
   "metadata": {},
   "source": [
    "- Clustering is able to identify two distinct clusters of HTML or Python related questions, without being given the category labels (HTML or Python)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "920081f8",
   "metadata": {},
   "source": [
    "## Anomaly / Outlier detection\n",
    "\n",
    "- We can add an anomalous piece of text and check if the outlier (anomaly) detection algorithm (Isolation Forest) can identify it as an outlier (anomaly), based on its embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75641213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c02cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"I am making cookies but don't \n",
    "                remember the correct ingredient proportions. \n",
    "                I have been unable to find \n",
    "                anything on the web.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defe0b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.get_embeddings([input_text])[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_l = question_embeddings.tolist()\n",
    "embeddings_l.append(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01f0dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = np.array(embeddings_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80637173",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \" + str(embeddings_array.shape))\n",
    "print(embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3cbee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the outlier text to the end of the stack overflow dataframe\n",
    "so_df = pd.read_csv('so_database_app.csv')\n",
    "new_row = pd.Series([input_text, None, \"baking\"], \n",
    "                    index=so_df.columns)\n",
    "so_df.loc[len(so_df)+1] = new_row\n",
    "so_df.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bedff86",
   "metadata": {},
   "source": [
    "#### Use Isolation Forest to identify potential outliers\n",
    "\n",
    "- `IsolationForest` classifier will predict `-1` for potential outliers, and `1` for non-outliers.\n",
    "- You can inspect the rows that were predicted to be potential outliers and verify that the question about baking is predicted to be an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9affe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = IsolationForest(contamination=0.005, \n",
    "                      random_state = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8450154",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.fit_predict(embeddings_array)\n",
    "\n",
    "print(f\"{len(preds)} predictions. Set of possible values: {set(preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88126f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_df.loc[preds == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471db3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "so_df.loc[preds == -1][\"input_text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d879dcb6",
   "metadata": {},
   "source": [
    "#### Remove the outlier about baking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e117a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_df = so_df.drop(so_df.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b0aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7aabb44",
   "metadata": {},
   "source": [
    "## Classification\n",
    "- Train a random forest model to classify the category of a Stack Overflow question (as either Python, R, HTML or CSS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe1502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a575e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae0ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-load the dataset from file\n",
    "so_df = pd.read_csv('so_database_app.csv')\n",
    "X = question_embeddings\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0344f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = so_df['category'].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300aa8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44b2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "908cbaf7",
   "metadata": {},
   "source": [
    "#### You can check the predictions on a few questions from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926bb8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f7ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) # compute accuracy\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a67abc2c",
   "metadata": {},
   "source": [
    "#### Try out the classifier on some questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a number between 0 and 1999\n",
    "i = 2\n",
    "label = so_df.loc[i,'category']\n",
    "question = so_df.loc[i,'input_text']\n",
    "\n",
    "# get the embedding of this question and predict its category\n",
    "question_embedding = model.get_embeddings([question])[0].values\n",
    "pred = clf.predict([question_embedding])\n",
    "\n",
    "print(f\"For question {i}, the prediction is `{pred[0]}`\")\n",
    "print(f\"The actual label is `{label}`\")\n",
    "print(\"The question text is:\")\n",
    "print(\"-\"*50)\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47433770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4f357d1-bbc9-4848-828c-00b36144ebdc",
   "metadata": {},
   "source": [
    "# Text Generation with Vertex AI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a416b662-c645-4048-8557-fd0f66d9818c",
   "metadata": {},
   "source": [
    "#### Project environment setup\n",
    "\n",
    "- Load credentials and relevant Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff77bf9c-93e6-4281-9775-f7e87935b37a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "PROJECT_ID = os.environ['PROJECT_ID']\n",
    "REGION = os.environ['REGION']\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"REGION: {REGION}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02fb2abd-f5d3-4fa0-8c4c-8d516df20c7f",
   "metadata": {},
   "source": [
    "### Prompt the model\n",
    "- We'll import a language model that has been trained to handle a variety of natural language tasks, `text-bison@001`.\n",
    "- For multi-turn dialogue with a language model, you can use, `chat-bison@001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f253273a-a0bd-41c8-bf64-138666578504",
   "metadata": {
    "height": 49
   },
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da149416-7fea-4bb3-b872-f64200f9b19c",
   "metadata": {
    "height": 66
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\n",
    "    \"text-bison@001\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36ac4f25-7818-49fc-b9e9-097886da8e82",
   "metadata": {},
   "source": [
    "#### Question Answering\n",
    "- You can ask an open-ended question to the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1b5ab-1bb8-4882-93e8-cc9381b18aad",
   "metadata": {
    "height": 66
   },
   "outputs": [],
   "source": [
    "prompt = \"I'm a high school student. \\\n",
    "Recommend me a programming activity to improve my skills.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aca913-e2c1-4ad5-b8ee-4945c9546102",
   "metadata": {
    "height": 49
   },
   "outputs": [],
   "source": [
    "print(generation_model.predict(prompt=prompt).text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd540471-cda7-43b2-aee3-b84cb1234c1a",
   "metadata": {},
   "source": [
    "#### Classify and elaborate\n",
    "- For more predictability of the language model's response, you can also ask the language model to choose among a list of answers and then elaborate on its answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1491242-77fb-4d8e-8bda-04f4894fdd83",
   "metadata": {
    "height": 134
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"I'm a high school student. \\\n",
    "Which of these activities do you suggest and why:\n",
    "a) learn Python\n",
    "b) learn Javascript\n",
    "c) learn Fortran\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28880988-e191-4991-baa8-fb7c89cc4874",
   "metadata": {
    "height": 49
   },
   "outputs": [],
   "source": [
    "print(generation_model.predict(prompt=prompt).text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "913e102f-e4d7-4ac8-9b76-3a0c1b3790b9",
   "metadata": {},
   "source": [
    "#### Extract information and format it as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdfe0f7-678c-4ad6-bdcb-4cc299512570",
   "metadata": {
    "height": 355
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\" A bright and promising wildlife biologist \\\n",
    "named Jesse Plank (Amara Patel) is determined to make her \\\n",
    "mark on the world. \n",
    "Jesse moves to Texas for what she believes is her dream job, \n",
    "only to discover a dark secret that will make \\\n",
    "her question everything. \n",
    "In the new lab she quickly befriends the outgoing \\\n",
    "lab tech named Maya Jones (Chloe Nguyen), \n",
    "and the lab director Sam Porter (Fredrik Johansson). \n",
    "Together the trio work long hours on their research \\\n",
    "in a hope to change the world for good. \n",
    "Along the way they meet the comical \\\n",
    "Brenna Ode (Eleanor Garcia) who is a marketing lead \\\n",
    "at the research institute, \n",
    "and marine biologist Siri Teller (Freya Johansson).\n",
    "\n",
    "Extract the characters, their jobs \\\n",
    "and the actors who played them from the above message as a table\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92efacae-afe4-4cd2-9d6d-cb565d102ccc",
   "metadata": {
    "height": 83
   },
   "outputs": [],
   "source": [
    "response = generation_model.predict(prompt=prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d80be8ca-4732-4b39-a5e6-5db1d3b02218",
   "metadata": {},
   "source": [
    "- You can copy-paste the text into a markdown cell to see if it displays a table."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18816c8e-a473-4eb5-8983-7c5f80f8aa15",
   "metadata": {},
   "source": [
    "| Character | Job | Actor |\n",
    "|---|---|---|\n",
    "| Jesse Plank | Wildlife Biologist | Amara Patel |\n",
    "| Maya Jones | Lab Tech | Chloe Nguyen |\n",
    "| Sam Porter | Lab Director | Fredrik Johansson |\n",
    "| Brenna Ode | Marketing Lead | Eleanor Garcia |\n",
    "| Siri Teller | Marine Biologist | Freya Johansson |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87b9c406-2b3f-4fb0-a181-201091013864",
   "metadata": {},
   "source": [
    "### Adjusting Creativity/Randomness\n",
    "- You can control the behavior of the language model's decoding strategy by adjusting the temperature, top-k, and top-n parameters.\n",
    "- For tasks for which you want the model to consistently output the same result for the same input, (such as classification or information extraction), set temperature to zero.\n",
    "- For tasks where you desire more creativity, such as brainstorming, summarization, choose a higher temperature (up to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f629e383-b4ad-4fa6-a949-435dd1f04da6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "temperature = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84519df4-9b6f-471c-8183-27cc4a151d7d",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "prompt = \"Complete the sentence: \\\n",
    "As I prepared the picture frame, \\\n",
    "I reached into my toolkit to fetch my:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2fe53e-d885-411c-bb2e-c3940884d30f",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "response = generation_model.predict(\n",
    "    prompt=prompt,\n",
    "    temperature=temperature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be86a8a-a58f-4b9e-bd66-9eced64e549c",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "print(f\"[temperature = {temperature}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5bd8e5-8fdc-4d94-acf9-10b7d3b34ba8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bf346d-2617-460e-bc47-f331b92ed053",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "response = generation_model.predict(\n",
    "    prompt=prompt,\n",
    "    temperature=temperature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e25ae3-a1ae-4379-ad32-8d79e52b7590",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "print(f\"[temperature = {temperature}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fea806c4-f955-4434-89a5-57fd9083e9d3",
   "metadata": {},
   "source": [
    "#### Top P\n",
    "- Top p: sample the minimum set of tokens whose probabilities add up to probability `p` or greater.\n",
    "- The default value for `top_p` is `0.95`.\n",
    "- If you want to adjust `top_p` and `top_k` and see different results, remember to set `temperature` to be greater than zero, otherwise the model will always choose the token with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98a491-6da8-4cca-bf16-85f9547e7fe5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "top_p = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e195211-60ea-4103-b083-19e78a26c920",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "prompt = \"Write an advertisement for jackets \\\n",
    "that involves blue elephants and avocados.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28093674-1a83-4abb-999a-030df75b2125",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "response = generation_model.predict(\n",
    "    prompt=prompt, \n",
    "    temperature=0.9, \n",
    "    top_p=top_p,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a4ce9d-7520-44bc-af24-dc1b6ad61b50",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "print(f\"[top_p = {top_p}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02817243-ef3f-440a-8846-33dc46963b41",
   "metadata": {},
   "source": [
    "#### Top k\n",
    "- The default value for `top_k` is `40`.\n",
    "- You can set `top_k` to values between `1` and `40`.\n",
    "- The decoding strategy applies `top_k`, then `top_p`, then `temperature` (in that order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6603255-1e19-47ce-8a77-634b33c1e99f",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "top_k = 20\n",
    "top_p = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef545757-b37c-4b39-b7e9-3ebc4ec1d910",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "response = generation_model.predict(\n",
    "    prompt=prompt, \n",
    "    temperature=0.9, \n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c4f8d-df9d-4d61-9ab6-19958f8b4129",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "print(f\"[top_p = {top_p}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c5ff2-03f0-4dcd-a808-c92c9c5bbb0a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15690d6f-6515-4b65-9a8f-b907f90711ea",
   "metadata": {},
   "source": [
    "# Semantic Search, Building a Q&A System"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f6eba99-d01c-469d-9d88-a4d73cbf12cb",
   "metadata": {},
   "source": [
    "#### Project environment setup\n",
    "\n",
    "- Load credentials and relevant Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600b34d4-4b4a-46f9-a0d6-9a312a09d1d0",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "PROJECT_ID = os.environ['PROJECT_ID']\n",
    "REGION = os.environ['REGION']\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"REGION: {REGION}\")\n",
    "print(f\"APPLICATION_CREDENTIALS: {os.environ['GOOGLE_APPLICATION_CREDENTIALS']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fe50e74-21cc-4cb8-9595-7ad2df27af06",
   "metadata": {},
   "source": [
    "## Load Stack Overflow questions and answers from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300058f6-3372-47fd-a4f5-63e92f7af612",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb1e4da-a583-493b-8f6b-e30e69281e24",
   "metadata": {
    "height": 49
   },
   "outputs": [],
   "source": [
    "so_database = pd.read_csv('../L4/data/so_database_app.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31de31c-ca30-4a11-8fbc-8f179cde839a",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "print(\"Shape: \" + str(so_database.shape))\n",
    "print(so_database)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "923834d6-e065-44c5-9032-0af3d032dd74",
   "metadata": {},
   "source": [
    "## Load the question embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31d02d1-632d-4139-bc31-d1dd4c760f26",
   "metadata": {
    "height": 49
   },
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcae98f-3d58-40dc-9244-65b084ce2910",
   "metadata": {
    "height": 66
   },
   "outputs": [],
   "source": [
    "embedding_model = TextEmbeddingModel.from_pretrained(\n",
    "    \"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb7c15-1f60-4ae1-9eee-cacf91ce40be",
   "metadata": {
    "height": 66
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import encode_text_to_embedding_batched"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fc34bc0-f2ec-4445-a3c8-2c49957ad167",
   "metadata": {},
   "source": [
    "- Here is the code that embeds the text.  You can adapt it for use in your own projects.  \n",
    "- To save on API calls, we've embedded the text already, so you can load it from the saved file in the next cell.\n",
    "\n",
    "```Python\n",
    "# Encode the stack overflow data\n",
    "\n",
    "so_questions = so_database.input_text.tolist()\n",
    "question_embeddings = encode_text_to_embedding_batched(\n",
    "            sentences = so_questions,\n",
    "            api_calls_per_second = 20/60, \n",
    "            batch_size = 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545a2ba-3d8c-4a61-9fd4-a9687df824e8",
   "metadata": {
    "height": 151
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../L4/data/question_embeddings_app.pkl', 'rb') as file:\n",
    "      \n",
    "    # Call load method to deserialze\n",
    "    question_embeddings = pickle.load(file)\n",
    "  \n",
    "    print(question_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d11d36e-e70a-4981-a37b-6b978520d90f",
   "metadata": {
    "height": 49
   },
   "outputs": [],
   "source": [
    "so_database['embeddings'] = question_embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8f8e5-4989-47cc-a3b1-55e782dea2d0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "so_database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "024e3893-c558-4a35-8680-dc95cd70e417",
   "metadata": {},
   "source": [
    "## Semantic Search\n",
    "\n",
    "When a user asks a question, we can embed their query on the fly and search over all of the Stack Overflow question embeddings to find the most simliar datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c0523-8bb7-44f2-b437-28b7ca14428f",
   "metadata": {
    "height": 100
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances_argmin as distances_argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a36b9-2929-4f83-bee7-2a6b82c30787",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "query = ['How to concat dataframes pandas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96226ac6-0be7-4243-aede-df8226cfc1b2",
   "metadata": {
    "height": 49
   },
   "outputs": [],
   "source": [
    "query_embedding = embedding_model.get_embeddings(query)[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2b5de8-eede-4cba-ae6e-5a56d14eb2b1",
   "metadata": {
    "height": 66
   },
   "outputs": [],
   "source": [
    "cos_sim_array = cosine_similarity([query_embedding],\n",
    "                                  list(so_database.embeddings.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f8418-93c4-416e-8df8-b15e96aa7bdf",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "cos_sim_array.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "582821c0-b31b-43ef-814e-5e5534b2d23c",
   "metadata": {},
   "source": [
    "Once we have a similarity value between our query embedding and each of the database embeddings, we can extract the index with the highest value. This embedding corresponds to the Stack Overflow post that is most similiar to the question \"How to concat dataframes pandas\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c4a855-e9bc-4283-b93a-13a76fed5adc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "index_doc_cosine = np.argmax(cos_sim_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47bc5ad-c085-4b4e-9984-8bdb2afa05eb",
   "metadata": {
    "height": 66
   },
   "outputs": [],
   "source": [
    "index_doc_distances = distances_argmin([query_embedding], \n",
    "                                       list(so_database.embeddings.values))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa5ae70-9109-4ff6-b5bc-e0e587abe177",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "so_database.input_text[index_doc_cosine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d957e90-c952-4d09-bca7-dbc7b9d3b2fb",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "so_database.output_text[index_doc_cosine]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7117a803-e2d0-460e-a431-2aeaf6c35a78",
   "metadata": {},
   "source": [
    "## Question answering with relevant context\n",
    "\n",
    "Now that we have found the most simliar Stack Overflow question, we can take the corresponding answer and use an LLM to produce a more conversational response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a6e5c-360d-4652-8082-a23a0e4eb445",
   "metadata": {
    "height": 49
   },
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1378f-8d7c-41f1-be5a-957a2855ebd0",
   "metadata": {
    "height": 66
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\n",
    "    \"text-bison@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1069f11-18f2-4f35-ade2-08cbde27a6c9",
   "metadata": {
    "height": 66
   },
   "outputs": [],
   "source": [
    "context = \"Question: \" + so_database.input_text[index_doc_cosine] +\\\n",
    "\"\\n Answer: \" + so_database.output_text[index_doc_cosine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19e0aba-8d5f-4997-b754-73e32d54b27e",
   "metadata": {
    "height": 185
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Here is the context: {context}\n",
    "             Using the relevant information from the context,\n",
    "             provide an answer to the query: {query}.\"\n",
    "             If the context doesn't provide \\\n",
    "             any relevant information, \\\n",
    "             answer with \\\n",
    "             [I couldn't find a good match in the \\\n",
    "             document database for your query]\n",
    "             \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460c33c-24db-4698-9e2c-509ff7629c73",
   "metadata": {
    "height": 168
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "t_value = 0.2\n",
    "response = generation_model.predict(prompt = prompt,\n",
    "                                    temperature = t_value,\n",
    "                                    max_output_tokens = 1024)\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82a6ad63-2511-411b-b25f-33ddc2eb06fd",
   "metadata": {},
   "source": [
    "## When the documents don't provide useful information\n",
    "\n",
    "Our current workflow returns the most similar question from our embeddings database. But what do we do when that question isn't actually relevant when answering the user query? In other words, we don't have a good match in our database.\n",
    "\n",
    "In addition to providing a more conversational response, LLMs can help us handle these cases where the most similiar document isn't actually a reasonable answer to the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a14e45-63be-4f5d-978c-046de430be41",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "query = ['How to make the perfect lasagna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232818b5-281b-42df-a41a-a4a11e720113",
   "metadata": {
    "height": 49
   },
   "outputs": [],
   "source": [
    "query_embedding = embedding_model.get_embeddings(query)[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c1979-bb51-4dbd-921a-53d43b603a1b",
   "metadata": {
    "height": 66
   },
   "outputs": [],
   "source": [
    "cos_sim_array = cosine_similarity([query_embedding], \n",
    "                                  list(so_database.embeddings.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccfded1-bf83-4d01-bdfa-a7dfd9f7eeee",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "cos_sim_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1e7ff-8db2-4f21-a3d3-7fdd2084bfc1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "index_doc = np.argmax(cos_sim_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b607d8-af01-4f0b-8ab6-53d9b7e9fba4",
   "metadata": {
    "height": 66
   },
   "outputs": [],
   "source": [
    "context = so_database.input_text[index_doc] + \\\n",
    "\"\\n Answer: \" + so_database.output_text[index_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5688d3-12ae-4472-8bb8-3e0496424258",
   "metadata": {
    "height": 168
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Here is the context: {context}\n",
    "             Using the relevant information from the context,\n",
    "             provide an answer to the query: {query}.\"\n",
    "             If the context doesn't provide \\\n",
    "             any relevant information, answer with \n",
    "             [I couldn't find a good match in the \\\n",
    "             document database for your query]\n",
    "             \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3685dd6a-7d08-4348-8ceb-7e1a640bfb2a",
   "metadata": {
    "height": 117
   },
   "outputs": [],
   "source": [
    "t_value = 0.2\n",
    "response = generation_model.predict(prompt = prompt,\n",
    "                                    temperature = t_value,\n",
    "                                    max_output_tokens = 1024)\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07ca1481-e353-46b0-8982-81db8eb23567",
   "metadata": {},
   "source": [
    "## Scale with approximate nearest neighbor search\n",
    "\n",
    "When dealing with a large dataset, computing the similarity between the query and each original embedded document in the database might be too expensive. Instead of doing that, you can use approximate nearest neighbor algorithms that find the most similar documents in a more efficient way.\n",
    "\n",
    "These algorithms usually work by creating an index for your data, and using that index to find the most similar documents for your queries. In this notebook, we will use ScaNN to demonstrate the benefits of efficient vector similarity search. First, you have to create an index for your embedded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320eee3a-4fdd-42e0-bfff-6c4504499447",
   "metadata": {
    "height": 168
   },
   "outputs": [],
   "source": [
    "import scann\n",
    "from utils import create_index\n",
    "\n",
    "#Create index using scann\n",
    "index = create_index(embedded_dataset = question_embeddings, \n",
    "                     num_leaves = 25,\n",
    "                     num_leaves_to_search = 10,\n",
    "                     training_sample_size = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb20d1-6f09-4afc-9ce3-be927b48aa15",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "query = \"how to concat dataframes pandas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3d624-b505-48a3-a639-ba976f57d920",
   "metadata": {
    "height": 219
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "start = time.time()\n",
    "query_embedding = embedding_model.get_embeddings([query])[0].values\n",
    "neighbors, distances = index.search(query_embedding, final_num_neighbors = 1)\n",
    "end = time.time()\n",
    "\n",
    "for id, dist in zip(neighbors, distances):\n",
    "    print(f\"[docid:{id}] [{dist}] -- {so_database.input_text[int(id)][:125]}...\")\n",
    "\n",
    "print(\"Latency (ms):\", 1000 * (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44cec14-4001-4207-8ee2-03917482d94a",
   "metadata": {
    "height": 185
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "query_embedding = embedding_model.get_embeddings([query])[0].values\n",
    "cos_sim_array = cosine_similarity([query_embedding], list(so_database.embeddings.values))\n",
    "index_doc = np.argmax(cos_sim_array)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"[docid:{index_doc}] [{np.max(cos_sim_array)}] -- {so_database.input_text[int(index_doc)][:125]}...\")\n",
    "\n",
    "print(\"Latency (ms):\", 1000 * (end - start))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
