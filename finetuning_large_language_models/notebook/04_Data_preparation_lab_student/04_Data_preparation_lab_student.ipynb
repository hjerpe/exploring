{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/exploring/finetuning_large_language_models/venvs/default_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer(text)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12764, 13, 849, 403, 368, 32]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded tokens back into text:  Hi, how are you?\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded tokens back into text: \", decoded_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize multiple texts at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded several texts:  [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "list_texts = [\"Hi, how are you?\", \"I'm good\", \"Yes\"]\n",
    "encoded_texts = tokenizer(list_texts)\n",
    "print(\"Encoded several texts: \", encoded_texts[\"input_ids\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding and truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using padding:  [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175, 0, 0, 0], [4374, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "encoded_texts_longest = tokenizer(list_texts, padding=True)\n",
    "print(\"Using padding: \", encoded_texts_longest[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using truncation:  [[12764, 13, 849], [42, 1353, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_truncation = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "print(\"Using truncation: \", encoded_texts_truncation[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using left-side truncation:  [[403, 368, 32], [42, 1353, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.truncation_side = \"left\"\n",
    "encoded_texts_truncation_left = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "print(\"Using left-side truncation: \", encoded_texts_truncation_left[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using both padding and truncation:  [[403, 368, 32], [42, 1353, 1175], [4374, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_both = tokenizer(list_texts, max_length=3, truncation=True, padding=True)\n",
    "print(\"Using both padding and truncation: \", encoded_texts_both[\"input_ids\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One datapoint in the finetuning dataset:\n",
      "{'answer': 'Lamini has documentation on Getting Started, Authentication, '\n",
      "           'Question Answer Model, Python Library, Batching, Error Handling, '\n",
      "           'Advanced topics, and class documentation on LLM Engine available '\n",
      "           'at https://lamini-ai.github.io/.',\n",
      " 'question': '### Question:\\n'\n",
      "             'What are the different types of documents available in the '\n",
      "             'repository (e.g., installation guide, API documentation, '\n",
      "             \"developer's guide)?\\n\"\n",
      "             '\\n'\n",
      "             '### Answer:'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = \"data/lamini_docs.jsonl\"\n",
    "instruction_dataset_df = pd.read_json(filename, lines=True)\n",
    "examples = instruction_dataset_df.to_dict()\n",
    "\n",
    "if \"question\" in examples and \"answer\" in examples:\n",
    "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "elif \"instruction\" in examples and \"response\" in examples:\n",
    "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "elif \"input\" in examples and \"output\" in examples:\n",
    "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "else:\n",
    "  text = examples[\"text\"][0]\n",
    "\n",
    "prompt_template = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\"\n",
    "\n",
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset = []\n",
    "for i in range(num_examples):\n",
    "  question = examples[\"question\"][i]\n",
    "  answer = examples[\"answer\"][i]\n",
    "  text_with_prompt_template = prompt_template.format(question=question)\n",
    "  finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"One datapoint in the finetuning dataset:\")\n",
    "pprint(finetuning_dataset[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4118 19782    27   187  1276   403   253  1027  3510   273  7177  2130\n",
      "    275   253 18491   313    70    15    72   904 12692  7102    13  8990\n",
      "  10097    13 13722   434  7102  6177   187   187  4118 37741    27    45\n",
      "   4988    74   556 10097   327 27669 11075   264    13  5271 23058    13\n",
      "  19782 37741 10031    13 13814 11397    13   378 16464    13 11759 10535\n",
      "   1981    13 21798 12989    13   285   966 10097   327 21708    46 10797\n",
      "   2130   387  5987  1358    77  4988    74    14  2284    15  7280    15\n",
      "    900 14206]]\n"
     ]
    }
   ],
   "source": [
    "text = finetuning_dataset[0][\"question\"] + finetuning_dataset[0][\"answer\"]\n",
    "tokenized_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",\n",
    "    padding=True\n",
    ")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 2048\n",
    "max_length = min(\n",
    "    tokenized_inputs[\"input_ids\"].shape[1],\n",
    "    max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",\n",
    "    truncation=True,\n",
    "    max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4118, 19782,    27,   187,  1276,   403,   253,  1027,  3510,\n",
       "          273,  7177,  2130,   275,   253, 18491,   313,    70,    15,\n",
       "           72,   904, 12692,  7102,    13,  8990, 10097,    13, 13722,\n",
       "          434,  7102,  6177,   187,   187,  4118, 37741,    27,    45,\n",
       "         4988,    74,   556, 10097,   327, 27669, 11075,   264,    13,\n",
       "         5271, 23058,    13, 19782, 37741, 10031,    13, 13814, 11397,\n",
       "           13,   378, 16464,    13, 11759, 10535,  1981,    13, 21798,\n",
       "        12989,    13,   285,   966, 10097,   327, 21708,    46, 10797,\n",
       "         2130,   387,  5987,  1358,    77,  4988,    74,    14,  2284,\n",
       "           15,  7280,    15,   900, 14206]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs[\"input_ids\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    if \"question\" in examples and \"answer\" in examples:\n",
    "      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "    elif \"input\" in examples and \"output\" in examples:\n",
    "      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "    else:\n",
    "      text = examples[\"text\"][0]\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        2048\n",
    "    )\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=filename, split=\"train\")\n",
    "\n",
    "tokenized_dataset = finetuning_dataset_loaded.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test/train splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "print(split_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some datasets for you to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_path = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = datasets.load_dataset(finetuning_dataset_path)\n",
    "print(finetuning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "taylor_swift_dataset = \"lamini/taylor_swift\"\n",
    "bts_dataset = \"lamini/bts\"\n",
    "open_llms = \"lamini/open_llms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the most popular Taylor Swift song among millennials? How does this song relate to the millennial generation? What is the significance of this song in the millennial culture?', 'answer': 'Taylor Swift\\'s \"Shake It Off\" is the most popular song among millennials. This song relates to the millennial generation as it is an anthem of self-acceptance and embracing one\\'s individuality. The song\\'s message of not letting others bring you down and to just dance it off resonates with the millennial culture, which is often characterized by a strong sense of individuality and a rejection of societal norms. Additionally, the song\\'s upbeat and catchy melody makes it a perfect fit for the millennial generation, which is known for its love of pop music.', 'input_ids': [1276, 310, 253, 954, 4633, 11276, 24619, 4498, 2190, 24933, 8075, 32, 1359, 1057, 436, 4498, 14588, 281, 253, 24933, 451, 5978, 32, 1737, 310, 253, 8453, 273, 436, 4498, 275, 253, 24933, 451, 4466, 32, 37979, 24619, 434, 346, 2809, 640, 733, 5566, 3, 310, 253, 954, 4633, 4498, 2190, 24933, 8075, 15, 831, 4498, 7033, 281, 253, 24933, 451, 5978, 347, 352, 310, 271, 49689, 273, 1881, 14, 14764, 593, 285, 41859, 581, 434, 2060, 414, 15, 380, 4498, 434, 3935, 273, 417, 13872, 2571, 3324, 368, 1066, 285, 281, 816, 11012, 352, 745, 8146, 684, 342, 253, 24933, 451, 4466, 13, 534, 310, 2223, 7943, 407, 247, 2266, 3282, 273, 2060, 414, 285, 247, 18235, 273, 38058, 22429, 15, 9157, 13, 253, 4498, 434, 598, 19505, 285, 5834, 90, 40641, 2789, 352, 247, 3962, 4944, 323, 253, 24933, 451, 5978, 13, 534, 310, 1929, 323, 697, 2389, 273, 1684, 3440, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1276, 310, 253, 954, 4633, 11276, 24619, 4498, 2190, 24933, 8075, 32, 1359, 1057, 436, 4498, 14588, 281, 253, 24933, 451, 5978, 32, 1737, 310, 253, 8453, 273, 436, 4498, 275, 253, 24933, 451, 4466, 32, 37979, 24619, 434, 346, 2809, 640, 733, 5566, 3, 310, 253, 954, 4633, 4498, 2190, 24933, 8075, 15, 831, 4498, 7033, 281, 253, 24933, 451, 5978, 347, 352, 310, 271, 49689, 273, 1881, 14, 14764, 593, 285, 41859, 581, 434, 2060, 414, 15, 380, 4498, 434, 3935, 273, 417, 13872, 2571, 3324, 368, 1066, 285, 281, 816, 11012, 352, 745, 8146, 684, 342, 253, 24933, 451, 4466, 13, 534, 310, 2223, 7943, 407, 247, 2266, 3282, 273, 2060, 414, 285, 247, 18235, 273, 38058, 22429, 15, 9157, 13, 253, 4498, 434, 598, 19505, 285, 5834, 90, 40641, 2789, 352, 247, 3962, 4944, 323, 253, 24933, 451, 5978, 13, 534, 310, 1929, 323, 697, 2389, 273, 1684, 3440, 15]}\n"
     ]
    }
   ],
   "source": [
    "dataset_swiftie = datasets.load_dataset(taylor_swift_dataset)\n",
    "print(dataset_swiftie[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'EleutherAI-gpt-neox-20b: EleutherAI-gpt-neox-20b: EleutherAI-gpt-neox-20b: What is the architecture of GPT-NeoX-20B?', 'answer': \"GPT-NeoX-20B's architecture intentionally resembles that of GPT-3, and is almost identical to that of GPT-J-6B.\", 'input_ids': [30377, 16580, 18128, 14, 72, 431, 14, 570, 1004, 14, 938, 67, 27, 13173, 16580, 18128, 14, 72, 431, 14, 570, 1004, 14, 938, 67, 27, 13173, 16580, 18128, 14, 72, 431, 14, 570, 1004, 14, 938, 67, 27, 1737, 310, 253, 10336, 273, 443, 5736, 14, 6560, 80, 57, 14, 938, 35, 32, 40, 5736, 14, 6560, 80, 57, 14, 938, 35, 434, 10336, 23209, 29217, 326, 273, 443, 5736, 14, 20, 13, 285, 310, 2761, 8931, 281, 326, 273, 443, 5736, 14, 43, 14, 23, 35, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [30377, 16580, 18128, 14, 72, 431, 14, 570, 1004, 14, 938, 67, 27, 13173, 16580, 18128, 14, 72, 431, 14, 570, 1004, 14, 938, 67, 27, 13173, 16580, 18128, 14, 72, 431, 14, 570, 1004, 14, 938, 67, 27, 1737, 310, 253, 10336, 273, 443, 5736, 14, 6560, 80, 57, 14, 938, 35, 32, 40, 5736, 14, 6560, 80, 57, 14, 938, 35, 434, 10336, 23209, 29217, 326, 273, 443, 5736, 14, 20, 13, 285, 310, 2761, 8931, 281, 326, 273, 443, 5736, 14, 43, 14, 23, 35, 15]}\n"
     ]
    }
   ],
   "source": [
    "dataset_open_llms = datasets.load_dataset(open_llms)\n",
    "print(dataset_open_llms[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': ['AlekseyKorshuk-chatml-pyg-v1: AlekseyKorshuk-chatml-pyg-v1: AlekseyKorshuk-chatml-pyg-v1: What dataset was used for training?',\n",
       "  'EleutherAI-gpt-neox-20b: EleutherAI-gpt-neox-20b: EleutherAI-gpt-neox-20b: What is the architecture of GPT-NeoX-20B?',\n",
       "  'EleutherAI-gpt-neox-20b: EleutherAI-gpt-neox-20b: What is the advantage of using GPT-NeoX-20B?',\n",
       "  'ausboss-llama-30b-supercot: What parameter sizes is this LoRA compatible with?',\n",
       "  'CalderaAI-30B-Lazarus: CalderaAI-30B-Lazarus: CalderaAI-30B-Lazarus: What is the SuperHOT Prototype model?',\n",
       "  'llama-65b: How many models does LLaMA have?',\n",
       "  'Aeala-VicUnlocked-alpaca-30b: What is the Torch data type of Aeala/VicUnlocked-alpaca-30b?',\n",
       "  'huggyllama-llama-65b: What are the features of huggyllama/llama-65b?',\n",
       "  'llama-7b: What is the goal of the AI community in developing the model?',\n",
       "  'llama-7b: llama-7b: llama-7b: What is the latest work of Meta?'],\n",
       " 'answer': ['The None dataset was used for training.',\n",
       "  \"GPT-NeoX-20B's architecture intentionally resembles that of GPT-3, and is almost identical to that of GPT-J-6B.\",\n",
       "  'The advantage of using GPT-NeoX-20B is that it is capable of performing zero and five-shot natural language tasks, zero and five-shot Basic Arithmetic and MATH, and zero-',\n",
       "  'This LoRA is compatible with any 7B, 13B or 30B 4-bit quantized LLaMa model, including ggml quantized converted bins.',\n",
       "  'Answer:',\n",
       "  'LLaMA has three models: LLaMA 65B, LLaMA 33B, and LLaMA 7B.',\n",
       "  'float16.',\n",
       "  'Vocabulary Size: 32000, Initializer Range: 0.02, Torch Data Type: float16',\n",
       "  'The goal of the AI community is to develop clear guidelines around responsible AI in general and responsible large language models in particular.',\n",
       "  'The latest work of Meta is the development of LLaMA, a platform for access to open source LLM models.'],\n",
       " 'input_ids': [[44163,\n",
       "   76,\n",
       "   5462,\n",
       "   44,\n",
       "   641,\n",
       "   73,\n",
       "   2788,\n",
       "   14,\n",
       "   23481,\n",
       "   1686,\n",
       "   14,\n",
       "   4789,\n",
       "   72,\n",
       "   14,\n",
       "   87,\n",
       "   18,\n",
       "   27,\n",
       "   16660,\n",
       "   76,\n",
       "   5462,\n",
       "   44,\n",
       "   641,\n",
       "   73,\n",
       "   2788,\n",
       "   14,\n",
       "   23481,\n",
       "   1686,\n",
       "   14,\n",
       "   4789,\n",
       "   72,\n",
       "   14,\n",
       "   87,\n",
       "   18,\n",
       "   27,\n",
       "   16660,\n",
       "   76,\n",
       "   5462,\n",
       "   44,\n",
       "   641,\n",
       "   73,\n",
       "   2788,\n",
       "   14,\n",
       "   23481,\n",
       "   1686,\n",
       "   14,\n",
       "   4789,\n",
       "   72,\n",
       "   14,\n",
       "   87,\n",
       "   18,\n",
       "   27,\n",
       "   1737,\n",
       "   10895,\n",
       "   369,\n",
       "   908,\n",
       "   323,\n",
       "   3733,\n",
       "   32,\n",
       "   510,\n",
       "   8256,\n",
       "   10895,\n",
       "   369,\n",
       "   908,\n",
       "   323,\n",
       "   3733,\n",
       "   15],\n",
       "  [30377,\n",
       "   16580,\n",
       "   18128,\n",
       "   14,\n",
       "   72,\n",
       "   431,\n",
       "   14,\n",
       "   570,\n",
       "   1004,\n",
       "   14,\n",
       "   938,\n",
       "   67,\n",
       "   27,\n",
       "   13173,\n",
       "   16580,\n",
       "   18128,\n",
       "   14,\n",
       "   72,\n",
       "   431,\n",
       "   14,\n",
       "   570,\n",
       "   1004,\n",
       "   14,\n",
       "   938,\n",
       "   67,\n",
       "   27,\n",
       "   13173,\n",
       "   16580,\n",
       "   18128,\n",
       "   14,\n",
       "   72,\n",
       "   431,\n",
       "   14,\n",
       "   570,\n",
       "   1004,\n",
       "   14,\n",
       "   938,\n",
       "   67,\n",
       "   27,\n",
       "   1737,\n",
       "   310,\n",
       "   253,\n",
       "   10336,\n",
       "   273,\n",
       "   443,\n",
       "   5736,\n",
       "   14,\n",
       "   6560,\n",
       "   80,\n",
       "   57,\n",
       "   14,\n",
       "   938,\n",
       "   35,\n",
       "   32,\n",
       "   40,\n",
       "   5736,\n",
       "   14,\n",
       "   6560,\n",
       "   80,\n",
       "   57,\n",
       "   14,\n",
       "   938,\n",
       "   35,\n",
       "   434,\n",
       "   10336,\n",
       "   23209,\n",
       "   29217,\n",
       "   326,\n",
       "   273,\n",
       "   443,\n",
       "   5736,\n",
       "   14,\n",
       "   20,\n",
       "   13,\n",
       "   285,\n",
       "   310,\n",
       "   2761,\n",
       "   8931,\n",
       "   281,\n",
       "   326,\n",
       "   273,\n",
       "   443,\n",
       "   5736,\n",
       "   14,\n",
       "   43,\n",
       "   14,\n",
       "   23,\n",
       "   35,\n",
       "   15],\n",
       "  [30377,\n",
       "   16580,\n",
       "   18128,\n",
       "   14,\n",
       "   72,\n",
       "   431,\n",
       "   14,\n",
       "   570,\n",
       "   1004,\n",
       "   14,\n",
       "   938,\n",
       "   67,\n",
       "   27,\n",
       "   13173,\n",
       "   16580,\n",
       "   18128,\n",
       "   14,\n",
       "   72,\n",
       "   431,\n",
       "   14,\n",
       "   570,\n",
       "   1004,\n",
       "   14,\n",
       "   938,\n",
       "   67,\n",
       "   27,\n",
       "   1737,\n",
       "   310,\n",
       "   253,\n",
       "   5750,\n",
       "   273,\n",
       "   970,\n",
       "   443,\n",
       "   5736,\n",
       "   14,\n",
       "   6560,\n",
       "   80,\n",
       "   57,\n",
       "   14,\n",
       "   938,\n",
       "   35,\n",
       "   32,\n",
       "   510,\n",
       "   5750,\n",
       "   273,\n",
       "   970,\n",
       "   443,\n",
       "   5736,\n",
       "   14,\n",
       "   6560,\n",
       "   80,\n",
       "   57,\n",
       "   14,\n",
       "   938,\n",
       "   35,\n",
       "   310,\n",
       "   326,\n",
       "   352,\n",
       "   310,\n",
       "   7032,\n",
       "   273,\n",
       "   9591,\n",
       "   5058,\n",
       "   285,\n",
       "   2620,\n",
       "   14,\n",
       "   11860,\n",
       "   3626,\n",
       "   3448,\n",
       "   8892,\n",
       "   13,\n",
       "   5058,\n",
       "   285,\n",
       "   2620,\n",
       "   14,\n",
       "   11860,\n",
       "   20233,\n",
       "   1780,\n",
       "   23343,\n",
       "   285,\n",
       "   353,\n",
       "   10948,\n",
       "   13,\n",
       "   285,\n",
       "   5058,\n",
       "   14],\n",
       "  [666,\n",
       "   67,\n",
       "   1730,\n",
       "   14,\n",
       "   620,\n",
       "   2902,\n",
       "   14,\n",
       "   1229,\n",
       "   67,\n",
       "   14,\n",
       "   12185,\n",
       "   27678,\n",
       "   27,\n",
       "   1737,\n",
       "   4764,\n",
       "   9552,\n",
       "   310,\n",
       "   436,\n",
       "   9497,\n",
       "   5214,\n",
       "   13333,\n",
       "   342,\n",
       "   32,\n",
       "   1552,\n",
       "   9497,\n",
       "   5214,\n",
       "   310,\n",
       "   13333,\n",
       "   342,\n",
       "   667,\n",
       "   818,\n",
       "   35,\n",
       "   13,\n",
       "   2145,\n",
       "   35,\n",
       "   390,\n",
       "   1884,\n",
       "   35,\n",
       "   577,\n",
       "   14,\n",
       "   2713,\n",
       "   2677,\n",
       "   1025,\n",
       "   418,\n",
       "   7647,\n",
       "   16490,\n",
       "   1566,\n",
       "   13,\n",
       "   1690,\n",
       "   305,\n",
       "   72,\n",
       "   1686,\n",
       "   2677,\n",
       "   1025,\n",
       "   11516,\n",
       "   27925,\n",
       "   15],\n",
       "  [4218,\n",
       "   491,\n",
       "   66,\n",
       "   18128,\n",
       "   14,\n",
       "   1229,\n",
       "   35,\n",
       "   14,\n",
       "   45,\n",
       "   25151,\n",
       "   316,\n",
       "   27,\n",
       "   47660,\n",
       "   66,\n",
       "   18128,\n",
       "   14,\n",
       "   1229,\n",
       "   35,\n",
       "   14,\n",
       "   45,\n",
       "   25151,\n",
       "   316,\n",
       "   27,\n",
       "   47660,\n",
       "   66,\n",
       "   18128,\n",
       "   14,\n",
       "   1229,\n",
       "   35,\n",
       "   14,\n",
       "   45,\n",
       "   25151,\n",
       "   316,\n",
       "   27,\n",
       "   1737,\n",
       "   310,\n",
       "   253,\n",
       "   6053,\n",
       "   41,\n",
       "   2415,\n",
       "   37319,\n",
       "   5174,\n",
       "   1566,\n",
       "   32,\n",
       "   32869,\n",
       "   27],\n",
       "  [620,\n",
       "   2902,\n",
       "   14,\n",
       "   2082,\n",
       "   67,\n",
       "   27,\n",
       "   1359,\n",
       "   1142,\n",
       "   3210,\n",
       "   1057,\n",
       "   418,\n",
       "   7647,\n",
       "   3788,\n",
       "   452,\n",
       "   32,\n",
       "   2293,\n",
       "   66,\n",
       "   3788,\n",
       "   556,\n",
       "   1264,\n",
       "   3210,\n",
       "   27,\n",
       "   418,\n",
       "   7647,\n",
       "   3788,\n",
       "   7251,\n",
       "   35,\n",
       "   13,\n",
       "   418,\n",
       "   7647,\n",
       "   3788,\n",
       "   5922,\n",
       "   35,\n",
       "   13,\n",
       "   285,\n",
       "   418,\n",
       "   7647,\n",
       "   3788,\n",
       "   818,\n",
       "   35,\n",
       "   15],\n",
       "  [34,\n",
       "   70,\n",
       "   7080,\n",
       "   14,\n",
       "   55,\n",
       "   280,\n",
       "   2447,\n",
       "   30730,\n",
       "   14,\n",
       "   267,\n",
       "   81,\n",
       "   20240,\n",
       "   14,\n",
       "   1229,\n",
       "   67,\n",
       "   27,\n",
       "   1737,\n",
       "   310,\n",
       "   253,\n",
       "   7608,\n",
       "   348,\n",
       "   941,\n",
       "   1511,\n",
       "   273,\n",
       "   329,\n",
       "   70,\n",
       "   7080,\n",
       "   16,\n",
       "   55,\n",
       "   280,\n",
       "   2447,\n",
       "   30730,\n",
       "   14,\n",
       "   267,\n",
       "   81,\n",
       "   20240,\n",
       "   14,\n",
       "   1229,\n",
       "   67,\n",
       "   32,\n",
       "   9599,\n",
       "   1036,\n",
       "   15],\n",
       "  [73,\n",
       "   814,\n",
       "   4233,\n",
       "   620,\n",
       "   2902,\n",
       "   14,\n",
       "   620,\n",
       "   2902,\n",
       "   14,\n",
       "   2082,\n",
       "   67,\n",
       "   27,\n",
       "   1737,\n",
       "   403,\n",
       "   253,\n",
       "   3386,\n",
       "   273,\n",
       "   15729,\n",
       "   4233,\n",
       "   620,\n",
       "   2902,\n",
       "   16,\n",
       "   620,\n",
       "   2902,\n",
       "   14,\n",
       "   2082,\n",
       "   67,\n",
       "   32,\n",
       "   55,\n",
       "   406,\n",
       "   25718,\n",
       "   19662,\n",
       "   27,\n",
       "   4567,\n",
       "   933,\n",
       "   13,\n",
       "   23280,\n",
       "   6081,\n",
       "   21277,\n",
       "   27,\n",
       "   470,\n",
       "   15,\n",
       "   2640,\n",
       "   13,\n",
       "   7608,\n",
       "   348,\n",
       "   5128,\n",
       "   8078,\n",
       "   27,\n",
       "   8253,\n",
       "   1036],\n",
       "  [620,\n",
       "   2902,\n",
       "   14,\n",
       "   24,\n",
       "   67,\n",
       "   27,\n",
       "   1737,\n",
       "   310,\n",
       "   253,\n",
       "   4736,\n",
       "   273,\n",
       "   253,\n",
       "   14980,\n",
       "   3114,\n",
       "   275,\n",
       "   6684,\n",
       "   253,\n",
       "   1566,\n",
       "   32,\n",
       "   510,\n",
       "   4736,\n",
       "   273,\n",
       "   253,\n",
       "   14980,\n",
       "   3114,\n",
       "   310,\n",
       "   281,\n",
       "   1287,\n",
       "   2590,\n",
       "   9600,\n",
       "   1475,\n",
       "   5506,\n",
       "   14980,\n",
       "   275,\n",
       "   2087,\n",
       "   285,\n",
       "   5506,\n",
       "   1781,\n",
       "   3448,\n",
       "   3210,\n",
       "   275,\n",
       "   1798,\n",
       "   15],\n",
       "  [620,\n",
       "   2902,\n",
       "   14,\n",
       "   24,\n",
       "   67,\n",
       "   27,\n",
       "   26198,\n",
       "   2902,\n",
       "   14,\n",
       "   24,\n",
       "   67,\n",
       "   27,\n",
       "   26198,\n",
       "   2902,\n",
       "   14,\n",
       "   24,\n",
       "   67,\n",
       "   27,\n",
       "   1737,\n",
       "   310,\n",
       "   253,\n",
       "   6323,\n",
       "   789,\n",
       "   273,\n",
       "   30680,\n",
       "   32,\n",
       "   510,\n",
       "   6323,\n",
       "   789,\n",
       "   273,\n",
       "   30680,\n",
       "   310,\n",
       "   253,\n",
       "   2440,\n",
       "   273,\n",
       "   418,\n",
       "   7647,\n",
       "   3788,\n",
       "   13,\n",
       "   247,\n",
       "   5147,\n",
       "   323,\n",
       "   2289,\n",
       "   281,\n",
       "   1527,\n",
       "   2603,\n",
       "   21708,\n",
       "   46,\n",
       "   3210,\n",
       "   15]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]],\n",
       " 'labels': [[44163,\n",
       "   76,\n",
       "   5462,\n",
       "   44,\n",
       "   641,\n",
       "   73,\n",
       "   2788,\n",
       "   14,\n",
       "   23481,\n",
       "   1686,\n",
       "   14,\n",
       "   4789,\n",
       "   72,\n",
       "   14,\n",
       "   87,\n",
       "   18,\n",
       "   27,\n",
       "   16660,\n",
       "   76,\n",
       "   5462,\n",
       "   44,\n",
       "   641,\n",
       "   73,\n",
       "   2788,\n",
       "   14,\n",
       "   23481,\n",
       "   1686,\n",
       "   14,\n",
       "   4789,\n",
       "   72,\n",
       "   14,\n",
       "   87,\n",
       "   18,\n",
       "   27,\n",
       "   16660,\n",
       "   76,\n",
       "   5462,\n",
       "   44,\n",
       "   641,\n",
       "   73,\n",
       "   2788,\n",
       "   14,\n",
       "   23481,\n",
       "   1686,\n",
       "   14,\n",
       "   4789,\n",
       "   72,\n",
       "   14,\n",
       "   87,\n",
       "   18,\n",
       "   27,\n",
       "   1737,\n",
       "   10895,\n",
       "   369,\n",
       "   908,\n",
       "   323,\n",
       "   3733,\n",
       "   32,\n",
       "   510,\n",
       "   8256,\n",
       "   10895,\n",
       "   369,\n",
       "   908,\n",
       "   323,\n",
       "   3733,\n",
       "   15],\n",
       "  [30377,\n",
       "   16580,\n",
       "   18128,\n",
       "   14,\n",
       "   72,\n",
       "   431,\n",
       "   14,\n",
       "   570,\n",
       "   1004,\n",
       "   14,\n",
       "   938,\n",
       "   67,\n",
       "   27,\n",
       "   13173,\n",
       "   16580,\n",
       "   18128,\n",
       "   14,\n",
       "   72,\n",
       "   431,\n",
       "   14,\n",
       "   570,\n",
       "   1004,\n",
       "   14,\n",
       "   938,\n",
       "   67,\n",
       "   27,\n",
       "   13173,\n",
       "   16580,\n",
       "   18128,\n",
       "   14,\n",
       "   72,\n",
       "   431,\n",
       "   14,\n",
       "   570,\n",
       "   1004,\n",
       "   14,\n",
       "   938,\n",
       "   67,\n",
       "   27,\n",
       "   1737,\n",
       "   310,\n",
       "   253,\n",
       "   10336,\n",
       "   273,\n",
       "   443,\n",
       "   5736,\n",
       "   14,\n",
       "   6560,\n",
       "   80,\n",
       "   57,\n",
       "   14,\n",
       "   938,\n",
       "   35,\n",
       "   32,\n",
       "   40,\n",
       "   5736,\n",
       "   14,\n",
       "   6560,\n",
       "   80,\n",
       "   57,\n",
       "   14,\n",
       "   938,\n",
       "   35,\n",
       "   434,\n",
       "   10336,\n",
       "   23209,\n",
       "   29217,\n",
       "   326,\n",
       "   273,\n",
       "   443,\n",
       "   5736,\n",
       "   14,\n",
       "   20,\n",
       "   13,\n",
       "   285,\n",
       "   310,\n",
       "   2761,\n",
       "   8931,\n",
       "   281,\n",
       "   326,\n",
       "   273,\n",
       "   443,\n",
       "   5736,\n",
       "   14,\n",
       "   43,\n",
       "   14,\n",
       "   23,\n",
       "   35,\n",
       "   15],\n",
       "  [30377,\n",
       "   16580,\n",
       "   18128,\n",
       "   14,\n",
       "   72,\n",
       "   431,\n",
       "   14,\n",
       "   570,\n",
       "   1004,\n",
       "   14,\n",
       "   938,\n",
       "   67,\n",
       "   27,\n",
       "   13173,\n",
       "   16580,\n",
       "   18128,\n",
       "   14,\n",
       "   72,\n",
       "   431,\n",
       "   14,\n",
       "   570,\n",
       "   1004,\n",
       "   14,\n",
       "   938,\n",
       "   67,\n",
       "   27,\n",
       "   1737,\n",
       "   310,\n",
       "   253,\n",
       "   5750,\n",
       "   273,\n",
       "   970,\n",
       "   443,\n",
       "   5736,\n",
       "   14,\n",
       "   6560,\n",
       "   80,\n",
       "   57,\n",
       "   14,\n",
       "   938,\n",
       "   35,\n",
       "   32,\n",
       "   510,\n",
       "   5750,\n",
       "   273,\n",
       "   970,\n",
       "   443,\n",
       "   5736,\n",
       "   14,\n",
       "   6560,\n",
       "   80,\n",
       "   57,\n",
       "   14,\n",
       "   938,\n",
       "   35,\n",
       "   310,\n",
       "   326,\n",
       "   352,\n",
       "   310,\n",
       "   7032,\n",
       "   273,\n",
       "   9591,\n",
       "   5058,\n",
       "   285,\n",
       "   2620,\n",
       "   14,\n",
       "   11860,\n",
       "   3626,\n",
       "   3448,\n",
       "   8892,\n",
       "   13,\n",
       "   5058,\n",
       "   285,\n",
       "   2620,\n",
       "   14,\n",
       "   11860,\n",
       "   20233,\n",
       "   1780,\n",
       "   23343,\n",
       "   285,\n",
       "   353,\n",
       "   10948,\n",
       "   13,\n",
       "   285,\n",
       "   5058,\n",
       "   14],\n",
       "  [666,\n",
       "   67,\n",
       "   1730,\n",
       "   14,\n",
       "   620,\n",
       "   2902,\n",
       "   14,\n",
       "   1229,\n",
       "   67,\n",
       "   14,\n",
       "   12185,\n",
       "   27678,\n",
       "   27,\n",
       "   1737,\n",
       "   4764,\n",
       "   9552,\n",
       "   310,\n",
       "   436,\n",
       "   9497,\n",
       "   5214,\n",
       "   13333,\n",
       "   342,\n",
       "   32,\n",
       "   1552,\n",
       "   9497,\n",
       "   5214,\n",
       "   310,\n",
       "   13333,\n",
       "   342,\n",
       "   667,\n",
       "   818,\n",
       "   35,\n",
       "   13,\n",
       "   2145,\n",
       "   35,\n",
       "   390,\n",
       "   1884,\n",
       "   35,\n",
       "   577,\n",
       "   14,\n",
       "   2713,\n",
       "   2677,\n",
       "   1025,\n",
       "   418,\n",
       "   7647,\n",
       "   16490,\n",
       "   1566,\n",
       "   13,\n",
       "   1690,\n",
       "   305,\n",
       "   72,\n",
       "   1686,\n",
       "   2677,\n",
       "   1025,\n",
       "   11516,\n",
       "   27925,\n",
       "   15],\n",
       "  [4218,\n",
       "   491,\n",
       "   66,\n",
       "   18128,\n",
       "   14,\n",
       "   1229,\n",
       "   35,\n",
       "   14,\n",
       "   45,\n",
       "   25151,\n",
       "   316,\n",
       "   27,\n",
       "   47660,\n",
       "   66,\n",
       "   18128,\n",
       "   14,\n",
       "   1229,\n",
       "   35,\n",
       "   14,\n",
       "   45,\n",
       "   25151,\n",
       "   316,\n",
       "   27,\n",
       "   47660,\n",
       "   66,\n",
       "   18128,\n",
       "   14,\n",
       "   1229,\n",
       "   35,\n",
       "   14,\n",
       "   45,\n",
       "   25151,\n",
       "   316,\n",
       "   27,\n",
       "   1737,\n",
       "   310,\n",
       "   253,\n",
       "   6053,\n",
       "   41,\n",
       "   2415,\n",
       "   37319,\n",
       "   5174,\n",
       "   1566,\n",
       "   32,\n",
       "   32869,\n",
       "   27],\n",
       "  [620,\n",
       "   2902,\n",
       "   14,\n",
       "   2082,\n",
       "   67,\n",
       "   27,\n",
       "   1359,\n",
       "   1142,\n",
       "   3210,\n",
       "   1057,\n",
       "   418,\n",
       "   7647,\n",
       "   3788,\n",
       "   452,\n",
       "   32,\n",
       "   2293,\n",
       "   66,\n",
       "   3788,\n",
       "   556,\n",
       "   1264,\n",
       "   3210,\n",
       "   27,\n",
       "   418,\n",
       "   7647,\n",
       "   3788,\n",
       "   7251,\n",
       "   35,\n",
       "   13,\n",
       "   418,\n",
       "   7647,\n",
       "   3788,\n",
       "   5922,\n",
       "   35,\n",
       "   13,\n",
       "   285,\n",
       "   418,\n",
       "   7647,\n",
       "   3788,\n",
       "   818,\n",
       "   35,\n",
       "   15],\n",
       "  [34,\n",
       "   70,\n",
       "   7080,\n",
       "   14,\n",
       "   55,\n",
       "   280,\n",
       "   2447,\n",
       "   30730,\n",
       "   14,\n",
       "   267,\n",
       "   81,\n",
       "   20240,\n",
       "   14,\n",
       "   1229,\n",
       "   67,\n",
       "   27,\n",
       "   1737,\n",
       "   310,\n",
       "   253,\n",
       "   7608,\n",
       "   348,\n",
       "   941,\n",
       "   1511,\n",
       "   273,\n",
       "   329,\n",
       "   70,\n",
       "   7080,\n",
       "   16,\n",
       "   55,\n",
       "   280,\n",
       "   2447,\n",
       "   30730,\n",
       "   14,\n",
       "   267,\n",
       "   81,\n",
       "   20240,\n",
       "   14,\n",
       "   1229,\n",
       "   67,\n",
       "   32,\n",
       "   9599,\n",
       "   1036,\n",
       "   15],\n",
       "  [73,\n",
       "   814,\n",
       "   4233,\n",
       "   620,\n",
       "   2902,\n",
       "   14,\n",
       "   620,\n",
       "   2902,\n",
       "   14,\n",
       "   2082,\n",
       "   67,\n",
       "   27,\n",
       "   1737,\n",
       "   403,\n",
       "   253,\n",
       "   3386,\n",
       "   273,\n",
       "   15729,\n",
       "   4233,\n",
       "   620,\n",
       "   2902,\n",
       "   16,\n",
       "   620,\n",
       "   2902,\n",
       "   14,\n",
       "   2082,\n",
       "   67,\n",
       "   32,\n",
       "   55,\n",
       "   406,\n",
       "   25718,\n",
       "   19662,\n",
       "   27,\n",
       "   4567,\n",
       "   933,\n",
       "   13,\n",
       "   23280,\n",
       "   6081,\n",
       "   21277,\n",
       "   27,\n",
       "   470,\n",
       "   15,\n",
       "   2640,\n",
       "   13,\n",
       "   7608,\n",
       "   348,\n",
       "   5128,\n",
       "   8078,\n",
       "   27,\n",
       "   8253,\n",
       "   1036],\n",
       "  [620,\n",
       "   2902,\n",
       "   14,\n",
       "   24,\n",
       "   67,\n",
       "   27,\n",
       "   1737,\n",
       "   310,\n",
       "   253,\n",
       "   4736,\n",
       "   273,\n",
       "   253,\n",
       "   14980,\n",
       "   3114,\n",
       "   275,\n",
       "   6684,\n",
       "   253,\n",
       "   1566,\n",
       "   32,\n",
       "   510,\n",
       "   4736,\n",
       "   273,\n",
       "   253,\n",
       "   14980,\n",
       "   3114,\n",
       "   310,\n",
       "   281,\n",
       "   1287,\n",
       "   2590,\n",
       "   9600,\n",
       "   1475,\n",
       "   5506,\n",
       "   14980,\n",
       "   275,\n",
       "   2087,\n",
       "   285,\n",
       "   5506,\n",
       "   1781,\n",
       "   3448,\n",
       "   3210,\n",
       "   275,\n",
       "   1798,\n",
       "   15],\n",
       "  [620,\n",
       "   2902,\n",
       "   14,\n",
       "   24,\n",
       "   67,\n",
       "   27,\n",
       "   26198,\n",
       "   2902,\n",
       "   14,\n",
       "   24,\n",
       "   67,\n",
       "   27,\n",
       "   26198,\n",
       "   2902,\n",
       "   14,\n",
       "   24,\n",
       "   67,\n",
       "   27,\n",
       "   1737,\n",
       "   310,\n",
       "   253,\n",
       "   6323,\n",
       "   789,\n",
       "   273,\n",
       "   30680,\n",
       "   32,\n",
       "   510,\n",
       "   6323,\n",
       "   789,\n",
       "   273,\n",
       "   30680,\n",
       "   310,\n",
       "   253,\n",
       "   2440,\n",
       "   273,\n",
       "   418,\n",
       "   7647,\n",
       "   3788,\n",
       "   13,\n",
       "   247,\n",
       "   5147,\n",
       "   323,\n",
       "   2289,\n",
       "   281,\n",
       "   1527,\n",
       "   2603,\n",
       "   21708,\n",
       "   46,\n",
       "   3210,\n",
       "   15]]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_open_llms[\"train\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how to push your own dataset to your Huggingface hub\n",
    "# !pip install huggingface_hub\n",
    "# !huggingface-cli login\n",
    "# split_dataset.push_to_hub(dataset_path_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default_venv",
   "language": "python",
   "name": "default_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
